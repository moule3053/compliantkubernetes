{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Compliant Kubernetes Compliant Kubernetes is a Certified Kubernetes distribution, i.e., an opinionated way of packaging and configuring Kubernetes together with other projects. Compliant Kubernetes reduces the compliance burden, as required to comply with: Health Insurance Portability and Accountability Act (HIPAA) Swedish Healthcare (Patientdatalagen) General Data Protection Regulation (GDPR) Payment Card Industry Data Security Standard (PCI DSS) Finansinspektionen's Regulatory Code (e.g., FFFS 2014:7) Other regulations that map to information security standards, such as ISO 27001 Why Compliant Kubernetes? Kubernetes has established itself as a go-to solution for high development velocity without vendor lock-in. However, vanilla Kubernetes is not usable in regulated industry, since it is not secure by default, nor by itself . Therefore, if you want to benefit from the speed of cloud native development in regulated industries, Kubernetes needs to be carefully configured. Furthermore, Kubernetes is a laser-focused project (\"Make each program do one thing well.\"), so it needs to be complemented with other cloud native projects. Compliant Kubernetes fills this gap.","title":"Overview"},{"location":"#welcome-to-compliant-kubernetes","text":"Compliant Kubernetes is a Certified Kubernetes distribution, i.e., an opinionated way of packaging and configuring Kubernetes together with other projects. Compliant Kubernetes reduces the compliance burden, as required to comply with: Health Insurance Portability and Accountability Act (HIPAA) Swedish Healthcare (Patientdatalagen) General Data Protection Regulation (GDPR) Payment Card Industry Data Security Standard (PCI DSS) Finansinspektionen's Regulatory Code (e.g., FFFS 2014:7) Other regulations that map to information security standards, such as ISO 27001","title":"Welcome to Compliant Kubernetes"},{"location":"#why-compliant-kubernetes","text":"Kubernetes has established itself as a go-to solution for high development velocity without vendor lock-in. However, vanilla Kubernetes is not usable in regulated industry, since it is not secure by default, nor by itself . Therefore, if you want to benefit from the speed of cloud native development in regulated industries, Kubernetes needs to be carefully configured. Furthermore, Kubernetes is a laser-focused project (\"Make each program do one thing well.\"), so it needs to be complemented with other cloud native projects. Compliant Kubernetes fills this gap.","title":"Why Compliant Kubernetes?"},{"location":"architecture/","text":"Architecture Below we present the architecture of Compliant Kubernetes, using the C4 model . For the nitty-gritty details, see Architectural Decision Records . Level 1: System Context Let us start with the system context. Compliance imposes restrictions on all levels of the tech stack. Your compliance focus should mostly lie on your application. Compliant Kubernetes ensures that the platform hosting your application is compliant. Finally, you need the whole software stack on a hardware that is managed in a compliant way, either via an ISO 27001-certified cloud provider or using on-prem hardware. Level 2: Clusters Most regulations require logging to a tamper-proof environment. This is usually interpreted as an attacker gaining access to your application should not be able to delete logs showing their attack and the harm caused by their attack. To achieve this, Compliant Kubernetes is implemented as two Kubernetes clusters A workload cluster , which hosts your application, and A service cluster , which hosts services for monitoring, logging and vulnerability management. Level 3: Individual Components Click on the diagram below to see the nuts-and-bolts of Compliant Kubernetes. Note Due to technical limitations, some compliance-related components still need to run in the workload cluster. These are visible when inspecting the workload cluster, for example, via the Kubernetes API . Currently, these components are: Falco, for intrusion detection; Prometheus, for collecting metrics; Fluentd, for collecting logs; OpenPolicyAgent, for enforcing Kubernetes API policies. Note that, the logs, metrics and alerts produced by these components are immediately pushed into the tamper-proof logging environment, hence this technical limitation does not weaken compliance.","title":"Architecture"},{"location":"architecture/#architecture","text":"Below we present the architecture of Compliant Kubernetes, using the C4 model . For the nitty-gritty details, see Architectural Decision Records .","title":"Architecture"},{"location":"architecture/#level-1-system-context","text":"Let us start with the system context. Compliance imposes restrictions on all levels of the tech stack. Your compliance focus should mostly lie on your application. Compliant Kubernetes ensures that the platform hosting your application is compliant. Finally, you need the whole software stack on a hardware that is managed in a compliant way, either via an ISO 27001-certified cloud provider or using on-prem hardware.","title":"Level 1: System Context"},{"location":"architecture/#level-2-clusters","text":"Most regulations require logging to a tamper-proof environment. This is usually interpreted as an attacker gaining access to your application should not be able to delete logs showing their attack and the harm caused by their attack. To achieve this, Compliant Kubernetes is implemented as two Kubernetes clusters A workload cluster , which hosts your application, and A service cluster , which hosts services for monitoring, logging and vulnerability management.","title":"Level 2: Clusters"},{"location":"architecture/#level-3-individual-components","text":"Click on the diagram below to see the nuts-and-bolts of Compliant Kubernetes. Note Due to technical limitations, some compliance-related components still need to run in the workload cluster. These are visible when inspecting the workload cluster, for example, via the Kubernetes API . Currently, these components are: Falco, for intrusion detection; Prometheus, for collecting metrics; Fluentd, for collecting logs; OpenPolicyAgent, for enforcing Kubernetes API policies. Note that, the logs, metrics and alerts produced by these components are immediately pushed into the tamper-proof logging environment, hence this technical limitation does not weaken compliance.","title":"Level 3: Individual Components"},{"location":"benefits/","text":"Exploring the benefits of Compliant Kubernetes If you are new to the Compliant Kubernetes project, you are encouraged to carry out your own proof-of-concept using Compliant Kubernetes to get a feel for its features and how it can provide tangible benefit to your application and operations. The following are suggested aspects to investigate, which also help building an understanding for the platform. The application As a Kubernetes-based platform distribution of software, any containerized application will of course do. However, to get the best possible understanding of the Compliant Kubernetes platform's features, we suggest that your application has: a publicly facing front end part, which connects to a back end business logic application, which connects to a database system. This will let you explore some of the features that the Compliant Kubernetes platform offers and see how these benefit your needs and workflows. Beware: PodSecurityPolicies in place Be mindful of not trying to start Pods that assume they can run using the root account. In regulated environments, doing this should of course not be permitted, as it needlessly increases your attack surface. So all your applications should run with as few permissions as possible. Add capabilities if you must, but don't try to run as root! Compliant Kubernetes benefits to explore Integration with your identity provider (IdP) of choice Since Compliant Kubernetes relies on Dex , it integrates with various identity providers, such as LDAP-based ones (Active Directory), SAML, OpenID Connect, GitHub, and more, including Google accounts. See the whole list of support by looking at the list of connectors . Set it up with your provider of choice to get a feel for how easily it will integrate with your workflow! Application and audit logs to Elasticsearch Your application just needs to write to standard output, as is typical in Kubernetes-based platform. Whatever it writes will be collected and stored in your preconfigured Elasticsearch service that Compliant Kubernetes ships with (OpenDistro for Elasticsearch). Access it from your dashboard! Check out the audit logs as well, to see that all API actions against the Kubernetes API are logged in Elasticsearch. Perhaps try to carry out administrative tasks with a non-privileged user to see that these are prevented and logged. You can of course set up alerting for this type of event. Monitoring data to Prometheus, viewed with Grafana The Pods of your application will automatically be monitored by Prometheus. Check it out from your dashboard and see the data update. Custom incident alerts in Alertmanager Create a couple of alerts that make sense for your application based on the data reported into Prometheus. Alertmanager integrates natively with e.g. Slack and PagerDuty and also supports a wide range of additional notification channels via webhooks so you can try setting up something that makes sense for your operations team. Image vulnerability scanning for known threats Upload your container images to the Compliant Kubernetes Harbor image registry. Verify that scanning takes place, and see if your images are secure. How about intentionally pushing an image based on some old base image to see the list populate fast! Intrusion detection system for unknown threats Make your application do unsafe things to trigger Falco, the intrusion detection system. Try to write something to the /etc directory! Policies and automatic enforcement Compliant Kubernetes integrates with the Open Policy Agent (OPA), which helps enforce policies automatically. Such policies can prevent e.g. the use of default passwords when connecting to databases, or configuration errors such as deploying a non-vetted Pod to production. Set up a policy that makes sense for your application and watch as OPA immediately stops violations to these policies to occur. It catches API requests to the Kubernetes API before they can touch resources in the cluster. Network isolation/segregation Set up standard Kubernetes Network Policies such that all inbound and outbound traffic is restricted to that which is necessary for the application to work, and other traffic is specifically denied: only the front end is publicly exposed; the front end can only initiate connections to the back end; the back end can only be connected to from the front end (no other components running in other namespaces); the back end only gets to initiate connections to the database and to the known external endpoints; nothing besides the back end application gets to connect to the database; and the database may never initiate connections to anything on its own. Doing this shows that not only you have network isolation/segregation up and running, but also, you have significantly reduced your attack surface. Should code get exploited in either component, it will still be limited in what damage it can do to the overall system. Automatic certificate management Of course your publicly exposed front end should support (only?) encrypted traffic. Set up the cert-manager to give your exposed service a certificate issued by Let's Encrypt.","title":"Exploring Benefits"},{"location":"benefits/#exploring-the-benefits-of-compliant-kubernetes","text":"If you are new to the Compliant Kubernetes project, you are encouraged to carry out your own proof-of-concept using Compliant Kubernetes to get a feel for its features and how it can provide tangible benefit to your application and operations. The following are suggested aspects to investigate, which also help building an understanding for the platform.","title":"Exploring the benefits of Compliant Kubernetes"},{"location":"benefits/#the-application","text":"As a Kubernetes-based platform distribution of software, any containerized application will of course do. However, to get the best possible understanding of the Compliant Kubernetes platform's features, we suggest that your application has: a publicly facing front end part, which connects to a back end business logic application, which connects to a database system. This will let you explore some of the features that the Compliant Kubernetes platform offers and see how these benefit your needs and workflows.","title":"The application"},{"location":"benefits/#beware-podsecuritypolicies-in-place","text":"Be mindful of not trying to start Pods that assume they can run using the root account. In regulated environments, doing this should of course not be permitted, as it needlessly increases your attack surface. So all your applications should run with as few permissions as possible. Add capabilities if you must, but don't try to run as root!","title":"Beware: PodSecurityPolicies in place"},{"location":"benefits/#compliant-kubernetes-benefits-to-explore","text":"","title":"Compliant Kubernetes benefits to explore"},{"location":"benefits/#integration-with-your-identity-provider-idp-of-choice","text":"Since Compliant Kubernetes relies on Dex , it integrates with various identity providers, such as LDAP-based ones (Active Directory), SAML, OpenID Connect, GitHub, and more, including Google accounts. See the whole list of support by looking at the list of connectors . Set it up with your provider of choice to get a feel for how easily it will integrate with your workflow!","title":"Integration with your identity provider (IdP) of choice"},{"location":"benefits/#application-and-audit-logs-to-elasticsearch","text":"Your application just needs to write to standard output, as is typical in Kubernetes-based platform. Whatever it writes will be collected and stored in your preconfigured Elasticsearch service that Compliant Kubernetes ships with (OpenDistro for Elasticsearch). Access it from your dashboard! Check out the audit logs as well, to see that all API actions against the Kubernetes API are logged in Elasticsearch. Perhaps try to carry out administrative tasks with a non-privileged user to see that these are prevented and logged. You can of course set up alerting for this type of event.","title":"Application and audit logs to Elasticsearch"},{"location":"benefits/#monitoring-data-to-prometheus-viewed-with-grafana","text":"The Pods of your application will automatically be monitored by Prometheus. Check it out from your dashboard and see the data update.","title":"Monitoring data to Prometheus, viewed with Grafana"},{"location":"benefits/#custom-incident-alerts-in-alertmanager","text":"Create a couple of alerts that make sense for your application based on the data reported into Prometheus. Alertmanager integrates natively with e.g. Slack and PagerDuty and also supports a wide range of additional notification channels via webhooks so you can try setting up something that makes sense for your operations team.","title":"Custom incident alerts in Alertmanager"},{"location":"benefits/#image-vulnerability-scanning-for-known-threats","text":"Upload your container images to the Compliant Kubernetes Harbor image registry. Verify that scanning takes place, and see if your images are secure. How about intentionally pushing an image based on some old base image to see the list populate fast!","title":"Image vulnerability scanning for known threats"},{"location":"benefits/#intrusion-detection-system-for-unknown-threats","text":"Make your application do unsafe things to trigger Falco, the intrusion detection system. Try to write something to the /etc directory!","title":"Intrusion detection system for unknown threats"},{"location":"benefits/#policies-and-automatic-enforcement","text":"Compliant Kubernetes integrates with the Open Policy Agent (OPA), which helps enforce policies automatically. Such policies can prevent e.g. the use of default passwords when connecting to databases, or configuration errors such as deploying a non-vetted Pod to production. Set up a policy that makes sense for your application and watch as OPA immediately stops violations to these policies to occur. It catches API requests to the Kubernetes API before they can touch resources in the cluster.","title":"Policies and automatic enforcement"},{"location":"benefits/#network-isolationsegregation","text":"Set up standard Kubernetes Network Policies such that all inbound and outbound traffic is restricted to that which is necessary for the application to work, and other traffic is specifically denied: only the front end is publicly exposed; the front end can only initiate connections to the back end; the back end can only be connected to from the front end (no other components running in other namespaces); the back end only gets to initiate connections to the database and to the known external endpoints; nothing besides the back end application gets to connect to the database; and the database may never initiate connections to anything on its own. Doing this shows that not only you have network isolation/segregation up and running, but also, you have significantly reduced your attack surface. Should code get exploited in either component, it will still be limited in what damage it can do to the overall system.","title":"Network isolation/segregation"},{"location":"benefits/#automatic-certificate-management","text":"Of course your publicly exposed front end should support (only?) encrypted traffic. Set up the cert-manager to give your exposed service a certificate issued by Let's Encrypt.","title":"Automatic certificate management"},{"location":"compliance/","text":"Compliance Basics Compliance will vary widely depending on: Jurisdiction (e.g., US vs. EU); Industry regulation (e.g., MedTech vs. FinTech); Company policies (e.g., log retention based on cost-risk analysis). The following is meant to offer an overview of compliance focusing on information security, and how Compliant Kubernetes reduces compliance burden. Click on the revelant blue text to find out more: Compliance: The Societal Perspective Organizations in certain sectors, such as BioTech, FinTech, MedTech, and those processing personal data, need public trust to operate. Such companies are allowed to handle sensitive data, create and destroy money, etc., in exchange for being compliant with certain regulations \u2014 in devtalk put, sticking to some rules set by regulators. For example: Any bank operating in Sweden is regulated by the Swedish Financial Authority (Finansinspektionen) and has to comply with FFFS. Any organization dealing with personal data is scrutinized by the Swedish Data Protection Authority (Datainspektionen) and needs to comply with GDPR. Any organization handling patient data needs to comply with HIPAA in the US or Patientdatalagen (PDL) in Sweden. Such regulation is not only aspirational, but is actually checked as often as yearly by an external auditor. If an organization is found to be non-compliant it may pay heavy fines or even lose its license to operate. Compliance: The Engineering Perspective Translating legalese into code involves several steps. First a Compliance Officer will identify what regulations apply to the company. Based on those regulations, they will draft policies to clarify how the company should operate \u2014 i.e., run its daily business \u2014 in the most efficient manner while complying with regulations. To ensure the policies do not have gaps, are non-overlapping and consistent, they will generally follow an information security standard , such as ISO/IEC 27001, SOC 2 or PCI DSS. Such information security standards list a set of controls , i.e., \"points\" in the organization where a process and a check needs to be put in place. The resulting policies need to be interpreted and implemented by each department. Some of these can be supported by, or entirely implemented by, technology. Compliant Kubernetes includes software to do just that, and thus, Compliant Kubernetes addresses the needs of the infrastructure team. In essence, Compliant Kubernetes are carefully configured Kubernetes clusters together with other open-source components. They reduce compliance burden by allowing an organization to focus on making their processes and application compliant, knowing that the underlying platform is compliant. As far as getting certification, a key aspect is the ability to point to documentation that clearly states that your tech stack fulfils all stipulated requirements. By relying on Compliant Kubernetes, the majority of this work is already done for you.","title":"Compliance Basics"},{"location":"compliance/#compliance-basics","text":"Compliance will vary widely depending on: Jurisdiction (e.g., US vs. EU); Industry regulation (e.g., MedTech vs. FinTech); Company policies (e.g., log retention based on cost-risk analysis). The following is meant to offer an overview of compliance focusing on information security, and how Compliant Kubernetes reduces compliance burden. Click on the revelant blue text to find out more:","title":"Compliance Basics"},{"location":"compliance/#compliance-the-societal-perspective","text":"Organizations in certain sectors, such as BioTech, FinTech, MedTech, and those processing personal data, need public trust to operate. Such companies are allowed to handle sensitive data, create and destroy money, etc., in exchange for being compliant with certain regulations \u2014 in devtalk put, sticking to some rules set by regulators. For example: Any bank operating in Sweden is regulated by the Swedish Financial Authority (Finansinspektionen) and has to comply with FFFS. Any organization dealing with personal data is scrutinized by the Swedish Data Protection Authority (Datainspektionen) and needs to comply with GDPR. Any organization handling patient data needs to comply with HIPAA in the US or Patientdatalagen (PDL) in Sweden. Such regulation is not only aspirational, but is actually checked as often as yearly by an external auditor. If an organization is found to be non-compliant it may pay heavy fines or even lose its license to operate.","title":"Compliance: The Societal Perspective"},{"location":"compliance/#compliance-the-engineering-perspective","text":"Translating legalese into code involves several steps. First a Compliance Officer will identify what regulations apply to the company. Based on those regulations, they will draft policies to clarify how the company should operate \u2014 i.e., run its daily business \u2014 in the most efficient manner while complying with regulations. To ensure the policies do not have gaps, are non-overlapping and consistent, they will generally follow an information security standard , such as ISO/IEC 27001, SOC 2 or PCI DSS. Such information security standards list a set of controls , i.e., \"points\" in the organization where a process and a check needs to be put in place. The resulting policies need to be interpreted and implemented by each department. Some of these can be supported by, or entirely implemented by, technology. Compliant Kubernetes includes software to do just that, and thus, Compliant Kubernetes addresses the needs of the infrastructure team. In essence, Compliant Kubernetes are carefully configured Kubernetes clusters together with other open-source components. They reduce compliance burden by allowing an organization to focus on making their processes and application compliant, knowing that the underlying platform is compliant. As far as getting certification, a key aspect is the ability to point to documentation that clearly states that your tech stack fulfils all stipulated requirements. By relying on Compliant Kubernetes, the majority of this work is already done for you.","title":"Compliance: The Engineering Perspective"},{"location":"faq/","text":"Why can't I kubectl run ? To increase security, Compliance Kubernetes does not allow by default to run containers as root. Additionally, the container image is not allowed to be pulled from a public docker hub registry and all Pods are required to be selected by some NetworkPolicy. This ensures that an active decision has been made for what network access the Pod should have and helps avoid running \"obscure things found on the internet\". Considering the above, you should start by pushing the container image you want to use to Harbor and make sure it doesn't run as root . See this document for how to use OIDC with docker. With that in place, you will need to create a NetworkPolicy for the Pod you want to run. Here is an example of how to create a NetworkPolicy that allows all TCP traffic (in and out) for Pods with the label run: blah . Note This is just an example, not a good idea! You should limit the policy to whatever your application really needs. kubectl apply -f - <<EOF apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: blah spec: podSelector: matchLabels: run: blah policyTypes: - Ingress - Egress ingress: # Allow all incoming traffic - {} egress: # Allow all outgoing traffic - {} EOF Now you are ready to run a Pod! Make sure you match the name with the label you used for the NetworkPolicy. Kubectl will automatically set the label run: <name-of-pod> when you create a Pod with kubectl run <name-of-pod> . Here is an example command (please replace the $MY_HARBOR_IMAGE ): kubectl run blah --rm -ti --image = $MY_HARBOR_IMAGE If your image runs as root by defaults, but can handle running as another user, you may override the user by adding a flag like this to the above command: --overrides='{ \"spec\": { \"securityContext\": \"runAsUser\": 1000, \"runAsGroup\": 1000 } }' I updated some Elasticsearch options but it didn't work If you update the Elasticsearch securityConfig you will have to make sure that the master Pod(s) are restarted so that they pick up the new Secret and then run the securityadmin.sh script. This happens for example if you switch from non-SSO to SSO. To reload the configuration you need to run the following commands: # Make the script executable kubectl -n elastic-system exec opendistro-es-master-0 -- chmod +x ./plugins/opendistro_security/tools/securityadmin.sh # Run the script to update the configuration kubectl -n elastic-system exec opendistro-es-master-0 -- ./plugins/opendistro_security/tools/securityadmin.sh \\ -f plugins/opendistro_security/securityconfig/config.yml \\ -icl -nhnv \\ -cacert config/admin-root-ca.pem \\ -cert config/admin-crt.pem \\ -key config/admin-key.pem Note that the above only reloads the config.yml (as specified with the -f ). If you made changes to other parts of the system you will need to point to the relevant file to reload, or reload everything like this: # Run the script to update \"everything\" (internal users, roles, configuration, etc.) kubectl -n elastic-system exec opendistro-es-master-0 -- ./plugins/opendistro_security/tools/securityadmin.sh \\ -cd plugins/opendistro_security/securityconfig/ \\ -icl -nhnv \\ -cacert config/admin-root-ca.pem \\ -cert config/admin-crt.pem \\ -key config/admin-key.pem When you update things other than config.yml you will also need to rerun the Configurer Job (e.g. by making some small change to the resource requests and applying the chart again). Will GrafanaLabs change to AGPL licenses affect Compliant Kubernetes TL;DR Users and administrators of Compliant Kubernetes are unaffected. Part of Compliant Kubernetes -- specifically the CISO dashboards -- are built on top of Grafana, which recently changed its license to AGPLv3 . In brief, if Grafana is exposed via a network connection -- as is the case with Compliant Kubernetes -- then AGPLv3 requires all source code including modifications to be made available. The exact difference between \"aggregate\" and \"modified version\" is somewhat unclear . Compliant Kubernetes only configures Grafana and does not change its source code. Hence, we determined that Compliant Kubernetes is an \"aggregate\" work and is unaffected by the \"viral\" clauses of AGPLv3. As a result, Compliant Kubernetes continues to be distributed under Apache 2.0 as before. Will Min.io change to AGPL licenses affect Compliant Kubernetes TL;DR Users and administrators of Compliant Kubernetes are unaffected. Min.io recently changed its license to AGPLv3 . Certain installations of Compliant Kubernetes may use Min.io for accessing object storage on Azure or GCP. However, Compliant Kubernetes does not currently include Min.io. In brief, if Min.io is exposed via a network connection, then AGPLv3 requires all source code including modifications to be made available. The exact difference between \"aggregate\" and \"modified version\" is somewhat unclear . When using Min.io with Compliant Kubernetes, we only use Min.io via its S3-compatible API. Hence, we determined that Compliant Kubernetes is an \"aggregate\" work and is unaffected by the \"viral\" clauses of AGPLv3. As a result, Compliant Kubernetes continues to be distributed under Apache 2.0 as before.","title":"FAQ"},{"location":"faq/#why-cant-i-kubectl-run","text":"To increase security, Compliance Kubernetes does not allow by default to run containers as root. Additionally, the container image is not allowed to be pulled from a public docker hub registry and all Pods are required to be selected by some NetworkPolicy. This ensures that an active decision has been made for what network access the Pod should have and helps avoid running \"obscure things found on the internet\". Considering the above, you should start by pushing the container image you want to use to Harbor and make sure it doesn't run as root . See this document for how to use OIDC with docker. With that in place, you will need to create a NetworkPolicy for the Pod you want to run. Here is an example of how to create a NetworkPolicy that allows all TCP traffic (in and out) for Pods with the label run: blah . Note This is just an example, not a good idea! You should limit the policy to whatever your application really needs. kubectl apply -f - <<EOF apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: blah spec: podSelector: matchLabels: run: blah policyTypes: - Ingress - Egress ingress: # Allow all incoming traffic - {} egress: # Allow all outgoing traffic - {} EOF Now you are ready to run a Pod! Make sure you match the name with the label you used for the NetworkPolicy. Kubectl will automatically set the label run: <name-of-pod> when you create a Pod with kubectl run <name-of-pod> . Here is an example command (please replace the $MY_HARBOR_IMAGE ): kubectl run blah --rm -ti --image = $MY_HARBOR_IMAGE If your image runs as root by defaults, but can handle running as another user, you may override the user by adding a flag like this to the above command: --overrides='{ \"spec\": { \"securityContext\": \"runAsUser\": 1000, \"runAsGroup\": 1000 } }'","title":"Why can't I kubectl run?"},{"location":"faq/#i-updated-some-elasticsearch-options-but-it-didnt-work","text":"If you update the Elasticsearch securityConfig you will have to make sure that the master Pod(s) are restarted so that they pick up the new Secret and then run the securityadmin.sh script. This happens for example if you switch from non-SSO to SSO. To reload the configuration you need to run the following commands: # Make the script executable kubectl -n elastic-system exec opendistro-es-master-0 -- chmod +x ./plugins/opendistro_security/tools/securityadmin.sh # Run the script to update the configuration kubectl -n elastic-system exec opendistro-es-master-0 -- ./plugins/opendistro_security/tools/securityadmin.sh \\ -f plugins/opendistro_security/securityconfig/config.yml \\ -icl -nhnv \\ -cacert config/admin-root-ca.pem \\ -cert config/admin-crt.pem \\ -key config/admin-key.pem Note that the above only reloads the config.yml (as specified with the -f ). If you made changes to other parts of the system you will need to point to the relevant file to reload, or reload everything like this: # Run the script to update \"everything\" (internal users, roles, configuration, etc.) kubectl -n elastic-system exec opendistro-es-master-0 -- ./plugins/opendistro_security/tools/securityadmin.sh \\ -cd plugins/opendistro_security/securityconfig/ \\ -icl -nhnv \\ -cacert config/admin-root-ca.pem \\ -cert config/admin-crt.pem \\ -key config/admin-key.pem When you update things other than config.yml you will also need to rerun the Configurer Job (e.g. by making some small change to the resource requests and applying the chart again).","title":"I updated some Elasticsearch options but it didn't work"},{"location":"faq/#will-grafanalabs-change-to-agpl-licenses-affect-compliant-kubernetes","text":"TL;DR Users and administrators of Compliant Kubernetes are unaffected. Part of Compliant Kubernetes -- specifically the CISO dashboards -- are built on top of Grafana, which recently changed its license to AGPLv3 . In brief, if Grafana is exposed via a network connection -- as is the case with Compliant Kubernetes -- then AGPLv3 requires all source code including modifications to be made available. The exact difference between \"aggregate\" and \"modified version\" is somewhat unclear . Compliant Kubernetes only configures Grafana and does not change its source code. Hence, we determined that Compliant Kubernetes is an \"aggregate\" work and is unaffected by the \"viral\" clauses of AGPLv3. As a result, Compliant Kubernetes continues to be distributed under Apache 2.0 as before.","title":"Will GrafanaLabs change to AGPL licenses affect Compliant Kubernetes"},{"location":"faq/#will-minio-change-to-agpl-licenses-affect-compliant-kubernetes","text":"TL;DR Users and administrators of Compliant Kubernetes are unaffected. Min.io recently changed its license to AGPLv3 . Certain installations of Compliant Kubernetes may use Min.io for accessing object storage on Azure or GCP. However, Compliant Kubernetes does not currently include Min.io. In brief, if Min.io is exposed via a network connection, then AGPLv3 requires all source code including modifications to be made available. The exact difference between \"aggregate\" and \"modified version\" is somewhat unclear . When using Min.io with Compliant Kubernetes, we only use Min.io via its S3-compatible API. Hence, we determined that Compliant Kubernetes is an \"aggregate\" work and is unaffected by the \"viral\" clauses of AGPLv3. As a result, Compliant Kubernetes continues to be distributed under Apache 2.0 as before.","title":"Will Min.io change to AGPL licenses affect Compliant Kubernetes"},{"location":"release-notes/","text":"Release Notes Compliant Kubernetes v0.17.0 - 2021-06-29 v0.16.0 - 2021-05-27 v0.15.0 - 2021-05-05 Note For a more detailed look check out the full changelog . v0.17.0 Released 2021-06-29. Changes: The dashboard tool Grafana has been updated to a new major version of 8.x.x. This introduces new features and fixes, as well as some possibly breaking changes. See their release notes for more information. The single-sign-on service Dex has been updated, bringing small changes and better consistency to the UI. Fixes, improvements to resource limits, resource usage, and stability. v0.16.0 Released 2021-05-27. Changes: The default retention values have been changed and streamlined for authlog* and other* . The former will be kept for a longer period of time while the latter for shorter, both have reduced sized according to their actual usage. Updates, fixes, and features to improve the security of the platform. v0.15.0 Released 2021-05-05. Changes: The search and analythics engine ElasticSearch now indexes the authlog* logs. Updates, fixes, and streamlined the install components to avoid redundant ones. Compliant Kubernetes Kubespray v2.16.0-ck8s1 - 2021-07-02 v2.15.0-ck8s1 - 2021-05-27 Note For a more detailed look check out the full changelog . v2.16.0-ck8s1 Released 2021-07-02. Changes: Kubespray updated, including Kubernetes upgrade to version 1.20.7. v2.15.0-ck8s1 Released 2021-05-27. First stable release!","title":"Release Notes"},{"location":"release-notes/#release-notes","text":"","title":"Release Notes"},{"location":"release-notes/#compliant-kubernetes","text":"v0.17.0 - 2021-06-29 v0.16.0 - 2021-05-27 v0.15.0 - 2021-05-05 Note For a more detailed look check out the full changelog .","title":"Compliant Kubernetes"},{"location":"release-notes/#v0170","text":"Released 2021-06-29. Changes: The dashboard tool Grafana has been updated to a new major version of 8.x.x. This introduces new features and fixes, as well as some possibly breaking changes. See their release notes for more information. The single-sign-on service Dex has been updated, bringing small changes and better consistency to the UI. Fixes, improvements to resource limits, resource usage, and stability.","title":"v0.17.0"},{"location":"release-notes/#v0160","text":"Released 2021-05-27. Changes: The default retention values have been changed and streamlined for authlog* and other* . The former will be kept for a longer period of time while the latter for shorter, both have reduced sized according to their actual usage. Updates, fixes, and features to improve the security of the platform.","title":"v0.16.0"},{"location":"release-notes/#v0150","text":"Released 2021-05-05. Changes: The search and analythics engine ElasticSearch now indexes the authlog* logs. Updates, fixes, and streamlined the install components to avoid redundant ones.","title":"v0.15.0"},{"location":"release-notes/#compliant-kubernetes-kubespray","text":"v2.16.0-ck8s1 - 2021-07-02 v2.15.0-ck8s1 - 2021-05-27 Note For a more detailed look check out the full changelog .","title":"Compliant Kubernetes Kubespray"},{"location":"release-notes/#v2160-ck8s1","text":"Released 2021-07-02. Changes: Kubespray updated, including Kubernetes upgrade to version 1.20.7.","title":"v2.16.0-ck8s1"},{"location":"release-notes/#v2150-ck8s1","text":"Released 2021-05-27. First stable release!","title":"v2.15.0-ck8s1"},{"location":"roadmap/","text":"Roadmap Light Compliant Kubernetes Renderings Some users have requested a Compliant Kubernetes workload cluster, but with external container registry or logging solution. Compliant Kubernetes needs to be revised to facilitate this. Multi-Region High-Availability Some business continuity policies may require redundancy across regions. It should be able to run Compliant Kubernetes across regions of a cloud provider. More generic Multi-Tenancy Right now, one service cluster can support multiple workload clusters, as long as they all use the same identity provider and reside in the same cloud provider. In the future, we want to relax these requirements for more flexible multi-tenancy scenarios. Support to run on top of multiple Kubernetes distributions, included managed services. Compliant Kubernetes can be configured on top of Kubernetes clusters created with kubespray or similar tools. It could be possible to run Compliant Kubernetes on top of managed Kubernetes services such as GKE, as well as distributions such as OpenShift. Out of the box service mesh support Compliant Kubernetes can be used with service meshes such as Istio and LinkerD, but does not come with a preconfigured service mesh installer as of now. Compliant Managed Services on top of Compliant Kubernetes With improved support for stateful applications and operators in Kubernetes, it is now possible to offer managed services on top of Kubernetes. Said services should reduce compliance burden, hence building them on top of Compliant Kubernetes is natural. The following managed services are envisioned for cloud providers who use Compliant Kubernetes: Managed Container Registry (e.g., Harbor); Managed Database (e.g., MariaDB, MySQL, PostgreSQL); Managed Message Queues (e.g., NATS, Kafka); Managed Caches (e.g., Redis); Managed Logging (e.g., Elasticsearch). Non-Goals CI/CD Compliant Kubernetes can be used with a wide range of CI/CD pipelines, including traditional push-style tools and pull-style solutions such as GitOps operators. Compliant Kubernetes will not be opinionated and prescribe a certain CI/CD technology.","title":"Roadmap"},{"location":"roadmap/#roadmap","text":"","title":"Roadmap"},{"location":"roadmap/#light-compliant-kubernetes-renderings","text":"Some users have requested a Compliant Kubernetes workload cluster, but with external container registry or logging solution. Compliant Kubernetes needs to be revised to facilitate this.","title":"Light Compliant Kubernetes Renderings"},{"location":"roadmap/#multi-region-high-availability","text":"Some business continuity policies may require redundancy across regions. It should be able to run Compliant Kubernetes across regions of a cloud provider.","title":"Multi-Region High-Availability"},{"location":"roadmap/#more-generic-multi-tenancy","text":"Right now, one service cluster can support multiple workload clusters, as long as they all use the same identity provider and reside in the same cloud provider. In the future, we want to relax these requirements for more flexible multi-tenancy scenarios.","title":"More generic Multi-Tenancy"},{"location":"roadmap/#support-to-run-on-top-of-multiple-kubernetes-distributions-included-managed-services","text":"Compliant Kubernetes can be configured on top of Kubernetes clusters created with kubespray or similar tools. It could be possible to run Compliant Kubernetes on top of managed Kubernetes services such as GKE, as well as distributions such as OpenShift.","title":"Support to run on top of multiple Kubernetes distributions, included managed services."},{"location":"roadmap/#out-of-the-box-service-mesh-support","text":"Compliant Kubernetes can be used with service meshes such as Istio and LinkerD, but does not come with a preconfigured service mesh installer as of now.","title":"Out of the box service mesh support"},{"location":"roadmap/#compliant-managed-services-on-top-of-compliant-kubernetes","text":"With improved support for stateful applications and operators in Kubernetes, it is now possible to offer managed services on top of Kubernetes. Said services should reduce compliance burden, hence building them on top of Compliant Kubernetes is natural. The following managed services are envisioned for cloud providers who use Compliant Kubernetes: Managed Container Registry (e.g., Harbor); Managed Database (e.g., MariaDB, MySQL, PostgreSQL); Managed Message Queues (e.g., NATS, Kafka); Managed Caches (e.g., Redis); Managed Logging (e.g., Elasticsearch).","title":"Compliant Managed Services on top of Compliant Kubernetes"},{"location":"roadmap/#non-goals","text":"","title":"Non-Goals"},{"location":"roadmap/#cicd","text":"Compliant Kubernetes can be used with a wide range of CI/CD pipelines, including traditional push-style tools and pull-style solutions such as GitOps operators. Compliant Kubernetes will not be opinionated and prescribe a certain CI/CD technology.","title":"CI/CD"},{"location":"vocabulary/","text":"Vocabulary This page introduces terminology used in the Compliant Kubernetes project. We assume that you are familiar with Kubernetes concepts . Administrator : A person or automation process (i.e., CI/CD pipeline) that creates, destoys, updates or otherwise maintains a Compliant Kubernetes installation. Control : \"Points\" in an organization that need a clear policy in order to comply with regulation. Regulation : Law or contractual requirements that an organization is required to follow to be allowed to operate. Operator : Software extension to Kubernetes (see Operator pattern ). Rarely used to mean \"administrator\". Service cluster : Kubernetes cluster that hosts monitoring, logging and technical vulnerability management components. These components are separated from the workload cluster to give an extra layer of security, as is required by some regulations. Workload cluster : Kubernetes cluster hosting the application that exposes end-user -- front-office or back-office -- functionality. User : A person or an automation process (i.e., CI/CD pipeline) that interacts with Compliant Kubernetes for the purpose of running and monitoring an application hosted by Compliant Kubernetes.","title":"Vocabulary"},{"location":"vocabulary/#vocabulary","text":"This page introduces terminology used in the Compliant Kubernetes project. We assume that you are familiar with Kubernetes concepts . Administrator : A person or automation process (i.e., CI/CD pipeline) that creates, destoys, updates or otherwise maintains a Compliant Kubernetes installation. Control : \"Points\" in an organization that need a clear policy in order to comply with regulation. Regulation : Law or contractual requirements that an organization is required to follow to be allowed to operate. Operator : Software extension to Kubernetes (see Operator pattern ). Rarely used to mean \"administrator\". Service cluster : Kubernetes cluster that hosts monitoring, logging and technical vulnerability management components. These components are separated from the workload cluster to give an extra layer of security, as is required by some regulations. Workload cluster : Kubernetes cluster hosting the application that exposes end-user -- front-office or back-office -- functionality. User : A person or an automation process (i.e., CI/CD pipeline) that interacts with Compliant Kubernetes for the purpose of running and monitoring an application hosted by Compliant Kubernetes.","title":"Vocabulary"},{"location":"adr/","text":"Architectural Decision Log Mapping to ISO 27001 Controls A.14.1.1 \"Information security requirements analysis and specification\" What are architectural decisions? Architectural decisions are high-level technical decisions that affect most stakeholders, in particular Compliant Kubernetes developers, administrators and users. A non-exhaustive list of architectural decisions is as follows: adding or removing tools; adding or removing components; changing what component talks to what other component; major (in the SemVer sense) component upgrades. Architectural decisions should be taken as directions to follow for future development and not issues to be fixed immediately. What triggers an architectural decision? An architectural decision generally starts with one of the following: A new features was requested by product management. An improvement was requested by engineering management. A new risk was discovered, usually by the architect, but also by any stakeholder. A new technology was discovered, that may help with a new feature, an improvement or to mitigate a risk. How are architectural decisions captured? Architectural decisions are captured via Architectural Decision Records or the tech radar . Both are stored in Git, hence a decision log is also captured as part of the Git commit messages. How are architectural decisions taken? Architectural decisions need to mitigate the following information security risks: a component might not fulfill advertised expectations; a component might be abandoned; a component might change direction and deviate from expectations; a component might require a lot of (initial or ongoing) training; a component might not take security seriously; a component might change its license, prohibiting its reuse or making its use expensive. The Compliant Kubernetes architect is overall responsible for this risk. How are these risks mitigated? Before taking in any new component to Compliant Kubernetes, we investigate and evaluate them. We prefer components that are: community-driven open-source projects , to reduce the risk of a component becoming abandoned, changing its license or changing direction in the interest of a single entity; as far as possible, we choose CNCF projects (preferably graduated ones) or projects which are governed by at least 3 different entities; projects with a good security track record , to avoid unexpected security vulnerabilities or delays in fixing security vulnerabilities; as far as possible, we choose projects with a clear security disclosure process and a clear security announcement process; projects that are popular , both from a usage and contribution perspective; as far as possible, we choose projects featuring well-known users and many contributors; projects that rely on technologies that our team is already trained on , to reduce the risk of requiring a lot of (initial or ongoing) training; as far as possible, we choose projects that overlap with the projects already on our tech radar ; projects that are simple to install and manage , to reduce required training and burden on administrators. Often, it is not possible to fulfill the above criteria. In that case, we take the following mitigations: Architectural Decision Records include recommendations on training to be taken by administrators. Closed-source or \"as-a-Service\" alternatives are used, if they are easy to replace thanks to broad API compatibility or standardization. These mitigations may be relaxed for components that are part of alpha or beta features, as these features -- and required components -- can be removed at our discretion. ADRs This log lists the architectural decisions for Compliant Kubernetes. ADR-0000 - Use Markdown Architectural Decision Records ADR-0001 - Use Rook for Storage Orchestrator ADR-0002 - Use Kubespray for Cluster Life-cycle ADR-0003 - Push Metrics via InfluxDB ADR-0004 - Plan for Usage without Wrapper Scripts ADR-0005 - Use Individual SSH Keys ADR-0006 - Use Standard Kubeconfig Mechanisms ADR-0007 - Make Monitoring Forwarders Storage Independent ADR-0008 - Use HostNetwork or LoadBalancer for Ingress ADR-0009 - Use ClusterIssuers for LetsEncrypt ADR-0010 - Run managed services in workload cluster ADR-0011 - Let upstream projects handle CRDs ADR-0012 - Do not persist Dex ADR-0013 - Configure Alerts in On-call Management Tool (e.g., Opsgenie) ADR-0014 - Use bats for testing bash wrappers ADR-0015 - We believe in community-driven open source ADR-0016 - gid=0 is okay, but not by default For new ADRs, please use template.md as basis. More information on MADR is available at https://adr.github.io/madr/ . General information about architectural decision records is available at https://adr.github.io/ . Index Regeneration Pre-requisites: Install npm Install adr-log Install make Run make -C docs/adr","title":"Architectural Decision Log"},{"location":"adr/#architectural-decision-log","text":"","title":"Architectural Decision Log"},{"location":"adr/#mapping-to-iso-27001-controls","text":"A.14.1.1 \"Information security requirements analysis and specification\"","title":"Mapping to ISO 27001 Controls"},{"location":"adr/#what-are-architectural-decisions","text":"Architectural decisions are high-level technical decisions that affect most stakeholders, in particular Compliant Kubernetes developers, administrators and users. A non-exhaustive list of architectural decisions is as follows: adding or removing tools; adding or removing components; changing what component talks to what other component; major (in the SemVer sense) component upgrades. Architectural decisions should be taken as directions to follow for future development and not issues to be fixed immediately.","title":"What are architectural decisions?"},{"location":"adr/#what-triggers-an-architectural-decision","text":"An architectural decision generally starts with one of the following: A new features was requested by product management. An improvement was requested by engineering management. A new risk was discovered, usually by the architect, but also by any stakeholder. A new technology was discovered, that may help with a new feature, an improvement or to mitigate a risk.","title":"What triggers an architectural decision?"},{"location":"adr/#how-are-architectural-decisions-captured","text":"Architectural decisions are captured via Architectural Decision Records or the tech radar . Both are stored in Git, hence a decision log is also captured as part of the Git commit messages.","title":"How are architectural decisions captured?"},{"location":"adr/#how-are-architectural-decisions-taken","text":"Architectural decisions need to mitigate the following information security risks: a component might not fulfill advertised expectations; a component might be abandoned; a component might change direction and deviate from expectations; a component might require a lot of (initial or ongoing) training; a component might not take security seriously; a component might change its license, prohibiting its reuse or making its use expensive. The Compliant Kubernetes architect is overall responsible for this risk.","title":"How are architectural decisions taken?"},{"location":"adr/#how-are-these-risks-mitigated","text":"Before taking in any new component to Compliant Kubernetes, we investigate and evaluate them. We prefer components that are: community-driven open-source projects , to reduce the risk of a component becoming abandoned, changing its license or changing direction in the interest of a single entity; as far as possible, we choose CNCF projects (preferably graduated ones) or projects which are governed by at least 3 different entities; projects with a good security track record , to avoid unexpected security vulnerabilities or delays in fixing security vulnerabilities; as far as possible, we choose projects with a clear security disclosure process and a clear security announcement process; projects that are popular , both from a usage and contribution perspective; as far as possible, we choose projects featuring well-known users and many contributors; projects that rely on technologies that our team is already trained on , to reduce the risk of requiring a lot of (initial or ongoing) training; as far as possible, we choose projects that overlap with the projects already on our tech radar ; projects that are simple to install and manage , to reduce required training and burden on administrators. Often, it is not possible to fulfill the above criteria. In that case, we take the following mitigations: Architectural Decision Records include recommendations on training to be taken by administrators. Closed-source or \"as-a-Service\" alternatives are used, if they are easy to replace thanks to broad API compatibility or standardization. These mitigations may be relaxed for components that are part of alpha or beta features, as these features -- and required components -- can be removed at our discretion.","title":"How are these risks mitigated?"},{"location":"adr/#adrs","text":"This log lists the architectural decisions for Compliant Kubernetes. ADR-0000 - Use Markdown Architectural Decision Records ADR-0001 - Use Rook for Storage Orchestrator ADR-0002 - Use Kubespray for Cluster Life-cycle ADR-0003 - Push Metrics via InfluxDB ADR-0004 - Plan for Usage without Wrapper Scripts ADR-0005 - Use Individual SSH Keys ADR-0006 - Use Standard Kubeconfig Mechanisms ADR-0007 - Make Monitoring Forwarders Storage Independent ADR-0008 - Use HostNetwork or LoadBalancer for Ingress ADR-0009 - Use ClusterIssuers for LetsEncrypt ADR-0010 - Run managed services in workload cluster ADR-0011 - Let upstream projects handle CRDs ADR-0012 - Do not persist Dex ADR-0013 - Configure Alerts in On-call Management Tool (e.g., Opsgenie) ADR-0014 - Use bats for testing bash wrappers ADR-0015 - We believe in community-driven open source ADR-0016 - gid=0 is okay, but not by default For new ADRs, please use template.md as basis. More information on MADR is available at https://adr.github.io/madr/ . General information about architectural decision records is available at https://adr.github.io/ .","title":"ADRs"},{"location":"adr/#index-regeneration","text":"Pre-requisites: Install npm Install adr-log Install make Run make -C docs/adr","title":"Index Regeneration"},{"location":"adr/0000-use-markdown-architectural-decision-records/","text":"Use Markdown Architectural Decision Records Context and Problem Statement We want to record architectural decisions made in this project. Which format and structure should these records follow? Considered Options MADR 2.1.2 \u2013 The Markdown Architectural Decision Records Formless \u2013 No conventions for file format and structure Decision Outcome Chosen option: \"MADR 2.1.2\", because We need to start somewhere, and it's better to have some format than no format. MADR seems to be good enough for our current needs.","title":"Use Markdown Architectural Decision Records"},{"location":"adr/0000-use-markdown-architectural-decision-records/#use-markdown-architectural-decision-records","text":"","title":"Use Markdown Architectural Decision Records"},{"location":"adr/0000-use-markdown-architectural-decision-records/#context-and-problem-statement","text":"We want to record architectural decisions made in this project. Which format and structure should these records follow?","title":"Context and Problem Statement"},{"location":"adr/0000-use-markdown-architectural-decision-records/#considered-options","text":"MADR 2.1.2 \u2013 The Markdown Architectural Decision Records Formless \u2013 No conventions for file format and structure","title":"Considered Options"},{"location":"adr/0000-use-markdown-architectural-decision-records/#decision-outcome","text":"Chosen option: \"MADR 2.1.2\", because We need to start somewhere, and it's better to have some format than no format. MADR seems to be good enough for our current needs.","title":"Decision Outcome"},{"location":"adr/0001-use-rook-storage-orchestrator/","text":"Use Rook for Storage Orchestrator Status: accepted Deciders: Cristian Klein, Lars Larsson, Pradyumna Kashyap, Daniel Harr, Viktor Forsberg, Fredrik Liv Date: 2020-11-16 Context and Problem Statement Compliant Kubernetes has the vision to reduce the compliance burden on multiple clouds (\"Multi-cloud. Open source. Compliant.\"). Many of the cloud providers we target do not have a storage provider or do not have a storage provider that integrates with Kubernetes. How should we support PersistentVolumeClaims in such cases? Decision Drivers Storage Orchestrator needs to be popular and well maintained, so that developer can focus on adding value on top of Kubernetes clusters. Storage Orchestrator needs to be easy to set up, easy to operate and battle-tested, so on-call administrators are not constantly woken up. Storage Orchestrator needs to have reasonable performance. (A local storage provider can deal with high-performance use-cases.) Considered Options Rook GlusterFS Longhorn NFS Storage Provider Decision Outcome Chosen option: \"Rook\", because it is CNCF graduated, hence it is most likely to drive development and adoption long-term. Prady tested it and showed it was easy to use. It supports Ceph as a backend, making it battle-tested. It has reasonable performance. Positive Consequences We no longer need to worry about cloud provider without native storage. Negative Consequences We need to deprecate our NFS storage provider. Some manual steps are required to set up partitions for Rook. These will be automated when the burden justifies it. Pros and Cons of the Options Longhorn Good, because it is a CNCF project. Good, because it is well integrated with Kubernetes. Bad, because it is not the most mature CNCF project in the storage class. Bad, because it was not easy to set up. GlusterFS Good, because it is battle-tested. Bad, because it is not as well integrated with Kubernetes as other projects. Bad, because it is not a CNCF project (driven by Red Hat). NFS Storage Provider Good, because we used it before and we have experience. Bad, because it is a non-redundant, snowflake, brittle solution.","title":"Use Rook for Storage Orchestrator"},{"location":"adr/0001-use-rook-storage-orchestrator/#use-rook-for-storage-orchestrator","text":"Status: accepted Deciders: Cristian Klein, Lars Larsson, Pradyumna Kashyap, Daniel Harr, Viktor Forsberg, Fredrik Liv Date: 2020-11-16","title":"Use Rook for Storage Orchestrator"},{"location":"adr/0001-use-rook-storage-orchestrator/#context-and-problem-statement","text":"Compliant Kubernetes has the vision to reduce the compliance burden on multiple clouds (\"Multi-cloud. Open source. Compliant.\"). Many of the cloud providers we target do not have a storage provider or do not have a storage provider that integrates with Kubernetes. How should we support PersistentVolumeClaims in such cases?","title":"Context and Problem Statement"},{"location":"adr/0001-use-rook-storage-orchestrator/#decision-drivers","text":"Storage Orchestrator needs to be popular and well maintained, so that developer can focus on adding value on top of Kubernetes clusters. Storage Orchestrator needs to be easy to set up, easy to operate and battle-tested, so on-call administrators are not constantly woken up. Storage Orchestrator needs to have reasonable performance. (A local storage provider can deal with high-performance use-cases.)","title":"Decision Drivers"},{"location":"adr/0001-use-rook-storage-orchestrator/#considered-options","text":"Rook GlusterFS Longhorn NFS Storage Provider","title":"Considered Options"},{"location":"adr/0001-use-rook-storage-orchestrator/#decision-outcome","text":"Chosen option: \"Rook\", because it is CNCF graduated, hence it is most likely to drive development and adoption long-term. Prady tested it and showed it was easy to use. It supports Ceph as a backend, making it battle-tested. It has reasonable performance.","title":"Decision Outcome"},{"location":"adr/0001-use-rook-storage-orchestrator/#positive-consequences","text":"We no longer need to worry about cloud provider without native storage.","title":"Positive Consequences"},{"location":"adr/0001-use-rook-storage-orchestrator/#negative-consequences","text":"We need to deprecate our NFS storage provider. Some manual steps are required to set up partitions for Rook. These will be automated when the burden justifies it.","title":"Negative Consequences"},{"location":"adr/0001-use-rook-storage-orchestrator/#pros-and-cons-of-the-options","text":"","title":"Pros and Cons of the Options "},{"location":"adr/0001-use-rook-storage-orchestrator/#longhorn","text":"Good, because it is a CNCF project. Good, because it is well integrated with Kubernetes. Bad, because it is not the most mature CNCF project in the storage class. Bad, because it was not easy to set up.","title":"Longhorn"},{"location":"adr/0001-use-rook-storage-orchestrator/#glusterfs","text":"Good, because it is battle-tested. Bad, because it is not as well integrated with Kubernetes as other projects. Bad, because it is not a CNCF project (driven by Red Hat).","title":"GlusterFS"},{"location":"adr/0001-use-rook-storage-orchestrator/#nfs-storage-provider","text":"Good, because we used it before and we have experience. Bad, because it is a non-redundant, snowflake, brittle solution.","title":"NFS Storage Provider"},{"location":"adr/0002-use-kubespray-for-cluster-lifecycle/","text":"Use Kubespray for Cluster Life-cycle Status: accepted Deciders: Lars, Johan, Cristian, Emil, Viktor, Geoff, Ewnetu, Fredrik (potentially others who attended the architecture meeting, but I can't remember) Date: 2020-11-17 Context and Problem Statement Compliant Kubernetes promises: \"Multi-cloud. Open source. Compliant\". So far, we delivered on our multi-cloud promise by using our in-house ck8s-cluster implementation. This strategy feels unsustainable for two reasons: First, we don't have the resources to catch up and keep up with open source projects in the cluster life-cycle space. Second, we don't want to differentiate on how to set up vanilla Kubernetes cluster, i.e., lower in the Kubernetes stack. Rather we want to differentiate on services on top of vanilla Kubernetes clusters. Decision Drivers We want to differentiate on top of vanilla Kubernetes cluster. We want to be able to run Compliant Kubernetes on top of as many cloud providers as possible. We promise building on top of best-of-breeds open source projets. We want to reduce burden with developing and maintaining our in-house tooling for cluster life-cycle management. Considered Options Rancher kubeadm via in-house tools (ck8s-cluster) kubespray kops Decision Outcome We chose kubespray, because it is best aligned with our interests, both feature- and roadmap-wise. It has a large community and is expected to be well maintained in the future. It uses kubeadm for domain knowledge on how to set up Kubernetes clusters. Positive Consequences We learn how to use a widely-used tool for cluster lifecycle management. We support many cloud providers. We can differentiate on top of vanilla Kubernetes. Negative Consequences We need training on kubespray. We need to port our tooling and practices to kubespray. We need to port compliantkubernetes-apps to work on kubespray. Pros and Cons of the Options Rancher Good, because it provides cluster life-cycle management at scale. Bad, because it creates clusters in an opinionated way, which is insufficiently flexible for our needs. Bad, because it is not a community project, hence entails long-term licensing uncertainty. kubeadm via in-house tool (ck8s-cluster) Good, because we know it and we built it. Good, because it works well for current use-cases. Bad, because it entails a lot of effort to develop and maintain. Bad, because it is lagging behind feature-wise with other cluster life-cycle solutions. kops Good, because it integrates well with the underlying cloud provider (e.g., AWS). Bad, because it supports fewer cloud providers than kubespray. NOTE: In the future, we might want to support compliantkubernetes-apps on top of both kops and kubespray, but this does not seem to bring value just now.","title":"Use Kubespray for Cluster Life-cycle"},{"location":"adr/0002-use-kubespray-for-cluster-lifecycle/#use-kubespray-for-cluster-life-cycle","text":"Status: accepted Deciders: Lars, Johan, Cristian, Emil, Viktor, Geoff, Ewnetu, Fredrik (potentially others who attended the architecture meeting, but I can't remember) Date: 2020-11-17","title":"Use Kubespray for Cluster Life-cycle"},{"location":"adr/0002-use-kubespray-for-cluster-lifecycle/#context-and-problem-statement","text":"Compliant Kubernetes promises: \"Multi-cloud. Open source. Compliant\". So far, we delivered on our multi-cloud promise by using our in-house ck8s-cluster implementation. This strategy feels unsustainable for two reasons: First, we don't have the resources to catch up and keep up with open source projects in the cluster life-cycle space. Second, we don't want to differentiate on how to set up vanilla Kubernetes cluster, i.e., lower in the Kubernetes stack. Rather we want to differentiate on services on top of vanilla Kubernetes clusters.","title":"Context and Problem Statement"},{"location":"adr/0002-use-kubespray-for-cluster-lifecycle/#decision-drivers","text":"We want to differentiate on top of vanilla Kubernetes cluster. We want to be able to run Compliant Kubernetes on top of as many cloud providers as possible. We promise building on top of best-of-breeds open source projets. We want to reduce burden with developing and maintaining our in-house tooling for cluster life-cycle management.","title":"Decision Drivers"},{"location":"adr/0002-use-kubespray-for-cluster-lifecycle/#considered-options","text":"Rancher kubeadm via in-house tools (ck8s-cluster) kubespray kops","title":"Considered Options"},{"location":"adr/0002-use-kubespray-for-cluster-lifecycle/#decision-outcome","text":"We chose kubespray, because it is best aligned with our interests, both feature- and roadmap-wise. It has a large community and is expected to be well maintained in the future. It uses kubeadm for domain knowledge on how to set up Kubernetes clusters.","title":"Decision Outcome"},{"location":"adr/0002-use-kubespray-for-cluster-lifecycle/#positive-consequences","text":"We learn how to use a widely-used tool for cluster lifecycle management. We support many cloud providers. We can differentiate on top of vanilla Kubernetes.","title":"Positive Consequences"},{"location":"adr/0002-use-kubespray-for-cluster-lifecycle/#negative-consequences","text":"We need training on kubespray. We need to port our tooling and practices to kubespray. We need to port compliantkubernetes-apps to work on kubespray.","title":"Negative Consequences"},{"location":"adr/0002-use-kubespray-for-cluster-lifecycle/#pros-and-cons-of-the-options","text":"","title":"Pros and Cons of the Options"},{"location":"adr/0002-use-kubespray-for-cluster-lifecycle/#rancher","text":"Good, because it provides cluster life-cycle management at scale. Bad, because it creates clusters in an opinionated way, which is insufficiently flexible for our needs. Bad, because it is not a community project, hence entails long-term licensing uncertainty.","title":"Rancher"},{"location":"adr/0002-use-kubespray-for-cluster-lifecycle/#kubeadm-via-in-house-tool-ck8s-cluster","text":"Good, because we know it and we built it. Good, because it works well for current use-cases. Bad, because it entails a lot of effort to develop and maintain. Bad, because it is lagging behind feature-wise with other cluster life-cycle solutions.","title":"kubeadm via in-house tool (ck8s-cluster)"},{"location":"adr/0002-use-kubespray-for-cluster-lifecycle/#kops","text":"Good, because it integrates well with the underlying cloud provider (e.g., AWS). Bad, because it supports fewer cloud providers than kubespray. NOTE: In the future, we might want to support compliantkubernetes-apps on top of both kops and kubespray, but this does not seem to bring value just now.","title":"kops"},{"location":"adr/0003-push-metrics-via-influxdb/","text":"Push Metrics via InfluxDB Status: accepted Deciders: Johan, Cristian, Viktor, Emil, Olle, Fredrik Date: 2020-11-19 Context and Problem Statement We want to support workload multi-tenancy, i.e., one service cluster -- hosting the tamper-proof logging environment -- and multiple workload clusters. Currently, the service cluster exposes two end-points for workload clusters: Dex, for authentication; Elastisearch, for pushing logs (append-only). Currently, the service cluster pulls metrics from the workload cluster. This makes it difficult to have multiple workload clusters connected to the same service cluster. Decision Drivers We want to support workload multi-tenancy. We want to untangle the life-cycle of the service cluster and workload cluster. The service cluster acts as a tamper-proof logging environment, hence it should be difficult to tamper with metrics from the workload cluster. Considered Options Service cluster exposes InfluxDB; workload cluster pushes metrics into InfluxDB. Migrate from InfluxDB to Thanos Migrate from InfluxDB to Cortex Decision Outcome We chose to push metrics from the workload cluster to the service cluster via InfluxDB, because it involves the least amount of effort and is sufficient for the current use-cases that we want to support. InfluxDB supports a writer role, which makes overwriting metrics difficult -- unfortunately, not impossible. Positive Consequences All of *.$opsDomain can point to the service cluster workers -- optionally fronted by a load-balancer -- which considerably simplifies setup. Multiple workload clusters can push metrics to the service cluster, which paves the path to workload multi-tenancy. The service cluster can be set up first, followed by one-or-more workload clusters. Workload clusters become more \"cattle\"-ish. Negative Consequences Existing Compliant Kubernetes clusters will need some manual migration steps, in particular changing the prometheus.$opsDomain DNS entry. The service cluster exposes yet another endpoint, which should only be available to workload clusters and not the Internet. HTTP authentication (over HTTPS) feels sufficient for now, but we need a follow-up decision on how to add another layer of protection to these endpoints. The workload clusters will have to properly label their metrics. Although not easy, metrics can be overwritten from the workload cluster. We will improve on this when (a) demand for closing this risk increases, (b) we re-evaluate long-term metrics storage. Pros and Cons of the Options Both Thanos and Cortex seems worthy projects to replace InfluxDB. At the time of this writing, they were both having CNCF Incubating status. The two projects feature a healthy collaboration and are likely to merge in the future. However, right now, migrating away from InfluxDB feels like it adds more cost than benefits. We will reevaluate this decision when InfluxDB is no longer sufficient for our needs.","title":"Push Metrics via InfluxDB"},{"location":"adr/0003-push-metrics-via-influxdb/#push-metrics-via-influxdb","text":"Status: accepted Deciders: Johan, Cristian, Viktor, Emil, Olle, Fredrik Date: 2020-11-19","title":"Push Metrics via InfluxDB"},{"location":"adr/0003-push-metrics-via-influxdb/#context-and-problem-statement","text":"We want to support workload multi-tenancy, i.e., one service cluster -- hosting the tamper-proof logging environment -- and multiple workload clusters. Currently, the service cluster exposes two end-points for workload clusters: Dex, for authentication; Elastisearch, for pushing logs (append-only). Currently, the service cluster pulls metrics from the workload cluster. This makes it difficult to have multiple workload clusters connected to the same service cluster.","title":"Context and Problem Statement"},{"location":"adr/0003-push-metrics-via-influxdb/#decision-drivers","text":"We want to support workload multi-tenancy. We want to untangle the life-cycle of the service cluster and workload cluster. The service cluster acts as a tamper-proof logging environment, hence it should be difficult to tamper with metrics from the workload cluster.","title":"Decision Drivers"},{"location":"adr/0003-push-metrics-via-influxdb/#considered-options","text":"Service cluster exposes InfluxDB; workload cluster pushes metrics into InfluxDB. Migrate from InfluxDB to Thanos Migrate from InfluxDB to Cortex","title":"Considered Options"},{"location":"adr/0003-push-metrics-via-influxdb/#decision-outcome","text":"We chose to push metrics from the workload cluster to the service cluster via InfluxDB, because it involves the least amount of effort and is sufficient for the current use-cases that we want to support. InfluxDB supports a writer role, which makes overwriting metrics difficult -- unfortunately, not impossible.","title":"Decision Outcome"},{"location":"adr/0003-push-metrics-via-influxdb/#positive-consequences","text":"All of *.$opsDomain can point to the service cluster workers -- optionally fronted by a load-balancer -- which considerably simplifies setup. Multiple workload clusters can push metrics to the service cluster, which paves the path to workload multi-tenancy. The service cluster can be set up first, followed by one-or-more workload clusters. Workload clusters become more \"cattle\"-ish.","title":"Positive Consequences"},{"location":"adr/0003-push-metrics-via-influxdb/#negative-consequences","text":"Existing Compliant Kubernetes clusters will need some manual migration steps, in particular changing the prometheus.$opsDomain DNS entry. The service cluster exposes yet another endpoint, which should only be available to workload clusters and not the Internet. HTTP authentication (over HTTPS) feels sufficient for now, but we need a follow-up decision on how to add another layer of protection to these endpoints. The workload clusters will have to properly label their metrics. Although not easy, metrics can be overwritten from the workload cluster. We will improve on this when (a) demand for closing this risk increases, (b) we re-evaluate long-term metrics storage.","title":"Negative Consequences"},{"location":"adr/0003-push-metrics-via-influxdb/#pros-and-cons-of-the-options","text":"Both Thanos and Cortex seems worthy projects to replace InfluxDB. At the time of this writing, they were both having CNCF Incubating status. The two projects feature a healthy collaboration and are likely to merge in the future. However, right now, migrating away from InfluxDB feels like it adds more cost than benefits. We will reevaluate this decision when InfluxDB is no longer sufficient for our needs.","title":"Pros and Cons of the Options"},{"location":"adr/0004-plan-for-usage-without-wrapper-scripts/","text":"Plan for Usage without Wrapper Scripts Status: accepted Deciders: Architecture Meeting Date: 2020-11-24 Context and Problem Statement We frequently write wrapper scripts. They bring the following value: They bind together several tools and make them work together as a whole, e.g., sops and kubectl . They encode domain knowledge and standard operating procedures, e.g., how to add a node, how a cluster should look like, where to find configuration files. They enforce best practices, e.g., encrypt secrets consumed or produced by tools. Unfortunately, wrapper scripts can also bring disadvantages: They make usages that are deviating from the \"good way\" difficult. They risk adding opacity and raise the adoption barrier. People used to the underlying tools may find it difficult to follow how those tools are invoked. They add overhead when adding new features or supporting new use-cases. They raise the learning curve, i.e., newcomers need to learn the wrapper scripts in addition to the underlying tools. Completely abstracting away the underlying tools is unlikely, due to the Law of Leaky Abstractions . Decision Drivers We want to make operations simple, predictable, resilient to human error and scalable. We want to have some predictability in how an environment is set up. We want to make Compliant Kubernetes flexible and agile. Considered Options On one extreme, we can enforce wrapper scripts as the only way forward. This would require significant investment, as these scripts would need to be very powerful and well documented. On the other extreme, we completely \"ban\" wrapper scripts. Decision Outcome We have chosen to keep wrapper scripts in general. However, they need to be written in a way that ensures that our artefacts (e.g., Terraform scripts, Ansible roles, Helmfiles and Helm Charts) are usable without wrapper scripts. Wrapper scripts should also be simple enough so they can be inspected and useful commands can be copy-pasted out. This ensures that said scripts do not need to be \"too\" powerful and \"too\" well documented, but at the same time they do brings the sought after value. This decision applies for new wrapper scripts. We will not rework old wrapper scripts. Positive Consequences The operations team can encode standard operating procedures and scale ways of working. Customer-facing developers can easily reuse artefacts for new use-cases, without significant development effort. Newcomers will (hopefully) find the right trade-off of barriers, depending on whether they are looking for flexibility or predictability. Negative Consequences There will be a constant temptation to do things outside wrapper scripts, which will complicated knowledge sharing, operations and support. When this becomes a significant issue, we will need to draft clear guidelines on what should belong in a wrapper scripts and what not.","title":"Plan for Usage without Wrapper Scripts"},{"location":"adr/0004-plan-for-usage-without-wrapper-scripts/#plan-for-usage-without-wrapper-scripts","text":"Status: accepted Deciders: Architecture Meeting Date: 2020-11-24","title":"Plan for Usage without Wrapper Scripts"},{"location":"adr/0004-plan-for-usage-without-wrapper-scripts/#context-and-problem-statement","text":"We frequently write wrapper scripts. They bring the following value: They bind together several tools and make them work together as a whole, e.g., sops and kubectl . They encode domain knowledge and standard operating procedures, e.g., how to add a node, how a cluster should look like, where to find configuration files. They enforce best practices, e.g., encrypt secrets consumed or produced by tools. Unfortunately, wrapper scripts can also bring disadvantages: They make usages that are deviating from the \"good way\" difficult. They risk adding opacity and raise the adoption barrier. People used to the underlying tools may find it difficult to follow how those tools are invoked. They add overhead when adding new features or supporting new use-cases. They raise the learning curve, i.e., newcomers need to learn the wrapper scripts in addition to the underlying tools. Completely abstracting away the underlying tools is unlikely, due to the Law of Leaky Abstractions .","title":"Context and Problem Statement"},{"location":"adr/0004-plan-for-usage-without-wrapper-scripts/#decision-drivers","text":"We want to make operations simple, predictable, resilient to human error and scalable. We want to have some predictability in how an environment is set up. We want to make Compliant Kubernetes flexible and agile.","title":"Decision Drivers"},{"location":"adr/0004-plan-for-usage-without-wrapper-scripts/#considered-options","text":"On one extreme, we can enforce wrapper scripts as the only way forward. This would require significant investment, as these scripts would need to be very powerful and well documented. On the other extreme, we completely \"ban\" wrapper scripts.","title":"Considered Options"},{"location":"adr/0004-plan-for-usage-without-wrapper-scripts/#decision-outcome","text":"We have chosen to keep wrapper scripts in general. However, they need to be written in a way that ensures that our artefacts (e.g., Terraform scripts, Ansible roles, Helmfiles and Helm Charts) are usable without wrapper scripts. Wrapper scripts should also be simple enough so they can be inspected and useful commands can be copy-pasted out. This ensures that said scripts do not need to be \"too\" powerful and \"too\" well documented, but at the same time they do brings the sought after value. This decision applies for new wrapper scripts. We will not rework old wrapper scripts.","title":"Decision Outcome"},{"location":"adr/0004-plan-for-usage-without-wrapper-scripts/#positive-consequences","text":"The operations team can encode standard operating procedures and scale ways of working. Customer-facing developers can easily reuse artefacts for new use-cases, without significant development effort. Newcomers will (hopefully) find the right trade-off of barriers, depending on whether they are looking for flexibility or predictability.","title":"Positive Consequences"},{"location":"adr/0004-plan-for-usage-without-wrapper-scripts/#negative-consequences","text":"There will be a constant temptation to do things outside wrapper scripts, which will complicated knowledge sharing, operations and support. When this becomes a significant issue, we will need to draft clear guidelines on what should belong in a wrapper scripts and what not.","title":"Negative Consequences"},{"location":"adr/0005-use-individual-ssh-keys/","text":"Use Individual SSH Keys Status: accepted Deciders: Cristian, Fredrik, Olle, Johan Date: 2021-01-28 Technical Story: Do not fiddle with the SSH key Create a process of how we should move to use personal SSH keys Context and Problem Statement Currently, we create per-cluster SSH key pairs, which are shared among administrators. This is problematic from an information security perspective for a few reasons: It reduces the auditability of various actions, e.g., who SSH-ed into the Kubernetes control plane Nodes. It makes credential management challenging, e.g., when onboarding/offboarding administrators. It makes credential rotation challenging, e.g., the new SSH key pair needs to be transmitted to all administrators. It encourages storing the SSH key pair without password protection. It makes it difficult to store SSH key pairs on an exfiltration-proof medium, such as a YubiKey. It violates the Principle of Least Astonishment. Decision Drivers We need to stick to information security best-practices. Considered Options Inject SSH keys via cloud-init. Manage SSH keys via an Ansible role. Decision Outcome We will manage SSH keys via an Ansible role, since it allows rotating/adding/deleting keys without rebooting nodes. Also, it caters to more environments, e.g., BYO-VMs and BYO-metal. The compliantkubernetes-kubespray project will make it easy to configure SSH keys. Bootstrapping The above decision raises a chicken-and-egg problem: Ansible needs SSH access to the nodes, but the SSH access is managed via Ansible. This issue is solved as follows. For cloud deployments, all Terraform providers support injecting at least one public SSH key via cloud-init: AWS Exoscale GCP OpenStack The administrator who creates the cluster bootstraps SSH access by providing their own public SSH key via cloud-init. Then, the Ansible role adds the public SSH keys of the other administrators. BYO-VM and BYO-metal deployments are handled similarly, except that the initial public SSH key is delivered by email/Slack to the VM/metal administrator. Recommendations to Operators Operators should devise procedures for onboarding and offboarding member of the on-call team, as well as rotating SSH keys. The public SSH keys of all on-call administrators could be stored in a repository in a single file with one key per line. The comment of the key should clearly identify the owner. Operator logs (be it stand-alone documents, git or GitOps-like repositories) should clearly list the SSH keys and identities of the administrators configured for each environment. Links ansible.posix.authorized_key Ansible Module","title":"Use Individual SSH Keys"},{"location":"adr/0005-use-individual-ssh-keys/#use-individual-ssh-keys","text":"Status: accepted Deciders: Cristian, Fredrik, Olle, Johan Date: 2021-01-28 Technical Story: Do not fiddle with the SSH key Create a process of how we should move to use personal SSH keys","title":"Use Individual SSH Keys"},{"location":"adr/0005-use-individual-ssh-keys/#context-and-problem-statement","text":"Currently, we create per-cluster SSH key pairs, which are shared among administrators. This is problematic from an information security perspective for a few reasons: It reduces the auditability of various actions, e.g., who SSH-ed into the Kubernetes control plane Nodes. It makes credential management challenging, e.g., when onboarding/offboarding administrators. It makes credential rotation challenging, e.g., the new SSH key pair needs to be transmitted to all administrators. It encourages storing the SSH key pair without password protection. It makes it difficult to store SSH key pairs on an exfiltration-proof medium, such as a YubiKey. It violates the Principle of Least Astonishment.","title":"Context and Problem Statement"},{"location":"adr/0005-use-individual-ssh-keys/#decision-drivers","text":"We need to stick to information security best-practices.","title":"Decision Drivers"},{"location":"adr/0005-use-individual-ssh-keys/#considered-options","text":"Inject SSH keys via cloud-init. Manage SSH keys via an Ansible role.","title":"Considered Options"},{"location":"adr/0005-use-individual-ssh-keys/#decision-outcome","text":"We will manage SSH keys via an Ansible role, since it allows rotating/adding/deleting keys without rebooting nodes. Also, it caters to more environments, e.g., BYO-VMs and BYO-metal. The compliantkubernetes-kubespray project will make it easy to configure SSH keys.","title":"Decision Outcome"},{"location":"adr/0005-use-individual-ssh-keys/#bootstrapping","text":"The above decision raises a chicken-and-egg problem: Ansible needs SSH access to the nodes, but the SSH access is managed via Ansible. This issue is solved as follows. For cloud deployments, all Terraform providers support injecting at least one public SSH key via cloud-init: AWS Exoscale GCP OpenStack The administrator who creates the cluster bootstraps SSH access by providing their own public SSH key via cloud-init. Then, the Ansible role adds the public SSH keys of the other administrators. BYO-VM and BYO-metal deployments are handled similarly, except that the initial public SSH key is delivered by email/Slack to the VM/metal administrator.","title":"Bootstrapping"},{"location":"adr/0005-use-individual-ssh-keys/#recommendations-to-operators","text":"Operators should devise procedures for onboarding and offboarding member of the on-call team, as well as rotating SSH keys. The public SSH keys of all on-call administrators could be stored in a repository in a single file with one key per line. The comment of the key should clearly identify the owner. Operator logs (be it stand-alone documents, git or GitOps-like repositories) should clearly list the SSH keys and identities of the administrators configured for each environment.","title":"Recommendations to Operators"},{"location":"adr/0005-use-individual-ssh-keys/#links","text":"ansible.posix.authorized_key Ansible Module","title":"Links"},{"location":"adr/0006-use-standard-kubeconfig-mechanisms/","text":"Use Standard Kubeconfig Mechanisms Status: accepted Deciders: Compliant Kubernetes Architecture Meeing Date: 2021-02-02 Context and Problem Statement To increase adoption of Compliant Kubernetes, we were asked to observe the Principle of Least Astonishment . Currently, Compliant Kubernetes's handing of kubeconfig is astonishing. Most tools in the ecosystem use the standard KUBECONFIG environment variable and kubecontext implemented in the client-go library. These tools leave it up to the user to set KUBECONFIG or use the default ~/.kube/config . Similarly, there is a default kubecontext which can be overwritten via command-line. Tools that get cluster credentials generate a context related to the name of the cluster. Tools that behave as such include: gcloud container clusters get-credentials az aks get-credentials kops helmfile helm kubectl fluxctl Decision Drivers Compliant Kubernetes needs to observe the Principle of Least Astonishment. Compliant Kubernetes needs to be compatible with various \"underlying\" vanilla Kubernetes tools. Compliant Kubernetes needs to be usable with various tools \"on top\". Considered Options Current solution, i.e., scripts wrapping kubeconfigs in sops which then execute \"fixed\" commands, like helmfile , helm and kubectl . \"Lighter\" scripts wrapping and unwrapping kubeconfig, allowing administrators to run helmfile , helm and kubectl as the administrator sees fit. Use standard kubeconfig mechanism. Decision Outcome We chose using standard kubeconfig mechanism, because it improves integration both with tools \"below\" Compliant Kubernetas and \"on top\" of Compliant Kubernetes. Tools that produce Kubernetes contexts are expected to use an approach similar to kubectl config set-cluster , set-credentials and set-context . The name of the cluster, user and context should be derived from the name of the cluster. Tools that consume Kubernetes contexts are expected to use an approach similar to kubectl , helm or helmfile (see links below). Links Organizing Cluster Access Using kubeconfig Files kubectx / kubens","title":"Use Standard Kubeconfig Mechanisms"},{"location":"adr/0006-use-standard-kubeconfig-mechanisms/#use-standard-kubeconfig-mechanisms","text":"Status: accepted Deciders: Compliant Kubernetes Architecture Meeing Date: 2021-02-02","title":"Use Standard Kubeconfig Mechanisms"},{"location":"adr/0006-use-standard-kubeconfig-mechanisms/#context-and-problem-statement","text":"To increase adoption of Compliant Kubernetes, we were asked to observe the Principle of Least Astonishment . Currently, Compliant Kubernetes's handing of kubeconfig is astonishing. Most tools in the ecosystem use the standard KUBECONFIG environment variable and kubecontext implemented in the client-go library. These tools leave it up to the user to set KUBECONFIG or use the default ~/.kube/config . Similarly, there is a default kubecontext which can be overwritten via command-line. Tools that get cluster credentials generate a context related to the name of the cluster. Tools that behave as such include: gcloud container clusters get-credentials az aks get-credentials kops helmfile helm kubectl fluxctl","title":"Context and Problem Statement"},{"location":"adr/0006-use-standard-kubeconfig-mechanisms/#decision-drivers","text":"Compliant Kubernetes needs to observe the Principle of Least Astonishment. Compliant Kubernetes needs to be compatible with various \"underlying\" vanilla Kubernetes tools. Compliant Kubernetes needs to be usable with various tools \"on top\".","title":"Decision Drivers"},{"location":"adr/0006-use-standard-kubeconfig-mechanisms/#considered-options","text":"Current solution, i.e., scripts wrapping kubeconfigs in sops which then execute \"fixed\" commands, like helmfile , helm and kubectl . \"Lighter\" scripts wrapping and unwrapping kubeconfig, allowing administrators to run helmfile , helm and kubectl as the administrator sees fit. Use standard kubeconfig mechanism.","title":"Considered Options"},{"location":"adr/0006-use-standard-kubeconfig-mechanisms/#decision-outcome","text":"We chose using standard kubeconfig mechanism, because it improves integration both with tools \"below\" Compliant Kubernetas and \"on top\" of Compliant Kubernetes. Tools that produce Kubernetes contexts are expected to use an approach similar to kubectl config set-cluster , set-credentials and set-context . The name of the cluster, user and context should be derived from the name of the cluster. Tools that consume Kubernetes contexts are expected to use an approach similar to kubectl , helm or helmfile (see links below).","title":"Decision Outcome"},{"location":"adr/0006-use-standard-kubeconfig-mechanisms/#links","text":"Organizing Cluster Access Using kubeconfig Files kubectx / kubens","title":"Links"},{"location":"adr/0007-make-monitoring-forwarders-storage-independent/","text":"Make Monitoring Forwarders Storage Independent Status: accepted Deciders: Axel, Cristian, Fredrik, Johan, Olle, Viktor Date: 2021-02-09 Context and Problem Statement In the context of this ADR, forwarders refers to any components that are necessary to forward monitoring information -- specifically traces, metrics and logs -- to some monitoring database. As of February 2021, Compliant Kubernetes employs two projects as forwarders: Prometheus for metrics forwarding; fluentd for log forwarding. Similarly, two projects are employed as monitoring databases: InfluxDB for metrics; Elasticsearch for logs. Overall, the monitoring system needs to be one order of magnitude more resilient than the monitored system. Forwarders improve the resilience of the monitoring system by providing buffering: In case the database is under maintenance or down, the buffer of the forwarders will ensure that no monitoring information is lost. Hence, forwarders are subject to the following tensions: More buffering implies storage, which make the forwarders vulnerable to storage outages (e.g., disk full, CSI hiccups); Less buffering implies higher risk of losing monitoring information when the database is under maintenance or down. Decision Drivers We want a robust monitoring system. We want to monitor the storage system. We want VM-template-based rendering of the workload cluster, which implies no cloud native storage integration. We want to make it easier to \"cleanup and start from a known good state\". We want to have self-healing and avoid manual actions after failure. We want to be able to find the root cause of an incident quickly. We want to run as many components non-root as possible and tightly integrate with securityContext . Considered Options Use underlying storage provider for increased buffering resilience ( current approach ). Use Local Persistent Volumes . Use emptyDir volumes. Use hostPath volumes. Decision Outcome Chosen option: emptyDir for Prometheus as forwarder, because it allows monitoring of the storage system in some cases (e.g. Rook) and can redeploy automatically after node failure. It also keeps the complexity down without much risk of data loss. Fluentd as forwarder is deployed via DaemonSet. Both, emptyDir and hostPath can be used. Positive Consequences We can monitor the storage system. Failure of the storage system does not affect monitoring forwarder. Forwarder can be easier deployed \"fresh\". Negative Consequences Buffered monitoring information is lost if node is lost. emptyDir can cause disk pressure. This can be handled by alerting on low disk space. Pros and Cons of the Options Use underlying storage provider Good, because the forwarder can be restarted on any node. Good, because the buffer can be large. Good, because no buffered monitoring information is lost if a node goes down. Good, because buffered monitoring information is preserved if the forwarder is redeployed. Bad, because non-node-local storage is generally slower. Note, however, that at least SafeSpring and CityCloud use a central Ceph storage cluster for the VM's boot disk, which wipes out node-local's storage advantage.) Bad, because the forwarder will fail if storage provider goes down. This is especially problematic for Exoscale, bare-metal and BYO-VMs. Bad, because the forwarder cannot monitor the storage provider (circular dependency). Bad, because setting right ownership requires init containers or alpha features . Use Local Persistent Volumes Bad, because the forwarder cannot be restarted on any node without manual action: \"if a node becomes unhealthy, then the local volume becomes inaccessible by the pod. The pod using this volume is unable to run.\". Bad, because the amount of forwarding depends on the node's local disk size. Bad, because buffered monitoring information is lost if the forwarder's node goes down. Good, because buffered monitoring information is preserved if the forwarder is redeployed. Good, because node-local storage is generally faster. Good, because the forwarder will survive failure of storage provider. Good, because the forwarder can monitor the storage provider (no circular dependency). Bad, because local persistent storage requires an additional configuration step. Bad, because setting right ownership requires init containers or alpha features . Use emptyDir Good, because the forwarder can be restarted on any node without manual action. Bad, because the amount of forwarding depends on the node's local disk size. Bad, because buffered monitoring information is lost if the forwarder's node goes down. Bad, because buffered monitoring information is lost if the forwarder is (not carefully enough) redeployed. Good, because node-local storage is generally faster. Good, because the forwarder will survive failure of storage provider. Good, because the forwarder can monitor the storage provider (no circular dependency). Good, because works out of the box. Good, because it integrates nicely with securityContext . Use hostPath Similar to Local Persistent Volumes, but Worse, because if the forwarder is redeployed on a new node, buffering information may appear/disappear. Better, because it requires no extra storage provider configuration. Links Prometheus Operator Storage","title":"Make Monitoring Forwarders Storage Independent"},{"location":"adr/0007-make-monitoring-forwarders-storage-independent/#make-monitoring-forwarders-storage-independent","text":"Status: accepted Deciders: Axel, Cristian, Fredrik, Johan, Olle, Viktor Date: 2021-02-09","title":"Make Monitoring Forwarders Storage Independent"},{"location":"adr/0007-make-monitoring-forwarders-storage-independent/#context-and-problem-statement","text":"In the context of this ADR, forwarders refers to any components that are necessary to forward monitoring information -- specifically traces, metrics and logs -- to some monitoring database. As of February 2021, Compliant Kubernetes employs two projects as forwarders: Prometheus for metrics forwarding; fluentd for log forwarding. Similarly, two projects are employed as monitoring databases: InfluxDB for metrics; Elasticsearch for logs. Overall, the monitoring system needs to be one order of magnitude more resilient than the monitored system. Forwarders improve the resilience of the monitoring system by providing buffering: In case the database is under maintenance or down, the buffer of the forwarders will ensure that no monitoring information is lost. Hence, forwarders are subject to the following tensions: More buffering implies storage, which make the forwarders vulnerable to storage outages (e.g., disk full, CSI hiccups); Less buffering implies higher risk of losing monitoring information when the database is under maintenance or down.","title":"Context and Problem Statement"},{"location":"adr/0007-make-monitoring-forwarders-storage-independent/#decision-drivers","text":"We want a robust monitoring system. We want to monitor the storage system. We want VM-template-based rendering of the workload cluster, which implies no cloud native storage integration. We want to make it easier to \"cleanup and start from a known good state\". We want to have self-healing and avoid manual actions after failure. We want to be able to find the root cause of an incident quickly. We want to run as many components non-root as possible and tightly integrate with securityContext .","title":"Decision Drivers"},{"location":"adr/0007-make-monitoring-forwarders-storage-independent/#considered-options","text":"Use underlying storage provider for increased buffering resilience ( current approach ). Use Local Persistent Volumes . Use emptyDir volumes. Use hostPath volumes.","title":"Considered Options"},{"location":"adr/0007-make-monitoring-forwarders-storage-independent/#decision-outcome","text":"Chosen option: emptyDir for Prometheus as forwarder, because it allows monitoring of the storage system in some cases (e.g. Rook) and can redeploy automatically after node failure. It also keeps the complexity down without much risk of data loss. Fluentd as forwarder is deployed via DaemonSet. Both, emptyDir and hostPath can be used.","title":"Decision Outcome"},{"location":"adr/0007-make-monitoring-forwarders-storage-independent/#positive-consequences","text":"We can monitor the storage system. Failure of the storage system does not affect monitoring forwarder. Forwarder can be easier deployed \"fresh\".","title":"Positive Consequences"},{"location":"adr/0007-make-monitoring-forwarders-storage-independent/#negative-consequences","text":"Buffered monitoring information is lost if node is lost. emptyDir can cause disk pressure. This can be handled by alerting on low disk space.","title":"Negative Consequences"},{"location":"adr/0007-make-monitoring-forwarders-storage-independent/#pros-and-cons-of-the-options","text":"","title":"Pros and Cons of the Options"},{"location":"adr/0007-make-monitoring-forwarders-storage-independent/#use-underlying-storage-provider","text":"Good, because the forwarder can be restarted on any node. Good, because the buffer can be large. Good, because no buffered monitoring information is lost if a node goes down. Good, because buffered monitoring information is preserved if the forwarder is redeployed. Bad, because non-node-local storage is generally slower. Note, however, that at least SafeSpring and CityCloud use a central Ceph storage cluster for the VM's boot disk, which wipes out node-local's storage advantage.) Bad, because the forwarder will fail if storage provider goes down. This is especially problematic for Exoscale, bare-metal and BYO-VMs. Bad, because the forwarder cannot monitor the storage provider (circular dependency). Bad, because setting right ownership requires init containers or alpha features .","title":"Use underlying storage provider"},{"location":"adr/0007-make-monitoring-forwarders-storage-independent/#use-local-persistent-volumes","text":"Bad, because the forwarder cannot be restarted on any node without manual action: \"if a node becomes unhealthy, then the local volume becomes inaccessible by the pod. The pod using this volume is unable to run.\". Bad, because the amount of forwarding depends on the node's local disk size. Bad, because buffered monitoring information is lost if the forwarder's node goes down. Good, because buffered monitoring information is preserved if the forwarder is redeployed. Good, because node-local storage is generally faster. Good, because the forwarder will survive failure of storage provider. Good, because the forwarder can monitor the storage provider (no circular dependency). Bad, because local persistent storage requires an additional configuration step. Bad, because setting right ownership requires init containers or alpha features .","title":"Use Local Persistent Volumes"},{"location":"adr/0007-make-monitoring-forwarders-storage-independent/#use-emptydir","text":"Good, because the forwarder can be restarted on any node without manual action. Bad, because the amount of forwarding depends on the node's local disk size. Bad, because buffered monitoring information is lost if the forwarder's node goes down. Bad, because buffered monitoring information is lost if the forwarder is (not carefully enough) redeployed. Good, because node-local storage is generally faster. Good, because the forwarder will survive failure of storage provider. Good, because the forwarder can monitor the storage provider (no circular dependency). Good, because works out of the box. Good, because it integrates nicely with securityContext .","title":"Use emptyDir"},{"location":"adr/0007-make-monitoring-forwarders-storage-independent/#use-hostpath","text":"Similar to Local Persistent Volumes, but Worse, because if the forwarder is redeployed on a new node, buffering information may appear/disappear. Better, because it requires no extra storage provider configuration.","title":"Use hostPath"},{"location":"adr/0007-make-monitoring-forwarders-storage-independent/#links","text":"Prometheus Operator Storage","title":"Links"},{"location":"adr/0008-use-hostnetwork-or-loadbalancer-for-ingress/","text":"Use HostNetwork or LoadBalancer for Ingress Status: accepted Deciders: Axel, Cristian, Fredrik, Johan, Olle, Viktor Date: 2021-02-09 Technical Story: Ingress configuration Context and Problem Statement Many regulations require traffic to be encrypted over public Internet. Compliant Kubernetes solves this problem via an Ingress controller and cert-manager . As of February 2021, Compliant Kubernetes comes by default with nginx-ingress , but Ambassador is planned as an alternative. The question is, how does traffic arrive at the Ingress controller? Decision Drivers We want to obey the Principle of Least Astonishment . We want to cater to hybrid cloud deployments, including bare-metal ones, which might lack support for Kubernetes-controlled load balancer . Some deployments, e.g., Bring-Your-Own VMs, might not allow integration with the underlying load balancer. We want to keep things simple. Considered Options Via the host network , i.e., some workers expose the Ingress controller on their port 80 and 443. Over a NodePort service , i.e., kube-proxy exposes the Ingress controller on a port between 30000-32767 on each worker. As a Service Type LoadBalancer , i.e., above plus Kubernetes provisions a load balancer via Service controller . Decision Outcome Chosen options: Use host network if Kubernetes-controlled load balancer is unavailable or undesired. If necessary, front the worker nodes with a manual or Terraform-controlled load-balancer. This includes: Where load-balancing does not add value, e.g., if a deployment is planned to have only a single-node or single-worker for the foreseeable future: Point the DNS entry to the worker IP instead. Exoscale currently falls in this category, due to its Kubernetes integration being rather recent. SafeSpring falls in this category, since it is missing load balancers. If the cloud provider is missing a storage controller, it might be undesirable to perform integration \"just\" for load-balancing. Use Service Type LoadBalancer when available. This includes: AWS, Azure, GCP and CityCloud. Additional considerations: This means that, generally, it will not be possible to set up the correct DNS entries until after we apply Compliant Kubernetes Apps. There is a risk for \"the Internet\" -- LetsEncrypt specifically -- to perform DNS lookups too soon and cause negative DNS caches with a long lifetime. Therefore, placeholder IP addresses must be used, e.g.: *.$BASE_DOMAIN 60s A 203.0.113.123 *.ops.$BASE_DOMAIN 60s A 203.0.113.123 203.0.113.123 is in TEST-NET-3 and okay to use as placeholder. This approach is inspired by kops and should not feel astonishing. Positive Consequences We make the best of each cloud provider. Obeys principle of least astonishment. We do not add a load balancer \"just because\". Negative Consequences Complexity is a bit increased, however, this feels like essential complexity. Links Cloud Controller Manager Ingress Nginx: Bare Metal Considerations","title":"Use HostNetwork or LoadBalancer for Ingress"},{"location":"adr/0008-use-hostnetwork-or-loadbalancer-for-ingress/#use-hostnetwork-or-loadbalancer-for-ingress","text":"Status: accepted Deciders: Axel, Cristian, Fredrik, Johan, Olle, Viktor Date: 2021-02-09 Technical Story: Ingress configuration","title":"Use HostNetwork or LoadBalancer for Ingress"},{"location":"adr/0008-use-hostnetwork-or-loadbalancer-for-ingress/#context-and-problem-statement","text":"Many regulations require traffic to be encrypted over public Internet. Compliant Kubernetes solves this problem via an Ingress controller and cert-manager . As of February 2021, Compliant Kubernetes comes by default with nginx-ingress , but Ambassador is planned as an alternative. The question is, how does traffic arrive at the Ingress controller?","title":"Context and Problem Statement"},{"location":"adr/0008-use-hostnetwork-or-loadbalancer-for-ingress/#decision-drivers","text":"We want to obey the Principle of Least Astonishment . We want to cater to hybrid cloud deployments, including bare-metal ones, which might lack support for Kubernetes-controlled load balancer . Some deployments, e.g., Bring-Your-Own VMs, might not allow integration with the underlying load balancer. We want to keep things simple.","title":"Decision Drivers"},{"location":"adr/0008-use-hostnetwork-or-loadbalancer-for-ingress/#considered-options","text":"Via the host network , i.e., some workers expose the Ingress controller on their port 80 and 443. Over a NodePort service , i.e., kube-proxy exposes the Ingress controller on a port between 30000-32767 on each worker. As a Service Type LoadBalancer , i.e., above plus Kubernetes provisions a load balancer via Service controller .","title":"Considered Options"},{"location":"adr/0008-use-hostnetwork-or-loadbalancer-for-ingress/#decision-outcome","text":"Chosen options: Use host network if Kubernetes-controlled load balancer is unavailable or undesired. If necessary, front the worker nodes with a manual or Terraform-controlled load-balancer. This includes: Where load-balancing does not add value, e.g., if a deployment is planned to have only a single-node or single-worker for the foreseeable future: Point the DNS entry to the worker IP instead. Exoscale currently falls in this category, due to its Kubernetes integration being rather recent. SafeSpring falls in this category, since it is missing load balancers. If the cloud provider is missing a storage controller, it might be undesirable to perform integration \"just\" for load-balancing. Use Service Type LoadBalancer when available. This includes: AWS, Azure, GCP and CityCloud. Additional considerations: This means that, generally, it will not be possible to set up the correct DNS entries until after we apply Compliant Kubernetes Apps. There is a risk for \"the Internet\" -- LetsEncrypt specifically -- to perform DNS lookups too soon and cause negative DNS caches with a long lifetime. Therefore, placeholder IP addresses must be used, e.g.: *.$BASE_DOMAIN 60s A 203.0.113.123 *.ops.$BASE_DOMAIN 60s A 203.0.113.123 203.0.113.123 is in TEST-NET-3 and okay to use as placeholder. This approach is inspired by kops and should not feel astonishing.","title":"Decision Outcome"},{"location":"adr/0008-use-hostnetwork-or-loadbalancer-for-ingress/#positive-consequences","text":"We make the best of each cloud provider. Obeys principle of least astonishment. We do not add a load balancer \"just because\".","title":"Positive Consequences"},{"location":"adr/0008-use-hostnetwork-or-loadbalancer-for-ingress/#negative-consequences","text":"Complexity is a bit increased, however, this feels like essential complexity.","title":"Negative Consequences"},{"location":"adr/0008-use-hostnetwork-or-loadbalancer-for-ingress/#links","text":"Cloud Controller Manager Ingress Nginx: Bare Metal Considerations","title":"Links"},{"location":"adr/0009-use-cluster-issuers-for-letsencrypt/","text":"Use ClusterIssuers for LetsEncrypt Status: accepted Deciders: Cristian, Lennart Date: 2021-02-26 Technical Story: Make apps less fragile Context and Problem Statement Data protection regulations require encrypting network traffic over public networks, e.g., via HTTPS. This requires provisioning and rotating TLS certificates. To automate this task, we use the cert-manager , which automates provisioning and rotation of TLS certificates from Let's Encrypt . There are two ways to configure Let's Encrypt as an issuers for cert-manager: Issuer and ClusterIssuer . The former is namespaced, whereas the latter is cluster-wide. Should we use Issuer or ClusterIssuer? Decision Drivers We want to make compliantkubernetes-apps less fragile, and LetsEncrypt ratelimiting is a cause of fragility. We want to make it easy for users to get started with Compliant Kubernetes in a \"secure by default\" manner. We want to have a clear separation between user and administrator resources, responsibilities and privileges. We want to keep the option open for \"light\" renderings, i.e., a single Kubernetes clusters that hosts both service cluster and workload cluster components. Considered Options Use one Issuer per namespace; users need to install their own Issuers in the workload clusters. Use ClusterIssuer in service cluster; let users install Issuers in the workload clusters as required. Use ClusterIssuer in both service cluster and workload cluster(s). Decision Outcome Chosen option: \"Use ClusterIssuers in the service cluster; optionally enable ClusterIssuers in the workload cluster(s)\", because it reduces fragility, clarifies responsibilities, makes it easy to get started securely. Each cluster is configured with an optional ClusterIssuer called letsencrypt-prod for LetsEncrypt production and letsencrypt-staging for LetsEncrypt staging. The email address for the ClusterIssuers is configured by the administrator. Recommendations to Operators Direct LetsEncrypt emails to a \"logging\" mailbox Although LetsEncrypt does not require an email address, cert-managers seems to require all ClusterIssuers/Issuers to be configured with a syntactically valid email address. Said email address will receive notifications when certificates are close to expiry. Given that Compliant Kubernetes comes with Cryptography dashboards, these emails do not seem useful. Hence, ClusterIssuer emails should be directed to an address that has \"logging\" but not \"alerting\" status. Separate registered domains LetsEncrypt production has a rate limit of 50 certificates per week per registered domain . For example, if awesome-website.workload-cluster.environment.elastisys.se points to the workload cluster's Ingress controller, then an excessive creation and destruction of Ingress resources may trigger rate limiting for all of elastisys.se . It is therefore advisable to: Use separate registered domains for development and production environments. Use separate registered domains for workload cluster(s) and the service cluster, or restrict which Ingress resources can be created by the user. Note that, the rate limiting risk exists with both Issuers and ClusterIssuers and was not introduced by this ADR.","title":"Use ClusterIssuers for LetsEncrypt"},{"location":"adr/0009-use-cluster-issuers-for-letsencrypt/#use-clusterissuers-for-letsencrypt","text":"Status: accepted Deciders: Cristian, Lennart Date: 2021-02-26 Technical Story: Make apps less fragile","title":"Use ClusterIssuers for LetsEncrypt"},{"location":"adr/0009-use-cluster-issuers-for-letsencrypt/#context-and-problem-statement","text":"Data protection regulations require encrypting network traffic over public networks, e.g., via HTTPS. This requires provisioning and rotating TLS certificates. To automate this task, we use the cert-manager , which automates provisioning and rotation of TLS certificates from Let's Encrypt . There are two ways to configure Let's Encrypt as an issuers for cert-manager: Issuer and ClusterIssuer . The former is namespaced, whereas the latter is cluster-wide. Should we use Issuer or ClusterIssuer?","title":"Context and Problem Statement"},{"location":"adr/0009-use-cluster-issuers-for-letsencrypt/#decision-drivers","text":"We want to make compliantkubernetes-apps less fragile, and LetsEncrypt ratelimiting is a cause of fragility. We want to make it easy for users to get started with Compliant Kubernetes in a \"secure by default\" manner. We want to have a clear separation between user and administrator resources, responsibilities and privileges. We want to keep the option open for \"light\" renderings, i.e., a single Kubernetes clusters that hosts both service cluster and workload cluster components.","title":"Decision Drivers"},{"location":"adr/0009-use-cluster-issuers-for-letsencrypt/#considered-options","text":"Use one Issuer per namespace; users need to install their own Issuers in the workload clusters. Use ClusterIssuer in service cluster; let users install Issuers in the workload clusters as required. Use ClusterIssuer in both service cluster and workload cluster(s).","title":"Considered Options"},{"location":"adr/0009-use-cluster-issuers-for-letsencrypt/#decision-outcome","text":"Chosen option: \"Use ClusterIssuers in the service cluster; optionally enable ClusterIssuers in the workload cluster(s)\", because it reduces fragility, clarifies responsibilities, makes it easy to get started securely. Each cluster is configured with an optional ClusterIssuer called letsencrypt-prod for LetsEncrypt production and letsencrypt-staging for LetsEncrypt staging. The email address for the ClusterIssuers is configured by the administrator.","title":"Decision Outcome"},{"location":"adr/0009-use-cluster-issuers-for-letsencrypt/#recommendations-to-operators","text":"","title":"Recommendations to Operators"},{"location":"adr/0009-use-cluster-issuers-for-letsencrypt/#direct-letsencrypt-emails-to-a-logging-mailbox","text":"Although LetsEncrypt does not require an email address, cert-managers seems to require all ClusterIssuers/Issuers to be configured with a syntactically valid email address. Said email address will receive notifications when certificates are close to expiry. Given that Compliant Kubernetes comes with Cryptography dashboards, these emails do not seem useful. Hence, ClusterIssuer emails should be directed to an address that has \"logging\" but not \"alerting\" status.","title":"Direct LetsEncrypt emails to a \"logging\" mailbox"},{"location":"adr/0009-use-cluster-issuers-for-letsencrypt/#separate-registered-domains","text":"LetsEncrypt production has a rate limit of 50 certificates per week per registered domain . For example, if awesome-website.workload-cluster.environment.elastisys.se points to the workload cluster's Ingress controller, then an excessive creation and destruction of Ingress resources may trigger rate limiting for all of elastisys.se . It is therefore advisable to: Use separate registered domains for development and production environments. Use separate registered domains for workload cluster(s) and the service cluster, or restrict which Ingress resources can be created by the user. Note that, the rate limiting risk exists with both Issuers and ClusterIssuers and was not introduced by this ADR.","title":"Separate registered domains"},{"location":"adr/0010-run-managed-services-in-workload-cluster/","text":"Run managed services in workload cluster Status: proposed. This ADR did not reach consensus, with strong arguments on both sides. However, due to needing a decision in a timely manner, this ADR is actually followed. Therefore, this ADR serves both for visibility and to document a fait accompli . Deciders: Cristian Date: 2021-04-29 Context and Problem Statement To truly offer our users an option to run containerized workloads in EU jurisdiction, they also need additional managed services, like databases, message queues, caches, etc. Where should these run? Decision Drivers Some of these services are chatty and need low latency. Some of these services might assume trusted clients over a trusted network. We want to make it easy to run these services with regulatory compliance in mind, e.g., we should be able to reuse Compliant Kubernetes features around monitoring, logging, access control and network segregation. We want to make it difficult for Compliant Kubernetes users to negatively affect managed services. We want to keep support for multiple workload cluster, i.e., application multi-tenancy. Many cloud providers do not support Service Type LoadBalancer, which complicates exposing non-HTTP services outside a Kubernetes cluster. Service cluster might not exist in a future packaging of Compliant Kubernetes. Considered Options Run managed services in workload cluster Run managed services in service cluster Run managed services in yet another cluster Decision Outcome Chosen option: \"run managed services in workload cluster\". Positive Consequences Latency is minimized: The application consuming the managed service is close to the managed service, without needing to go through intermediate software components, such as a TCP Ingress controller. NetworkPolicies can be reused for communication segregation. OpenID and RBAC in the workload cluster can be reused for user access control. Kubernetes audit log can be re-used for auditing user access managed services. Such access is required, e.g., for manual database migrations and \"rare\" operations like GDPR data correction requests. Ease of exposition: No need for Service Type LoadBalancer, which is not supported on all cloud providers. Negative Consequences Blurs separation of responsibilities between user and administrator. The managed service is easier impacted by user misusage, e.g., bringing a Node into OOM. Workload cluster can no longer be deployed with \u201cfree for all\u201d security. Operators need to push and fight back against loose access control. Links Service Type LoadBalancer Exposing TCP and UDP services with ingress-nginx Redis Security","title":"Run managed services in workload cluster"},{"location":"adr/0010-run-managed-services-in-workload-cluster/#run-managed-services-in-workload-cluster","text":"Status: proposed. This ADR did not reach consensus, with strong arguments on both sides. However, due to needing a decision in a timely manner, this ADR is actually followed. Therefore, this ADR serves both for visibility and to document a fait accompli . Deciders: Cristian Date: 2021-04-29","title":"Run managed services in workload cluster"},{"location":"adr/0010-run-managed-services-in-workload-cluster/#context-and-problem-statement","text":"To truly offer our users an option to run containerized workloads in EU jurisdiction, they also need additional managed services, like databases, message queues, caches, etc. Where should these run?","title":"Context and Problem Statement"},{"location":"adr/0010-run-managed-services-in-workload-cluster/#decision-drivers","text":"Some of these services are chatty and need low latency. Some of these services might assume trusted clients over a trusted network. We want to make it easy to run these services with regulatory compliance in mind, e.g., we should be able to reuse Compliant Kubernetes features around monitoring, logging, access control and network segregation. We want to make it difficult for Compliant Kubernetes users to negatively affect managed services. We want to keep support for multiple workload cluster, i.e., application multi-tenancy. Many cloud providers do not support Service Type LoadBalancer, which complicates exposing non-HTTP services outside a Kubernetes cluster. Service cluster might not exist in a future packaging of Compliant Kubernetes.","title":"Decision Drivers"},{"location":"adr/0010-run-managed-services-in-workload-cluster/#considered-options","text":"Run managed services in workload cluster Run managed services in service cluster Run managed services in yet another cluster","title":"Considered Options"},{"location":"adr/0010-run-managed-services-in-workload-cluster/#decision-outcome","text":"Chosen option: \"run managed services in workload cluster\".","title":"Decision Outcome"},{"location":"adr/0010-run-managed-services-in-workload-cluster/#positive-consequences","text":"Latency is minimized: The application consuming the managed service is close to the managed service, without needing to go through intermediate software components, such as a TCP Ingress controller. NetworkPolicies can be reused for communication segregation. OpenID and RBAC in the workload cluster can be reused for user access control. Kubernetes audit log can be re-used for auditing user access managed services. Such access is required, e.g., for manual database migrations and \"rare\" operations like GDPR data correction requests. Ease of exposition: No need for Service Type LoadBalancer, which is not supported on all cloud providers.","title":"Positive Consequences"},{"location":"adr/0010-run-managed-services-in-workload-cluster/#negative-consequences","text":"Blurs separation of responsibilities between user and administrator. The managed service is easier impacted by user misusage, e.g., bringing a Node into OOM. Workload cluster can no longer be deployed with \u201cfree for all\u201d security. Operators need to push and fight back against loose access control.","title":"Negative Consequences"},{"location":"adr/0010-run-managed-services-in-workload-cluster/#links","text":"Service Type LoadBalancer Exposing TCP and UDP services with ingress-nginx Redis Security","title":"Links"},{"location":"adr/0011-let-upstream-projects-handle-crds/","text":"Let upstream projects handle CRDs Status: accepted Deciders: Compliant Kubernetes Arch Meeting Date: 2021-04-29 Technical Story: #446 #369 #391 #402 #436 . Context and Problem Statement CustomResourceDefinitions (CRDs) are tricky. They are essentially a mechanism to change the API of Kubernetes. Helm 2 had zero support for CRDs. Helm 3 has support for installing CRDs , but not upgrading them. How should we handle CRDs? Decision Drivers CRDs add complexity and need to be treated specially. Generally need to \u201ctrim fat\u201d and rely on upstream. Considered Options Install and upgrade CRDs as part of the bootstrap step, which is a Helm 2 legacy. Rely on whatever mechanism is proposed by upstream Helm Charts. Decision Outcome Chosen option: \"Rely on upstream\", because it trims fat and reduces astonishment. At installation, rely on upstream's approach to install CRDs (see below). At upgrade, propagate upstream migration steps in CK8s migration steps in each release notes. An issue template was created to ensure we won't forget. Since we \"vendor in\" all Charts, CRDs can be discovered using: grep -R 'kind: CustomResourceDefinition' Positive Consequences Less astonishing, compared to installing Chart \"by hand\". Less maintenance, i.e., there is only one source of truth for CRDs. Negative Consequences None really. Detailed Audit A detailed audit was performed of all CRDs in Compliant Kubernetes on 2021-04-27. As a summary, all projects encourage installing CRDs as part of standard helm install . Most projects encourage following manual migration steps to handle CRDs. Some projects handle CRD upgrades. A detailed analysis is listed below: cert-manager Installation: The cert-manager Helm Chart includes the installCRDs value -- by default it is set to false . If set to true , then CRDs are automatically installed when installing cert-manager, albeit not using the CRDs mechanism provided by Helm. Upgrade: CRDs are supposed to be upgraded manually . dex Dex can be configured without CRDs. ADR-0012 argues for that approach. gatekeeper Installation: Gatekeeper installs CRDs using the mechanism provided by Helm . Upgrade: Gatekeeper wants you to either uninstall-install or run a helm_migrate.sh . Prometheus (kube-prometheus-stack) Installation: kube-prometheus-stack installs CRDs using standard Helm mechanism . Upgrade: kube-prometheus-stack expects you to run manual upgrade steps . Velero Installation: Velero install CRDs using standard Helm mechanism . Upgrade: Velero includes magic to upgrade CRDs .","title":"Let upstream projects handle CRDs"},{"location":"adr/0011-let-upstream-projects-handle-crds/#let-upstream-projects-handle-crds","text":"Status: accepted Deciders: Compliant Kubernetes Arch Meeting Date: 2021-04-29 Technical Story: #446 #369 #391 #402 #436 .","title":"Let upstream projects handle CRDs"},{"location":"adr/0011-let-upstream-projects-handle-crds/#context-and-problem-statement","text":"CustomResourceDefinitions (CRDs) are tricky. They are essentially a mechanism to change the API of Kubernetes. Helm 2 had zero support for CRDs. Helm 3 has support for installing CRDs , but not upgrading them. How should we handle CRDs?","title":"Context and Problem Statement"},{"location":"adr/0011-let-upstream-projects-handle-crds/#decision-drivers","text":"CRDs add complexity and need to be treated specially. Generally need to \u201ctrim fat\u201d and rely on upstream.","title":"Decision Drivers"},{"location":"adr/0011-let-upstream-projects-handle-crds/#considered-options","text":"Install and upgrade CRDs as part of the bootstrap step, which is a Helm 2 legacy. Rely on whatever mechanism is proposed by upstream Helm Charts.","title":"Considered Options"},{"location":"adr/0011-let-upstream-projects-handle-crds/#decision-outcome","text":"Chosen option: \"Rely on upstream\", because it trims fat and reduces astonishment. At installation, rely on upstream's approach to install CRDs (see below). At upgrade, propagate upstream migration steps in CK8s migration steps in each release notes. An issue template was created to ensure we won't forget. Since we \"vendor in\" all Charts, CRDs can be discovered using: grep -R 'kind: CustomResourceDefinition'","title":"Decision Outcome"},{"location":"adr/0011-let-upstream-projects-handle-crds/#positive-consequences","text":"Less astonishing, compared to installing Chart \"by hand\". Less maintenance, i.e., there is only one source of truth for CRDs.","title":"Positive Consequences"},{"location":"adr/0011-let-upstream-projects-handle-crds/#negative-consequences","text":"None really.","title":"Negative Consequences"},{"location":"adr/0011-let-upstream-projects-handle-crds/#detailed-audit","text":"A detailed audit was performed of all CRDs in Compliant Kubernetes on 2021-04-27. As a summary, all projects encourage installing CRDs as part of standard helm install . Most projects encourage following manual migration steps to handle CRDs. Some projects handle CRD upgrades. A detailed analysis is listed below:","title":"Detailed Audit"},{"location":"adr/0011-let-upstream-projects-handle-crds/#cert-manager","text":"Installation: The cert-manager Helm Chart includes the installCRDs value -- by default it is set to false . If set to true , then CRDs are automatically installed when installing cert-manager, albeit not using the CRDs mechanism provided by Helm. Upgrade: CRDs are supposed to be upgraded manually .","title":"cert-manager"},{"location":"adr/0011-let-upstream-projects-handle-crds/#dex","text":"Dex can be configured without CRDs. ADR-0012 argues for that approach.","title":"dex"},{"location":"adr/0011-let-upstream-projects-handle-crds/#gatekeeper","text":"Installation: Gatekeeper installs CRDs using the mechanism provided by Helm . Upgrade: Gatekeeper wants you to either uninstall-install or run a helm_migrate.sh .","title":"gatekeeper"},{"location":"adr/0011-let-upstream-projects-handle-crds/#prometheus-kube-prometheus-stack","text":"Installation: kube-prometheus-stack installs CRDs using standard Helm mechanism . Upgrade: kube-prometheus-stack expects you to run manual upgrade steps .","title":"Prometheus (kube-prometheus-stack)"},{"location":"adr/0011-let-upstream-projects-handle-crds/#velero","text":"Installation: Velero install CRDs using standard Helm mechanism . Upgrade: Velero includes magic to upgrade CRDs .","title":"Velero"},{"location":"adr/0012-do-not-persist-dex/","text":"Do not persist Dex Status: accepted Deciders: Compliant Kubernetes Architecture Meeting Date: 2021-04-29 Technical Story: Reduce Helmfile concurrency for improved predictability Context and Problem Statement Dex requires persisting state to perform various tasks such as track refresh tokens, preventing replays, and rotating keys. What persistence option should we use? Decision Drivers CRDs add complexity Storage adds complexity Considered Options Use \"memory\" storage Use CRD-based storage Decision Outcome Chosen option: \"use memory\", because it simplified operations with little negative impact. Positive Consequences Dex brings no additional CRDs, which simplified upgrades. Dex brings no state, which simplified upgrades. Negative Consequences The authentication flow is disrupted, if Dex is rebooted exactly during an authentication flow. There is no user impact if Dex is restarted after the JWT was issued. Cristian tested this with kubectl and Grafana. Since we will only reboot Dex during maintenance windows, this is unlikely to be an issue in the foreseeable future. Other Considerations If Dex becomes a bottleneck and needs replication, or if we want to avoid disrupting authentication flows during operations on Dex, we will have to revisit this ADR.","title":"Do not persist Dex"},{"location":"adr/0012-do-not-persist-dex/#do-not-persist-dex","text":"Status: accepted Deciders: Compliant Kubernetes Architecture Meeting Date: 2021-04-29 Technical Story: Reduce Helmfile concurrency for improved predictability","title":"Do not persist Dex"},{"location":"adr/0012-do-not-persist-dex/#context-and-problem-statement","text":"Dex requires persisting state to perform various tasks such as track refresh tokens, preventing replays, and rotating keys. What persistence option should we use?","title":"Context and Problem Statement"},{"location":"adr/0012-do-not-persist-dex/#decision-drivers","text":"CRDs add complexity Storage adds complexity","title":"Decision Drivers"},{"location":"adr/0012-do-not-persist-dex/#considered-options","text":"Use \"memory\" storage Use CRD-based storage","title":"Considered Options"},{"location":"adr/0012-do-not-persist-dex/#decision-outcome","text":"Chosen option: \"use memory\", because it simplified operations with little negative impact.","title":"Decision Outcome"},{"location":"adr/0012-do-not-persist-dex/#positive-consequences","text":"Dex brings no additional CRDs, which simplified upgrades. Dex brings no state, which simplified upgrades.","title":"Positive Consequences"},{"location":"adr/0012-do-not-persist-dex/#negative-consequences","text":"The authentication flow is disrupted, if Dex is rebooted exactly during an authentication flow. There is no user impact if Dex is restarted after the JWT was issued. Cristian tested this with kubectl and Grafana. Since we will only reboot Dex during maintenance windows, this is unlikely to be an issue in the foreseeable future.","title":"Negative Consequences"},{"location":"adr/0012-do-not-persist-dex/#other-considerations","text":"If Dex becomes a bottleneck and needs replication, or if we want to avoid disrupting authentication flows during operations on Dex, we will have to revisit this ADR.","title":"Other Considerations"},{"location":"adr/0013-configure-alerts-in-omt/","text":"Configure Alerts in On-call Management Tool (e.g., Opsgenie) Status: accepted Deciders: Compliant Kubernetes Architecture Meeting Date: 2021-06-03 Technical Story: See \"Investigate how to systematically work with alerts\" Context and Problem Statement Alerts are some noteworthy IT event, like a Node becoming un-ready, login failure or a disk getting full. Terminology differs across tooling and organizations, but one generally cares about: P1 (critical) alerts, which require immediate human attention -- the person on-call needs to be notified immediately -- and; P2 (high) alerts which require human attention with 24 hours -- the person on-call needs to be notified next morning; P3 (moderate) alerts which do not require immediate human attention, but should be regularly reviewed. Other priorities (e.g., P4 and below) are generally used for informational purposes. Dealing with alerts correctly entails prioritizing them (e.g., P1, P2, P3), deciding if someone should be notified, who should be notified, how they should be notified (e.g., SMS or email) and when. \"Who\", \"how\" and \"when\" should include escalation, if the previous notification was not acknowledged within a pre-configured time interval, then the same person if notified via a different channel or a new person is notified. Under-alerting -- e.g., notifying an on-call person too late -- may lead to Service Level Agreement (SLA) violations and a general feeling of administrator anxiety: \"Is everything okay, or is alerting not working?\". Over-alerting -- e.g., notifying a person too often about low-priority alerts -- leads to alert fatigue and \"crying wolf\" where even important alerts are eventually ignored. Hence, configuring the right level of alerting -- in particular notifications -- is extremely important both for SLA fulfillment and a happy on-call team. Where should alerting be configured, so as to quickly converge to the optimal alerting level? Decision Drivers Allow to quickly silence, un-silence and re-prioritize alerts. Allow arbitrary flexibility, e.g., who should be notified, when should notification happen, when should escalation happen, for what cluster and namespaces should notification happen, etc. Leverage existing tools and processes. Considered Options Configure alerting in Compliant Kubernetes, specifically alertmanager. Configure alerting in an On-call Management Tool (OMT), e.g., Opsgenie, PagerDuty. Decision Outcome Chosen option: Compliant Kubernetes \u201cover-alerts\u201d, i.e., forwards all alerts and all relevant information to an On-Call Management Tool (OMT, e.g., Opsgenie). Configuration of alerts happens in the OMT. Positive Consequences Clear separation of concerns. Alerting does not require per-customer configuration of Compliant Kubernetes. Leverages existing tools and processes. We do not need to implement complex alert filtering in Compliant Kubernetes, e.g., silence alerts during maintenance windows, silence alerts during Swedish holidays, etc. Negative Consequences Does not capture alerting know-how in Compliant Kubernetes. Migration to a new OMT means all alerting configuration needs to be migrated to the new tool. Fortunately, this can be done incrementally. Recommendations to Operators Operators should familiarize themselves with the capabilities of OMT, e.g., OpsGenie. This should be first done using a web UI, since that improves discoverability of such capabilities. When alerting configuration becomes too complex and/or repetitive, administrators should employ a configuration management tools, such as Terraform, to configure the OMT. Links Opsgenie documentation Alertmanager documentation Terraform Opsgenie provider Pulumni Opsgenie module","title":"Configure Alerts in On-call Management Tool (e.g., Opsgenie)"},{"location":"adr/0013-configure-alerts-in-omt/#configure-alerts-in-on-call-management-tool-eg-opsgenie","text":"Status: accepted Deciders: Compliant Kubernetes Architecture Meeting Date: 2021-06-03 Technical Story: See \"Investigate how to systematically work with alerts\"","title":"Configure Alerts in On-call Management Tool (e.g., Opsgenie)"},{"location":"adr/0013-configure-alerts-in-omt/#context-and-problem-statement","text":"Alerts are some noteworthy IT event, like a Node becoming un-ready, login failure or a disk getting full. Terminology differs across tooling and organizations, but one generally cares about: P1 (critical) alerts, which require immediate human attention -- the person on-call needs to be notified immediately -- and; P2 (high) alerts which require human attention with 24 hours -- the person on-call needs to be notified next morning; P3 (moderate) alerts which do not require immediate human attention, but should be regularly reviewed. Other priorities (e.g., P4 and below) are generally used for informational purposes. Dealing with alerts correctly entails prioritizing them (e.g., P1, P2, P3), deciding if someone should be notified, who should be notified, how they should be notified (e.g., SMS or email) and when. \"Who\", \"how\" and \"when\" should include escalation, if the previous notification was not acknowledged within a pre-configured time interval, then the same person if notified via a different channel or a new person is notified. Under-alerting -- e.g., notifying an on-call person too late -- may lead to Service Level Agreement (SLA) violations and a general feeling of administrator anxiety: \"Is everything okay, or is alerting not working?\". Over-alerting -- e.g., notifying a person too often about low-priority alerts -- leads to alert fatigue and \"crying wolf\" where even important alerts are eventually ignored. Hence, configuring the right level of alerting -- in particular notifications -- is extremely important both for SLA fulfillment and a happy on-call team. Where should alerting be configured, so as to quickly converge to the optimal alerting level?","title":"Context and Problem Statement"},{"location":"adr/0013-configure-alerts-in-omt/#decision-drivers","text":"Allow to quickly silence, un-silence and re-prioritize alerts. Allow arbitrary flexibility, e.g., who should be notified, when should notification happen, when should escalation happen, for what cluster and namespaces should notification happen, etc. Leverage existing tools and processes.","title":"Decision Drivers"},{"location":"adr/0013-configure-alerts-in-omt/#considered-options","text":"Configure alerting in Compliant Kubernetes, specifically alertmanager. Configure alerting in an On-call Management Tool (OMT), e.g., Opsgenie, PagerDuty.","title":"Considered Options"},{"location":"adr/0013-configure-alerts-in-omt/#decision-outcome","text":"Chosen option: Compliant Kubernetes \u201cover-alerts\u201d, i.e., forwards all alerts and all relevant information to an On-Call Management Tool (OMT, e.g., Opsgenie). Configuration of alerts happens in the OMT.","title":"Decision Outcome"},{"location":"adr/0013-configure-alerts-in-omt/#positive-consequences","text":"Clear separation of concerns. Alerting does not require per-customer configuration of Compliant Kubernetes. Leverages existing tools and processes. We do not need to implement complex alert filtering in Compliant Kubernetes, e.g., silence alerts during maintenance windows, silence alerts during Swedish holidays, etc.","title":"Positive Consequences"},{"location":"adr/0013-configure-alerts-in-omt/#negative-consequences","text":"Does not capture alerting know-how in Compliant Kubernetes. Migration to a new OMT means all alerting configuration needs to be migrated to the new tool. Fortunately, this can be done incrementally.","title":"Negative Consequences"},{"location":"adr/0013-configure-alerts-in-omt/#recommendations-to-operators","text":"Operators should familiarize themselves with the capabilities of OMT, e.g., OpsGenie. This should be first done using a web UI, since that improves discoverability of such capabilities. When alerting configuration becomes too complex and/or repetitive, administrators should employ a configuration management tools, such as Terraform, to configure the OMT.","title":"Recommendations to Operators"},{"location":"adr/0013-configure-alerts-in-omt/#links","text":"Opsgenie documentation Alertmanager documentation Terraform Opsgenie provider Pulumni Opsgenie module","title":"Links"},{"location":"adr/0014-use-bats-for-testing-bash-wrappers/","text":"Use bats for testing bash wrappers Status: accepted Deciders: Compliant Kubernetes Architecture Meeting Date: 2021-06-03 Context and Problem Statement We write wrapper scripts for simpler and consistent operations. How should we test these scripts? Decision Drivers We want to use the best tools out there. We want to reduce tools sprawl, i.e., the collective cost (e.g., training) of adding a new tool should outweigh the collective benefit of the new tool. We want to make contributions inviting. Considered Options Do not test bash scripts. (We write perfect scripts 100% of the time, right? :smile:) Use alias for mocking, diff and test for assertions. Use bats Decision Outcome Chosen option: \"bats\", because the benefit of using a standard and rather light tool outweighs the cost of collective training on the new tool. Positive Consequences We use a pretty standard tool for testing in the bash universe. We do not risk re-inventing the while by writing our own wrappers around alias , diff and test . Negative Consequences We need to learn another tool, fortunately, it seems pretty light. Other Considerations Be very mindful about not overusing bash. Generally bash should only be used for things that you would do in the terminal, but got tired of copy-pasting, like: Running commands Copying files Setting environment variables Minor path translations For more advanced functionality prefer upstreaming into Ansible roles/libraries, Helm Charts, upstream source code, etc.","title":"Use bats for testing bash wrappers"},{"location":"adr/0014-use-bats-for-testing-bash-wrappers/#use-bats-for-testing-bash-wrappers","text":"Status: accepted Deciders: Compliant Kubernetes Architecture Meeting Date: 2021-06-03","title":"Use bats for testing bash wrappers"},{"location":"adr/0014-use-bats-for-testing-bash-wrappers/#context-and-problem-statement","text":"We write wrapper scripts for simpler and consistent operations. How should we test these scripts?","title":"Context and Problem Statement"},{"location":"adr/0014-use-bats-for-testing-bash-wrappers/#decision-drivers","text":"We want to use the best tools out there. We want to reduce tools sprawl, i.e., the collective cost (e.g., training) of adding a new tool should outweigh the collective benefit of the new tool. We want to make contributions inviting.","title":"Decision Drivers"},{"location":"adr/0014-use-bats-for-testing-bash-wrappers/#considered-options","text":"Do not test bash scripts. (We write perfect scripts 100% of the time, right? :smile:) Use alias for mocking, diff and test for assertions. Use bats","title":"Considered Options"},{"location":"adr/0014-use-bats-for-testing-bash-wrappers/#decision-outcome","text":"Chosen option: \"bats\", because the benefit of using a standard and rather light tool outweighs the cost of collective training on the new tool.","title":"Decision Outcome"},{"location":"adr/0014-use-bats-for-testing-bash-wrappers/#positive-consequences","text":"We use a pretty standard tool for testing in the bash universe. We do not risk re-inventing the while by writing our own wrappers around alias , diff and test .","title":"Positive Consequences"},{"location":"adr/0014-use-bats-for-testing-bash-wrappers/#negative-consequences","text":"We need to learn another tool, fortunately, it seems pretty light.","title":"Negative Consequences"},{"location":"adr/0014-use-bats-for-testing-bash-wrappers/#other-considerations","text":"Be very mindful about not overusing bash. Generally bash should only be used for things that you would do in the terminal, but got tired of copy-pasting, like: Running commands Copying files Setting environment variables Minor path translations For more advanced functionality prefer upstreaming into Ansible roles/libraries, Helm Charts, upstream source code, etc.","title":"Other Considerations"},{"location":"adr/0015-we-believe-in-community-driven-open-source/","text":"We believe in community-driven open source Status: accepted Deciders: Rob, Johan, Cristian (a.k.a., Product Management working group) Date: 2021-08-17 Context and Problem Statement We often get bombarded with questions like \"Why don't you use X?\" or \"Why don't you build on top of Y?\", sometimes preceded by \"product/project X already has feature Y\". Needless to say, this can cause a \"Simpsons Already Did It\" feeling. This ADR clarifies one of the core values of the Compliant Kubernetes project, namely our belief in community-driven open source. The ADR is useful to clarify both to internal and external stakeholders the choices we make. Decision Drivers We do not want to depend on the interests of any single company, be it small or large. Our customers need to have a business continuity plan, see ISO 27001, Annex A.17 . Therefore, we want to make it easy to \"exit\" Compliant Kubernetes and take over platform management. We want to use the best tools out there. Considered Options Prefer closed source solutions. Prefer single-company open source solutions. Prefer community-drive open source solutions. Decision Outcome Chosen option: \"prefer community-driven open source solutions\". Positive Consequences We do not depend on the interests of any single company. Our customers do not depend on the interests of any single company. Business continuity is significantly simplified for our customers. We have better chances at influencing projects in a direction that is useful to us and our customers. The smaller the project, the easier to influence. Negative Consequences Sometimes we might need to give up \"that cool new feature\" until the community-driven open source solution catches up with their closed source or single-company open source alternative. Alternatively, we might need to put extra time and effort to develop \"that cool new feature\" ourselves. As they are not bound by vendor liability -- e.g., end-of-life promises -- community-driven projects present a greater risk of being abandoned. The smaller the project, the higher the risk.","title":"We believe in community-driven open source"},{"location":"adr/0015-we-believe-in-community-driven-open-source/#we-believe-in-community-driven-open-source","text":"Status: accepted Deciders: Rob, Johan, Cristian (a.k.a., Product Management working group) Date: 2021-08-17","title":"We believe in community-driven open source"},{"location":"adr/0015-we-believe-in-community-driven-open-source/#context-and-problem-statement","text":"We often get bombarded with questions like \"Why don't you use X?\" or \"Why don't you build on top of Y?\", sometimes preceded by \"product/project X already has feature Y\". Needless to say, this can cause a \"Simpsons Already Did It\" feeling. This ADR clarifies one of the core values of the Compliant Kubernetes project, namely our belief in community-driven open source. The ADR is useful to clarify both to internal and external stakeholders the choices we make.","title":"Context and Problem Statement"},{"location":"adr/0015-we-believe-in-community-driven-open-source/#decision-drivers","text":"We do not want to depend on the interests of any single company, be it small or large. Our customers need to have a business continuity plan, see ISO 27001, Annex A.17 . Therefore, we want to make it easy to \"exit\" Compliant Kubernetes and take over platform management. We want to use the best tools out there.","title":"Decision Drivers"},{"location":"adr/0015-we-believe-in-community-driven-open-source/#considered-options","text":"Prefer closed source solutions. Prefer single-company open source solutions. Prefer community-drive open source solutions.","title":"Considered Options"},{"location":"adr/0015-we-believe-in-community-driven-open-source/#decision-outcome","text":"Chosen option: \"prefer community-driven open source solutions\".","title":"Decision Outcome"},{"location":"adr/0015-we-believe-in-community-driven-open-source/#positive-consequences","text":"We do not depend on the interests of any single company. Our customers do not depend on the interests of any single company. Business continuity is significantly simplified for our customers. We have better chances at influencing projects in a direction that is useful to us and our customers. The smaller the project, the easier to influence.","title":"Positive Consequences"},{"location":"adr/0015-we-believe-in-community-driven-open-source/#negative-consequences","text":"Sometimes we might need to give up \"that cool new feature\" until the community-driven open source solution catches up with their closed source or single-company open source alternative. Alternatively, we might need to put extra time and effort to develop \"that cool new feature\" ourselves. As they are not bound by vendor liability -- e.g., end-of-life promises -- community-driven projects present a greater risk of being abandoned. The smaller the project, the higher the risk.","title":"Negative Consequences"},{"location":"adr/0016-gid-0-is-okey-but-not-by-default/","text":"gid=0 is okay, but not by default Status: accepted Deciders: Cristian, Lars, Olle Date: 2021-08-23 Context and Problem Statement OpenShift likes to shift (pun intended) the UID -- i.e., assign arbitrary UIDs -- to containers. They do this as an additional security feature, given that OpenShift is a multi-tentant Kubernetes solution. Each OpenShift project received a non-overlapping UID range. Hence, in case an attacker escapes a container, it will be more difficult to interfere with other processes. However, this shifting of UIDs introduces an additional complexity: What if a process wants to write to the filesystem? What uid, gid and permissions should the files and folders have? To solve this problem, the OpenShift documentation (see \"Support arbitrary user ids\" ) recommends setting gid=0 on those files and folders. Specifically, the Dockerfiles of the container images should contain: RUN chgrp -R 0 /some/directory && chmod -R g = u /some/directory During execution, OpenShift assigns gid=0 as a supplementary group to containers, so as to give them access to the required files. In contrast to OpenShift, Compliant Kubernetes is not a multi-tenant solution. Given previous vulnerabilities in Kubernetes that affected tenant isolation (e.g., CVE-2020-8554 ), we believe that non-trusting users should not share a workload cluster. Hence, we do not assign arbitrary UIDs to containers and do not need to assign gid=0 as a supplementary group. The gid=0 practice above seems to have made its way in quite a few Dockerfiles , however, it is far from being the default outside OpenShift. What should Compliant Kubernetes do with the gid=0 practice? Decision Drivers For user expectations, we want to make it easy to start with Compliant Kubernetes. For better security and easier audits, we do not want to add unnecessary permissions. ID mapping in mounts has landed in Linux 5.12. Once this feature is used in container runtimes and Kubernetes, the gid=0 problem will go away. Considered Options Allow gid=0 by default. Disallow gid=0 by default -- this is what Kubespray does. Never allow gid=0 . Decision Outcome Chosen option: \"disallow gid=0 by default\". Enabling it on a case-by-case basis is okay. Positive Consequences We do not unnecessarily add a permission to containers. Negative Consequences Some users will complain about their container images not starting, and we will need to add a less restricted PodSecurityPolicy in their cluster. Other Considerations PodSecurityPolicies are deprecated in favor of PodSecurity Admission . This decision will have to be revisited once PodSecurity Admission is stable. In case we notice that the gid=0 practice is gaining significant uptake, we will have to revisit this decision to allow gid=0 by default. In case ID mapping is implemented in container runtimes and Kubernetes, this problem will likely go away. In that case, this decision might be revisited to never allow gid=0 .","title":"gid=0 is okay, but not by default"},{"location":"adr/0016-gid-0-is-okey-but-not-by-default/#gid0-is-okay-but-not-by-default","text":"Status: accepted Deciders: Cristian, Lars, Olle Date: 2021-08-23","title":"gid=0 is okay, but not by default"},{"location":"adr/0016-gid-0-is-okey-but-not-by-default/#context-and-problem-statement","text":"OpenShift likes to shift (pun intended) the UID -- i.e., assign arbitrary UIDs -- to containers. They do this as an additional security feature, given that OpenShift is a multi-tentant Kubernetes solution. Each OpenShift project received a non-overlapping UID range. Hence, in case an attacker escapes a container, it will be more difficult to interfere with other processes. However, this shifting of UIDs introduces an additional complexity: What if a process wants to write to the filesystem? What uid, gid and permissions should the files and folders have? To solve this problem, the OpenShift documentation (see \"Support arbitrary user ids\" ) recommends setting gid=0 on those files and folders. Specifically, the Dockerfiles of the container images should contain: RUN chgrp -R 0 /some/directory && chmod -R g = u /some/directory During execution, OpenShift assigns gid=0 as a supplementary group to containers, so as to give them access to the required files. In contrast to OpenShift, Compliant Kubernetes is not a multi-tenant solution. Given previous vulnerabilities in Kubernetes that affected tenant isolation (e.g., CVE-2020-8554 ), we believe that non-trusting users should not share a workload cluster. Hence, we do not assign arbitrary UIDs to containers and do not need to assign gid=0 as a supplementary group. The gid=0 practice above seems to have made its way in quite a few Dockerfiles , however, it is far from being the default outside OpenShift. What should Compliant Kubernetes do with the gid=0 practice?","title":"Context and Problem Statement"},{"location":"adr/0016-gid-0-is-okey-but-not-by-default/#decision-drivers","text":"For user expectations, we want to make it easy to start with Compliant Kubernetes. For better security and easier audits, we do not want to add unnecessary permissions. ID mapping in mounts has landed in Linux 5.12. Once this feature is used in container runtimes and Kubernetes, the gid=0 problem will go away.","title":"Decision Drivers"},{"location":"adr/0016-gid-0-is-okey-but-not-by-default/#considered-options","text":"Allow gid=0 by default. Disallow gid=0 by default -- this is what Kubespray does. Never allow gid=0 .","title":"Considered Options"},{"location":"adr/0016-gid-0-is-okey-but-not-by-default/#decision-outcome","text":"Chosen option: \"disallow gid=0 by default\". Enabling it on a case-by-case basis is okay.","title":"Decision Outcome"},{"location":"adr/0016-gid-0-is-okey-but-not-by-default/#positive-consequences","text":"We do not unnecessarily add a permission to containers.","title":"Positive Consequences"},{"location":"adr/0016-gid-0-is-okey-but-not-by-default/#negative-consequences","text":"Some users will complain about their container images not starting, and we will need to add a less restricted PodSecurityPolicy in their cluster.","title":"Negative Consequences"},{"location":"adr/0016-gid-0-is-okey-but-not-by-default/#other-considerations","text":"PodSecurityPolicies are deprecated in favor of PodSecurity Admission . This decision will have to be revisited once PodSecurity Admission is stable. In case we notice that the gid=0 practice is gaining significant uptake, we will have to revisit this decision to allow gid=0 by default. In case ID mapping is implemented in container runtimes and Kubernetes, this problem will likely go away. In that case, this decision might be revisited to never allow gid=0 .","title":"Other Considerations"},{"location":"adr/template/","text":"[short title of solved problem and solution] Status: [proposed | rejected | accepted | deprecated | \u2026 | superseded by ADR-0005 Deciders: [list everyone involved in the decision] Date: [YYYY-MM-DD when the decision was last updated] Technical Story: [description | ticket/issue URL] Context and Problem Statement [Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.] Decision Drivers [driver 1, e.g., a force, facing concern, \u2026] [driver 2, e.g., a force, facing concern, \u2026] \u2026 Considered Options [option 1] [option 2] [option 3] \u2026 Decision Outcome Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. Positive Consequences [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026] \u2026 Negative Consequences [e.g., compromising quality attribute, follow-up decisions required, \u2026] \u2026 Pros and Cons of the Options [option 1] [example | description | pointer to more information | \u2026] Good, because [argument a] Good, because [argument b] Bad, because [argument c] \u2026 [option 2] [example | description | pointer to more information | \u2026] Good, because [argument a] Good, because [argument b] Bad, because [argument c] \u2026 [option 3] [example | description | pointer to more information | \u2026] Good, because [argument a] Good, because [argument b] Bad, because [argument c] \u2026 Links [Link type] [Link to ADR] \u2026","title":"[short title of solved problem and solution]"},{"location":"adr/template/#short-title-of-solved-problem-and-solution","text":"Status: [proposed | rejected | accepted | deprecated | \u2026 | superseded by ADR-0005 Deciders: [list everyone involved in the decision] Date: [YYYY-MM-DD when the decision was last updated] Technical Story: [description | ticket/issue URL]","title":"[short title of solved problem and solution]"},{"location":"adr/template/#context-and-problem-statement","text":"[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]","title":"Context and Problem Statement"},{"location":"adr/template/#decision-drivers","text":"[driver 1, e.g., a force, facing concern, \u2026] [driver 2, e.g., a force, facing concern, \u2026] \u2026","title":"Decision Drivers "},{"location":"adr/template/#considered-options","text":"[option 1] [option 2] [option 3] \u2026","title":"Considered Options"},{"location":"adr/template/#decision-outcome","text":"Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)].","title":"Decision Outcome"},{"location":"adr/template/#positive-consequences","text":"[e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026] \u2026","title":"Positive Consequences "},{"location":"adr/template/#negative-consequences","text":"[e.g., compromising quality attribute, follow-up decisions required, \u2026] \u2026","title":"Negative Consequences "},{"location":"adr/template/#pros-and-cons-of-the-options","text":"","title":"Pros and Cons of the Options "},{"location":"adr/template/#option-1","text":"[example | description | pointer to more information | \u2026] Good, because [argument a] Good, because [argument b] Bad, because [argument c] \u2026","title":"[option 1]"},{"location":"adr/template/#option-2","text":"[example | description | pointer to more information | \u2026] Good, because [argument a] Good, because [argument b] Bad, because [argument c] \u2026","title":"[option 2]"},{"location":"adr/template/#option-3","text":"[example | description | pointer to more information | \u2026] Good, because [argument a] Good, because [argument b] Bad, because [argument c] \u2026","title":"[option 3]"},{"location":"adr/template/#links","text":"[Link type] [Link to ADR] \u2026","title":"Links "},{"location":"ciso-guide/","text":"CISO Guide Overview This guide is for the Chief Information Security Officer (CISO) who needs to prove to an internal or external auditor that the application runs on top of a compliant platform. The CISO can be described via the following user stories: As an information security officer, I want to audit the Compliant Kubernetes cluster, so as to comply with continuous compliance policies. As an information security officer, I want to quickly identify compliance violation and convert them into actionable tasks for developers. The CISO only needs: a modern browser (recent versions of Chrome, Firefox or Edge will do); the URL to the Compliant Kubernetes dashboard (usually https://grafana.example.com); credentials for the Compliant Kubernetes cluster. If in doubt, contact the Compliant Kubernetes administrator.","title":"Overview"},{"location":"ciso-guide/#ciso-guide-overview","text":"This guide is for the Chief Information Security Officer (CISO) who needs to prove to an internal or external auditor that the application runs on top of a compliant platform. The CISO can be described via the following user stories: As an information security officer, I want to audit the Compliant Kubernetes cluster, so as to comply with continuous compliance policies. As an information security officer, I want to quickly identify compliance violation and convert them into actionable tasks for developers. The CISO only needs: a modern browser (recent versions of Chrome, Firefox or Edge will do); the URL to the Compliant Kubernetes dashboard (usually https://grafana.example.com); credentials for the Compliant Kubernetes cluster. If in doubt, contact the Compliant Kubernetes administrator.","title":"CISO Guide Overview"},{"location":"ciso-guide/backup/","text":"Backup Dashboard Relevant Regulations GDPR Article 32 : In assessing the appropriate level of security account shall be taken in particular of the risks that are presented by processing, in particular from accidental or unlawful destruction, loss , alteration, unauthorised disclosure of, or access to personal data transmitted, stored or otherwise processed. [highlights added] HIPAA Part 164\u2014SECURITY AND PRIVACY (A) Data backup plan (Required). Establish and implement procedures to create and maintain retrievable exact copies of electronic protected health information. (B) Disaster recovery plan (Required). Establish (and implement as needed) procedures to restore any loss of data. Mapping to ISO 27001 Controls A.12.3.1 Information Backup A.17.1.1 Planning Information Security Continuity Compliant Kubernetes Backup Dashboard The Compliant Kubernetes Backup Dashboard allows to quickly audit the status of backups and ensure the Recovery Point Objective are met. Handling Non-Compliance In case there is a violation of backup policies: Ask the administrator to check the status of the backup jobs . Ask the developers to check if they correctly marked Kubernetes resources with the necessary backup annotations .","title":"Backup"},{"location":"ciso-guide/backup/#backup-dashboard","text":"","title":"Backup Dashboard"},{"location":"ciso-guide/backup/#relevant-regulations","text":"GDPR Article 32 : In assessing the appropriate level of security account shall be taken in particular of the risks that are presented by processing, in particular from accidental or unlawful destruction, loss , alteration, unauthorised disclosure of, or access to personal data transmitted, stored or otherwise processed. [highlights added] HIPAA Part 164\u2014SECURITY AND PRIVACY (A) Data backup plan (Required). Establish and implement procedures to create and maintain retrievable exact copies of electronic protected health information. (B) Disaster recovery plan (Required). Establish (and implement as needed) procedures to restore any loss of data.","title":"Relevant Regulations"},{"location":"ciso-guide/backup/#mapping-to-iso-27001-controls","text":"A.12.3.1 Information Backup A.17.1.1 Planning Information Security Continuity","title":"Mapping to ISO 27001 Controls"},{"location":"ciso-guide/backup/#compliant-kubernetes-backup-dashboard","text":"The Compliant Kubernetes Backup Dashboard allows to quickly audit the status of backups and ensure the Recovery Point Objective are met.","title":"Compliant Kubernetes Backup Dashboard"},{"location":"ciso-guide/backup/#handling-non-compliance","text":"In case there is a violation of backup policies: Ask the administrator to check the status of the backup jobs . Ask the developers to check if they correctly marked Kubernetes resources with the necessary backup annotations .","title":"Handling Non-Compliance"},{"location":"ciso-guide/cryptography/","text":"Cryptography Dashboard Relevant Regulations GDPR Article 32 : Taking into account the state of the art [...] the controller and the processor shall implement [...] as appropriate [...] encryption of personal data; In assessing the appropriate level of security account shall be taken in particular of the risks that are presented by processing, in particular from accidental or unlawful destruction, loss, alteration, unauthorised disclosure of, or access to personal data transmitted , stored or otherwise processed. [highlights added] HIPAA Part 164\u2014SECURITY AND PRIVACY (ii) Encryption (Addressable). Implement a mechanism to encrypt electronic protected health information whenever deemed appropriate. Mapping to ISO 27001 Controls A.10 Cryptography Compliant Kubernetes Cryptography Dashboard The Compliant Kubernetes Cryptography Dashboard allows to quickly audit the status of cryptography. It shows, amongst others, the public Internet endpoints (Ingresses) that are encrypted and the expiry time. Default Compliant Kubernetes configurations automatically renew certificates before expiry. Handling Non-Compliance In case there is a violation of cryptography policies: If a certificate is expired and was not renewed, ask the administrator to check the status of cert-manager and ingress-controller component. If an endpoint is not encrypted, ask the developers to set the necessary Ingress annotations .","title":"Cryptography"},{"location":"ciso-guide/cryptography/#cryptography-dashboard","text":"","title":"Cryptography Dashboard"},{"location":"ciso-guide/cryptography/#relevant-regulations","text":"GDPR Article 32 : Taking into account the state of the art [...] the controller and the processor shall implement [...] as appropriate [...] encryption of personal data; In assessing the appropriate level of security account shall be taken in particular of the risks that are presented by processing, in particular from accidental or unlawful destruction, loss, alteration, unauthorised disclosure of, or access to personal data transmitted , stored or otherwise processed. [highlights added] HIPAA Part 164\u2014SECURITY AND PRIVACY (ii) Encryption (Addressable). Implement a mechanism to encrypt electronic protected health information whenever deemed appropriate.","title":"Relevant Regulations"},{"location":"ciso-guide/cryptography/#mapping-to-iso-27001-controls","text":"A.10 Cryptography","title":"Mapping to ISO 27001 Controls"},{"location":"ciso-guide/cryptography/#compliant-kubernetes-cryptography-dashboard","text":"The Compliant Kubernetes Cryptography Dashboard allows to quickly audit the status of cryptography. It shows, amongst others, the public Internet endpoints (Ingresses) that are encrypted and the expiry time. Default Compliant Kubernetes configurations automatically renew certificates before expiry.","title":"Compliant Kubernetes Cryptography Dashboard"},{"location":"ciso-guide/cryptography/#handling-non-compliance","text":"In case there is a violation of cryptography policies: If a certificate is expired and was not renewed, ask the administrator to check the status of cert-manager and ingress-controller component. If an endpoint is not encrypted, ask the developers to set the necessary Ingress annotations .","title":"Handling Non-Compliance"},{"location":"ciso-guide/intrusion-detection/","text":"Intrusion Detection Dashboard Relevant Regulations GDPR Article 32 : Taking into account the state of the art [...] the controller and the processor shall implement [...] as appropriate [...] encryption of personal data; In assessing the appropriate level of security account shall be taken in particular of the risks that are presented by processing, in particular from accidental or unlawful destruction, loss, alteration, unauthorised disclosure of, or access to personal data transmitted , stored or otherwise processed. [highlights added] HIPAA Part 164\u2014SECURITY AND PRIVACY (B) Protection from malicious software (Addressable). Procedures for guarding against, detecting, and reporting malicious software. [highlights added] Mapping to ISO 27001 Controls A.12.2.1 Controls Against Malware A.12.6.1 Management of Technical Vulnerabilities A.16.1.7 Collection of Evidence Compliant Kubernetes Intrusion Detection Dashboard The Compliant Kubernetes Intrusion Detection Dashboard allows to quickly audit any suspicious activity performed by code inside the cluster, such as writing to suspicious files (e.g., in /etc ) or attempting suspicious external network connections (e.g., SSH to a command-and-control server). Such activities may indicate anything from a misconfiguration issue to an ongoing attack. Therefore, this dashboard should be regularly reviewed, perhaps even daily. Handling Non-Compliance Make sure you have a proper incident management policy in place. If an attack is ongoing, it might be better to take the system offline to protect data from getting in the wrong hands. Operators need to be trained on what events justify such an extreme action, otherwise, escalating the issue along the reporting chain may add delays that favor the attacker. In less severe cases, simply contact the developers to investigate their code and fix any potential misconfiguration.","title":"Intrusion Detection"},{"location":"ciso-guide/intrusion-detection/#intrusion-detection-dashboard","text":"","title":"Intrusion Detection Dashboard"},{"location":"ciso-guide/intrusion-detection/#relevant-regulations","text":"GDPR Article 32 : Taking into account the state of the art [...] the controller and the processor shall implement [...] as appropriate [...] encryption of personal data; In assessing the appropriate level of security account shall be taken in particular of the risks that are presented by processing, in particular from accidental or unlawful destruction, loss, alteration, unauthorised disclosure of, or access to personal data transmitted , stored or otherwise processed. [highlights added] HIPAA Part 164\u2014SECURITY AND PRIVACY (B) Protection from malicious software (Addressable). Procedures for guarding against, detecting, and reporting malicious software. [highlights added]","title":"Relevant Regulations"},{"location":"ciso-guide/intrusion-detection/#mapping-to-iso-27001-controls","text":"A.12.2.1 Controls Against Malware A.12.6.1 Management of Technical Vulnerabilities A.16.1.7 Collection of Evidence","title":"Mapping to ISO 27001 Controls"},{"location":"ciso-guide/intrusion-detection/#compliant-kubernetes-intrusion-detection-dashboard","text":"The Compliant Kubernetes Intrusion Detection Dashboard allows to quickly audit any suspicious activity performed by code inside the cluster, such as writing to suspicious files (e.g., in /etc ) or attempting suspicious external network connections (e.g., SSH to a command-and-control server). Such activities may indicate anything from a misconfiguration issue to an ongoing attack. Therefore, this dashboard should be regularly reviewed, perhaps even daily.","title":"Compliant Kubernetes Intrusion Detection Dashboard"},{"location":"ciso-guide/intrusion-detection/#handling-non-compliance","text":"Make sure you have a proper incident management policy in place. If an attack is ongoing, it might be better to take the system offline to protect data from getting in the wrong hands. Operators need to be trained on what events justify such an extreme action, otherwise, escalating the issue along the reporting chain may add delays that favor the attacker. In less severe cases, simply contact the developers to investigate their code and fix any potential misconfiguration.","title":"Handling Non-Compliance"},{"location":"ciso-guide/kubernetes-status/","text":"Kubernetes Status Dashboard Relevant Regulations GDPR Article 32 : The ability to ensure the ongoing confidentiality, integrity, availability and resilience of processing systems and services; [highlights added] Mapping to ISO 27001 Controls A.12.1.3 Capacity Management The use of resources must be monitored, tuned and projections made of future capacity requirements to ensure the required system performance to meet the business objectives. Compliant Kubernetes Status Dashboard The Compliant Kubernetes Status Dashboard shows a quick overview of the status of your kubernetes cluster. This includes: Unhealthy pods Unhealthy nodes Resource requested of the total resources in the cluster Pods with missing resource requests This makes it easy to identify when your cluster is not working correctly and helps you identify configuration that isn't following best practise.","title":"Kubernetes Status"},{"location":"ciso-guide/kubernetes-status/#kubernetes-status-dashboard","text":"","title":"Kubernetes Status Dashboard"},{"location":"ciso-guide/kubernetes-status/#relevant-regulations","text":"GDPR Article 32 : The ability to ensure the ongoing confidentiality, integrity, availability and resilience of processing systems and services; [highlights added]","title":"Relevant Regulations"},{"location":"ciso-guide/kubernetes-status/#mapping-to-iso-27001-controls","text":"A.12.1.3 Capacity Management The use of resources must be monitored, tuned and projections made of future capacity requirements to ensure the required system performance to meet the business objectives.","title":"Mapping to ISO 27001 Controls"},{"location":"ciso-guide/kubernetes-status/#compliant-kubernetes-status-dashboard","text":"The Compliant Kubernetes Status Dashboard shows a quick overview of the status of your kubernetes cluster. This includes: Unhealthy pods Unhealthy nodes Resource requested of the total resources in the cluster Pods with missing resource requests This makes it easy to identify when your cluster is not working correctly and helps you identify configuration that isn't following best practise.","title":"Compliant Kubernetes Status Dashboard"},{"location":"ciso-guide/log-review/","text":"Log Review This document highlights the risks that can be mitigated by regularly reviewing logs and makes concrete recommendations on how to do log review. Relevant Regulations GDPR Article 32 : Taking into account the state of the art, the costs of implementation and the nature, scope, context and purposes of processing as well as the risk of varying likelihood and severity for the rights and freedoms of natural persons, the controller and the processor shall implement appropriate technical and organisational measures to ensure a level of security appropriate to the risk, including inter alia as appropriate: [...] a process for regularly testing, assessing and evaluating the effectiveness of technical and organisational measures for ensuring the security of the processing. HSLF-FS 2016:40 : 2 \u00a7 V\u00e5rdgivaren ska genom ledningssystemet s\u00e4kerst\u00e4lla att [...] 4. \u00e5tg\u00e4rder kan h\u00e4rledas till en anv\u00e4ndare (sp\u00e5rbarhet) i informationssystem som \u00e4r helt eller delvis automatiserade. Mapping to ISO 27001 Controls A.12.4.1 \"Event Logging\" A.12.4.3 \"Administrator and Operator Logs\" Purpose Compliant Kubernetes captures application logs and audit logs in a tamper-proof logging environment, which we call the service cluster. By \"tamper-proof\", we mean that even a complete compromise of production infrastructure does not allow an attacker to erase or change existing log entries, as would be required to hide their activity and avoid suspecion. Note Attackers can, however, inject new \"weird\" logs entries. However, that wouldn't remove their tracks and would only trigger more suspecion. However, said logs only help with information security if they are regularly reviewed for suspicious activity. Prefer to use logs for catching \"unknown unknowns\". For known bad failures -- e.g., a fluentd Pod restarting -- prefer alerts. Risks Periodically reviewing logs can mitigate the following information security risks: Information disclosure : Regularly reviewing logs can reveal an attack attempt or an ongoing attack. Downtime : Regularly reviewing logs can reveal misbehaving components (e.g., Pod restarts, various errors) and inform fixes before it leads to downtime. Silent corruption : Regularly reviewing logs can reveal data corruption. How to do log review By review period , we mean the time elapsed since the last review of the logs, e.g., 30 days. Aim for a review which is both wide and deep . By wide we mean that you should vary the time interval, time point, filters, etc., when reviewing log entries. By deep we mean that you should actually read and try to understand a sample of logs. Open up a browser and open the Compliant Kubernetes logs of the cluster you are reviewing. This functionality is currently offered by Kibana and Elasticsearch. Search for the following keywords on all indices -- i.e., search over each index pattern -- over the last review period: error , failed , failure , deny , denied , blocked , invalid , expired , unable , unauthorized , bad , 401 , 403 , 500 , unknown . Sample a few keywords you recently encountered during your work, e.g., already installed or not found ; be creative and unpredictable. Vary the time point, the time interval, filters, etc. Go wide : For each query (index pattern, keyword, timepoint, time interval and filter combination), look at the timeline and see if there is an unexpected increase or decrease in the count of log lines. If you find any, focus your attention on those. Go deep : For each query, sample at least 10 log entries, read them and make sure you understand what they mean. Think about the following: What are potential causes? What are potential implications? Time: Do the entries appear periodically or randomly? Space: Does a specific component trigger them? Is the entry generated by the platform or the application? If anything catches your attention vary the time point, time interval and various filters to understand if the log entry is a risk indicator or not. Look for unknown unknowns . Any failures, especially authentication failures, which feature a significant increase are risk indicators. Contact the person owning the component, e.g., the application developer or Compliant Kubernetes architect, to better understand if the entry is suspecious or not. Perhaps it is due to a recent change -- as indicated by an operator log -- and indicates no risk. Possible resolutions If you found a suspecious activity, escalate. If the log entry is due to a bug in Compliant Kubernetes, file an issue.","title":"Log Review"},{"location":"ciso-guide/log-review/#log-review","text":"This document highlights the risks that can be mitigated by regularly reviewing logs and makes concrete recommendations on how to do log review.","title":"Log Review"},{"location":"ciso-guide/log-review/#relevant-regulations","text":"GDPR Article 32 : Taking into account the state of the art, the costs of implementation and the nature, scope, context and purposes of processing as well as the risk of varying likelihood and severity for the rights and freedoms of natural persons, the controller and the processor shall implement appropriate technical and organisational measures to ensure a level of security appropriate to the risk, including inter alia as appropriate: [...] a process for regularly testing, assessing and evaluating the effectiveness of technical and organisational measures for ensuring the security of the processing. HSLF-FS 2016:40 : 2 \u00a7 V\u00e5rdgivaren ska genom ledningssystemet s\u00e4kerst\u00e4lla att [...] 4. \u00e5tg\u00e4rder kan h\u00e4rledas till en anv\u00e4ndare (sp\u00e5rbarhet) i informationssystem som \u00e4r helt eller delvis automatiserade.","title":"Relevant Regulations"},{"location":"ciso-guide/log-review/#mapping-to-iso-27001-controls","text":"A.12.4.1 \"Event Logging\" A.12.4.3 \"Administrator and Operator Logs\"","title":"Mapping to ISO 27001 Controls"},{"location":"ciso-guide/log-review/#purpose","text":"Compliant Kubernetes captures application logs and audit logs in a tamper-proof logging environment, which we call the service cluster. By \"tamper-proof\", we mean that even a complete compromise of production infrastructure does not allow an attacker to erase or change existing log entries, as would be required to hide their activity and avoid suspecion. Note Attackers can, however, inject new \"weird\" logs entries. However, that wouldn't remove their tracks and would only trigger more suspecion. However, said logs only help with information security if they are regularly reviewed for suspicious activity. Prefer to use logs for catching \"unknown unknowns\". For known bad failures -- e.g., a fluentd Pod restarting -- prefer alerts.","title":"Purpose"},{"location":"ciso-guide/log-review/#risks","text":"Periodically reviewing logs can mitigate the following information security risks: Information disclosure : Regularly reviewing logs can reveal an attack attempt or an ongoing attack. Downtime : Regularly reviewing logs can reveal misbehaving components (e.g., Pod restarts, various errors) and inform fixes before it leads to downtime. Silent corruption : Regularly reviewing logs can reveal data corruption.","title":"Risks"},{"location":"ciso-guide/log-review/#how-to-do-log-review","text":"By review period , we mean the time elapsed since the last review of the logs, e.g., 30 days. Aim for a review which is both wide and deep . By wide we mean that you should vary the time interval, time point, filters, etc., when reviewing log entries. By deep we mean that you should actually read and try to understand a sample of logs. Open up a browser and open the Compliant Kubernetes logs of the cluster you are reviewing. This functionality is currently offered by Kibana and Elasticsearch. Search for the following keywords on all indices -- i.e., search over each index pattern -- over the last review period: error , failed , failure , deny , denied , blocked , invalid , expired , unable , unauthorized , bad , 401 , 403 , 500 , unknown . Sample a few keywords you recently encountered during your work, e.g., already installed or not found ; be creative and unpredictable. Vary the time point, the time interval, filters, etc. Go wide : For each query (index pattern, keyword, timepoint, time interval and filter combination), look at the timeline and see if there is an unexpected increase or decrease in the count of log lines. If you find any, focus your attention on those. Go deep : For each query, sample at least 10 log entries, read them and make sure you understand what they mean. Think about the following: What are potential causes? What are potential implications? Time: Do the entries appear periodically or randomly? Space: Does a specific component trigger them? Is the entry generated by the platform or the application? If anything catches your attention vary the time point, time interval and various filters to understand if the log entry is a risk indicator or not. Look for unknown unknowns . Any failures, especially authentication failures, which feature a significant increase are risk indicators. Contact the person owning the component, e.g., the application developer or Compliant Kubernetes architect, to better understand if the entry is suspecious or not. Perhaps it is due to a recent change -- as indicated by an operator log -- and indicates no risk.","title":"How to do log review"},{"location":"ciso-guide/log-review/#possible-resolutions","text":"If you found a suspecious activity, escalate. If the log entry is due to a bug in Compliant Kubernetes, file an issue.","title":"Possible resolutions"},{"location":"ciso-guide/network-security/","text":"Network Security Dashboard Relevant Regulations GDPR Article 32 : Taking into account the state of the art [...] the controller and the processor shall implement [...] as appropriate [...] encryption of personal data; In assessing the appropriate level of security account shall be taken in particular of the risks that are presented by processing, in particular from accidental or unlawful destruction, loss, alteration, unauthorised disclosure of, or access to personal data transmitted , stored or otherwise processed. [highlights added] HIPAA Part 164\u2014SECURITY AND PRIVACY (2) Protect against any reasonably anticipated threats or hazards to the security or integrity of such information. Mapping to ISO 27001 Controls A.13 Communications Security Compliant Kubernetes Network Security Dashboard The Compliant Kubernetes Network Security Dashboard allows to audit violations of NetworkPolicies (i.e., \"firewall rules\"). In the best case, denied traffic indicates a misconfiguration. In worst case, denied traffic indicates an ongoing security attack. Significant or unexpected increases of allowed traffic should also be closely monitored. In best case, these may indicate inefficient application code which may cause capacity issues later. In worst case, these may indicate an attempt to exfiltrate large amounts of data or to use the cluster as a reflector for an amplification attack . Therefore, this dashboard should be regularly reviewed, perhaps even daily. Handling Non-Compliance Make sure you have a proper incident management policy in place. If an attack is ongoing, it might be better to take the system offline to protect data from getting in the wrong hands. Operators need to be trained on what events justify such an extreme action, otherwise, escalating the issue along the reporting chain may add delays that favor the attacker. In less severe cases, simply contact the developers to investigate their code, fix needless communication attempts or update their NetworkPolicies accordingly to fix any potential misconfiguration. Further Reading Network Policies","title":"Network Security"},{"location":"ciso-guide/network-security/#network-security-dashboard","text":"","title":"Network Security Dashboard"},{"location":"ciso-guide/network-security/#relevant-regulations","text":"GDPR Article 32 : Taking into account the state of the art [...] the controller and the processor shall implement [...] as appropriate [...] encryption of personal data; In assessing the appropriate level of security account shall be taken in particular of the risks that are presented by processing, in particular from accidental or unlawful destruction, loss, alteration, unauthorised disclosure of, or access to personal data transmitted , stored or otherwise processed. [highlights added] HIPAA Part 164\u2014SECURITY AND PRIVACY (2) Protect against any reasonably anticipated threats or hazards to the security or integrity of such information.","title":"Relevant Regulations"},{"location":"ciso-guide/network-security/#mapping-to-iso-27001-controls","text":"A.13 Communications Security","title":"Mapping to ISO 27001 Controls"},{"location":"ciso-guide/network-security/#compliant-kubernetes-network-security-dashboard","text":"The Compliant Kubernetes Network Security Dashboard allows to audit violations of NetworkPolicies (i.e., \"firewall rules\"). In the best case, denied traffic indicates a misconfiguration. In worst case, denied traffic indicates an ongoing security attack. Significant or unexpected increases of allowed traffic should also be closely monitored. In best case, these may indicate inefficient application code which may cause capacity issues later. In worst case, these may indicate an attempt to exfiltrate large amounts of data or to use the cluster as a reflector for an amplification attack . Therefore, this dashboard should be regularly reviewed, perhaps even daily.","title":"Compliant Kubernetes Network Security Dashboard"},{"location":"ciso-guide/network-security/#handling-non-compliance","text":"Make sure you have a proper incident management policy in place. If an attack is ongoing, it might be better to take the system offline to protect data from getting in the wrong hands. Operators need to be trained on what events justify such an extreme action, otherwise, escalating the issue along the reporting chain may add delays that favor the attacker. In less severe cases, simply contact the developers to investigate their code, fix needless communication attempts or update their NetworkPolicies accordingly to fix any potential misconfiguration.","title":"Handling Non-Compliance"},{"location":"ciso-guide/network-security/#further-reading","text":"Network Policies","title":"Further Reading"},{"location":"ciso-guide/policy-as-code/","text":"Policy-as-Code Dashboard Relevant Regulations Although \"policy-as-code\" is not explicit in any regulation, enforcing policies in a consistent technical manner (\"policy-as-code\") is seen as an important strategy to reduce compliance violations, as well as reduce the overhead of complying. Mapping to ISO 27001 Controls A.18.2.2 Compliance with Security Policies & Standards A.18.2.3 Technical Compliance Review Compliant Kubernetes Policy-as-Code Dashboard Some of your policies are best enforced in code, e.g., Ingress resources do not have encryption set up or PersistentVolumeClaims do not have the necessary backup annotations. Setting up such policies as code is highly dependent on your organization, your risk appetite and your operations. Policies that make sense enforcing by code may be required in some organizations, whereas others might see it as unnecessary and prefer simply treat codified policies as aspirational. Whatever your situation, the Compliant Kubernetes Policy-as-Code Dashboard allows to quickly audit what Kubernetes resources are set up in a non-compliant way or how many policy violations were avoided by Compliant Kubernetes. Handling Non-Compliance If an application or user keeps violating a policy, start by reviewing the policy. If the policy seems well codified, contact the developer or the application owner to determine why policy violations occurs or need to be prevented by Compliant Kubernetes. If a policy is missing or too strict, contact the Compliant Kubernetes administrators.","title":"Policy-as-Code"},{"location":"ciso-guide/policy-as-code/#policy-as-code-dashboard","text":"","title":"Policy-as-Code Dashboard"},{"location":"ciso-guide/policy-as-code/#relevant-regulations","text":"Although \"policy-as-code\" is not explicit in any regulation, enforcing policies in a consistent technical manner (\"policy-as-code\") is seen as an important strategy to reduce compliance violations, as well as reduce the overhead of complying.","title":"Relevant Regulations"},{"location":"ciso-guide/policy-as-code/#mapping-to-iso-27001-controls","text":"A.18.2.2 Compliance with Security Policies & Standards A.18.2.3 Technical Compliance Review","title":"Mapping to ISO 27001 Controls"},{"location":"ciso-guide/policy-as-code/#compliant-kubernetes-policy-as-code-dashboard","text":"Some of your policies are best enforced in code, e.g., Ingress resources do not have encryption set up or PersistentVolumeClaims do not have the necessary backup annotations. Setting up such policies as code is highly dependent on your organization, your risk appetite and your operations. Policies that make sense enforcing by code may be required in some organizations, whereas others might see it as unnecessary and prefer simply treat codified policies as aspirational. Whatever your situation, the Compliant Kubernetes Policy-as-Code Dashboard allows to quickly audit what Kubernetes resources are set up in a non-compliant way or how many policy violations were avoided by Compliant Kubernetes.","title":"Compliant Kubernetes Policy-as-Code Dashboard"},{"location":"ciso-guide/policy-as-code/#handling-non-compliance","text":"If an application or user keeps violating a policy, start by reviewing the policy. If the policy seems well codified, contact the developer or the application owner to determine why policy violations occurs or need to be prevented by Compliant Kubernetes. If a policy is missing or too strict, contact the Compliant Kubernetes administrators.","title":"Handling Non-Compliance"},{"location":"ciso-guide/vulnerability/","text":"Vulnerability Dashboard Relevant Regulations GDPR Article 32 : Taking into account the state of the art [...] the controller and the processor shall implement [...] as appropriate [...] a process for regularly testing, assessing and evaluating the effectiveness of technical and organisational measures for ensuring the security of the processing. In assessing the appropriate level of security account shall be taken in particular of the risks that are presented by processing, in particular from accidental or unlawful destruction, loss, alteration, unauthorised disclosure of, or access to personal data transmitted , stored or otherwise processed. [highlights added] HIPAA Part 164\u2014SECURITY AND PRIVACY (2) Protect against any reasonably anticipated threats or hazards to the security or integrity of such information. Mapping to ISO 27001 Controls A.12.6.1 Management of Technical Vulnerabilities Compliant Kubernetes Vulnerability Dashboard The Compliant Kubernetes Vulnerability Dashboard allows to audit what vulnerable container images are running in production. The dashboard allows to asses increase or decrease of exposure over time. It also allows to prioritize vulnerabilities based on CVE score (CVSS). Therefore, this dashboard should be regularly reviewed, perhaps even daily. A vulnerability management process should be in place to decide how to systematically handle vulnerabilities. Handling Non-Compliance Containers should preferably be redeployed with an image that received the necessary security fixes. In case the security fix cannot be deployed in a timely manner -- e.g., due to a slow fix from the vendor -- then the affected containers should be terminated. In all cases, isolating a container using NetworkPolicies, non-root user accounts, no service account token, etc. can make a vulnerability more difficult to exploit. Further Reading Vulnerability management CVE CVSS Starboard","title":"Vulnerability Management"},{"location":"ciso-guide/vulnerability/#vulnerability-dashboard","text":"","title":"Vulnerability Dashboard"},{"location":"ciso-guide/vulnerability/#relevant-regulations","text":"GDPR Article 32 : Taking into account the state of the art [...] the controller and the processor shall implement [...] as appropriate [...] a process for regularly testing, assessing and evaluating the effectiveness of technical and organisational measures for ensuring the security of the processing. In assessing the appropriate level of security account shall be taken in particular of the risks that are presented by processing, in particular from accidental or unlawful destruction, loss, alteration, unauthorised disclosure of, or access to personal data transmitted , stored or otherwise processed. [highlights added] HIPAA Part 164\u2014SECURITY AND PRIVACY (2) Protect against any reasonably anticipated threats or hazards to the security or integrity of such information.","title":"Relevant Regulations"},{"location":"ciso-guide/vulnerability/#mapping-to-iso-27001-controls","text":"A.12.6.1 Management of Technical Vulnerabilities","title":"Mapping to ISO 27001 Controls"},{"location":"ciso-guide/vulnerability/#compliant-kubernetes-vulnerability-dashboard","text":"The Compliant Kubernetes Vulnerability Dashboard allows to audit what vulnerable container images are running in production. The dashboard allows to asses increase or decrease of exposure over time. It also allows to prioritize vulnerabilities based on CVE score (CVSS). Therefore, this dashboard should be regularly reviewed, perhaps even daily. A vulnerability management process should be in place to decide how to systematically handle vulnerabilities.","title":"Compliant Kubernetes Vulnerability Dashboard"},{"location":"ciso-guide/vulnerability/#handling-non-compliance","text":"Containers should preferably be redeployed with an image that received the necessary security fixes. In case the security fix cannot be deployed in a timely manner -- e.g., due to a slow fix from the vendor -- then the affected containers should be terminated. In all cases, isolating a container using NetworkPolicies, non-root user accounts, no service account token, etc. can make a vulnerability more difficult to exploit.","title":"Handling Non-Compliance"},{"location":"ciso-guide/vulnerability/#further-reading","text":"Vulnerability management CVE CVSS Starboard","title":"Further Reading"},{"location":"contributor-guide/","text":"Contributor guide Definition of Done When working in regulated industries, it is really important to have the bar high for when something can be called \"done\". In Compliant Kubernetes, we use the following definition of done: Code and documentation is merged on the main branch of upstream projects. This may cause time delays which are outside your control. However, if we cannot convince upstream projects to take our contributions, then we better know about this as soon as possible. A Compliant Kubernetes relying on an abandoned upstream branch is unsustainable. Code is merged in the Compliant Kubernetes project. Documentation is up-to-date. IT systems used in regulated industries need to have documentation. (See ISO 27001 A.12.1.1 \"Documented Operating Procedures\" ). You may either point to upstream documentation -- if Compliant Kubernetes does not add any specifics -- or write a dedicated section/page. Prefer to refer to upstream documentation -- potentially updating that one -- instead of duplicating it in Compliant Kubernetes. You provide evidence for completion. This can be terminal output, screenshot or -- even better, but more time consuming -- a screencast with voice-over explanations. Ideally, these should be attached in the PR to convince the reviewer that the code and documentation are as intended. Submitting PRs To make the review process as smooth as possible for everyone we have some steps that we'd like you to follow Look through our DEVELOPMENT.md The pre-commit hook will run on all PRs to main , so either make sure to have it installed by running: pre-commit install Or manually run it before committing pre-commit run Make sure to follow the PR template, see this for more details. Alternatively start a PR and you'll see it there. Setting up your environment To install all required tools, please follow the instructions here . Tips and tricks To make your life easier we suggest to use language server for the language that you're editing. E.g. terraform: terraform-ls yaml: yaml-language-server To catch pre-commit errors early, direct in your editor, it's also suggested to install plugins for these tools. markdownlint shellcheck When developing and you only working on a single application it will be faster to only deploy that application instead of applying all charts. This can be done by figuring out the app label for the application in question by running: bin/ck8s ops helmfile {wc|sc} list When you figured out the app label (lets say it's dex in this case) you can check the diff of your work by running: bin/ck8s ops helmfile {wc|sc} -l app=dex diff Instead of running helmfile apply , it might be useful to run helmfile sync . This will do a 3-way upgrade and make sure that the helm state matches the objects actually running in kubernetes. This will make sure that you haven't manually edited something for debugging and forgot about it. bin/ck8s ops helmfile {wc|sc} -l app=dex sync Object storage To make creating and deletion of buckets easy, we've a script to help you with that, see here (the quickstart has instructions on how to use it) . DNS These following snippets can be used to setup/remove all DNS records required for ck8s using exoscales cli. Start by setting up some variables: DOMAIN=\"example.com\" IP=\"203.0.113.123\" # IP to LB/ingress endpoint for the service cluster CK8S_ENVIRONMENT_NAME=\"my-cluster-name\" SUBDOMAINS=( \"*.ops.${CK8S_ENVIRONMENT_NAME}\" \"grafana.${CK8S_ENVIRONMENT_NAME}\" \"harbor.${CK8S_ENVIRONMENT_NAME}\" \"kibana.${CK8S_ENVIRONMENT_NAME}\" \"dex.${CK8S_ENVIRONMENT_NAME}\" \"notary.harbor.${CK8S_ENVIRONMENT_NAME}\" ) # Adding the A records for SUBDOMAIN in \"${SUBDOMAINS[@]}\"; do exo dns add A \"${DOMAIN}\" -a \"${IP}\" -n \"${SUBDOMAIN}\" done # Removing the records for SUBDOMAIN in \"${SUBDOMAINS[@]}\"; do exo dns remove \"${DOMAIN}\" \"${SUBDOMAIN}\" done Reusing clusters If you for some reason need to reinstall Compliant Kubernetes from scratch, we have some scripts that removes all objects created by this repo. The scripts can be found here (clean-sc.sh and clean-wc.sh) .","title":"Overview"},{"location":"contributor-guide/#contributor-guide","text":"","title":"Contributor guide"},{"location":"contributor-guide/#definition-of-done","text":"When working in regulated industries, it is really important to have the bar high for when something can be called \"done\". In Compliant Kubernetes, we use the following definition of done: Code and documentation is merged on the main branch of upstream projects. This may cause time delays which are outside your control. However, if we cannot convince upstream projects to take our contributions, then we better know about this as soon as possible. A Compliant Kubernetes relying on an abandoned upstream branch is unsustainable. Code is merged in the Compliant Kubernetes project. Documentation is up-to-date. IT systems used in regulated industries need to have documentation. (See ISO 27001 A.12.1.1 \"Documented Operating Procedures\" ). You may either point to upstream documentation -- if Compliant Kubernetes does not add any specifics -- or write a dedicated section/page. Prefer to refer to upstream documentation -- potentially updating that one -- instead of duplicating it in Compliant Kubernetes. You provide evidence for completion. This can be terminal output, screenshot or -- even better, but more time consuming -- a screencast with voice-over explanations. Ideally, these should be attached in the PR to convince the reviewer that the code and documentation are as intended.","title":"Definition of Done"},{"location":"contributor-guide/#submitting-prs","text":"To make the review process as smooth as possible for everyone we have some steps that we'd like you to follow Look through our DEVELOPMENT.md The pre-commit hook will run on all PRs to main , so either make sure to have it installed by running: pre-commit install Or manually run it before committing pre-commit run Make sure to follow the PR template, see this for more details. Alternatively start a PR and you'll see it there.","title":"Submitting PRs"},{"location":"contributor-guide/#setting-up-your-environment","text":"To install all required tools, please follow the instructions here .","title":"Setting up your environment"},{"location":"contributor-guide/#tips-and-tricks","text":"To make your life easier we suggest to use language server for the language that you're editing. E.g. terraform: terraform-ls yaml: yaml-language-server To catch pre-commit errors early, direct in your editor, it's also suggested to install plugins for these tools. markdownlint shellcheck When developing and you only working on a single application it will be faster to only deploy that application instead of applying all charts. This can be done by figuring out the app label for the application in question by running: bin/ck8s ops helmfile {wc|sc} list When you figured out the app label (lets say it's dex in this case) you can check the diff of your work by running: bin/ck8s ops helmfile {wc|sc} -l app=dex diff Instead of running helmfile apply , it might be useful to run helmfile sync . This will do a 3-way upgrade and make sure that the helm state matches the objects actually running in kubernetes. This will make sure that you haven't manually edited something for debugging and forgot about it. bin/ck8s ops helmfile {wc|sc} -l app=dex sync","title":"Tips and tricks"},{"location":"contributor-guide/#object-storage","text":"To make creating and deletion of buckets easy, we've a script to help you with that, see here (the quickstart has instructions on how to use it) .","title":"Object storage"},{"location":"contributor-guide/#dns","text":"These following snippets can be used to setup/remove all DNS records required for ck8s using exoscales cli. Start by setting up some variables: DOMAIN=\"example.com\" IP=\"203.0.113.123\" # IP to LB/ingress endpoint for the service cluster CK8S_ENVIRONMENT_NAME=\"my-cluster-name\" SUBDOMAINS=( \"*.ops.${CK8S_ENVIRONMENT_NAME}\" \"grafana.${CK8S_ENVIRONMENT_NAME}\" \"harbor.${CK8S_ENVIRONMENT_NAME}\" \"kibana.${CK8S_ENVIRONMENT_NAME}\" \"dex.${CK8S_ENVIRONMENT_NAME}\" \"notary.harbor.${CK8S_ENVIRONMENT_NAME}\" ) # Adding the A records for SUBDOMAIN in \"${SUBDOMAINS[@]}\"; do exo dns add A \"${DOMAIN}\" -a \"${IP}\" -n \"${SUBDOMAIN}\" done # Removing the records for SUBDOMAIN in \"${SUBDOMAINS[@]}\"; do exo dns remove \"${DOMAIN}\" \"${SUBDOMAIN}\" done","title":"DNS"},{"location":"contributor-guide/#reusing-clusters","text":"If you for some reason need to reinstall Compliant Kubernetes from scratch, we have some scripts that removes all objects created by this repo. The scripts can be found here (clean-sc.sh and clean-wc.sh) .","title":"Reusing clusters"},{"location":"operator-manual/","text":"Operator Manual Overview This manual is for Compliant Kubernetes administrators. Operators can be described via the following user stories: As an administrator I want to create/destroy/upgrade a Compliant Kubernetes cluster. As an administrator I want to re-configure a Compliant Kubernetes cluster. As an on-call administrator I want to be alerted when abnormal activity is detected, suggesting a pending intrusion. As an on-call administrator I want to be alerted when the Compliant Kubernetes cluster is unhealthy. As an on-call administrator I want \"break glass\" to investigate and recover an unhealthy Compliant Kubernetes cluster.","title":"Overview"},{"location":"operator-manual/#operator-manual-overview","text":"This manual is for Compliant Kubernetes administrators. Operators can be described via the following user stories: As an administrator I want to create/destroy/upgrade a Compliant Kubernetes cluster. As an administrator I want to re-configure a Compliant Kubernetes cluster. As an on-call administrator I want to be alerted when abnormal activity is detected, suggesting a pending intrusion. As an on-call administrator I want to be alerted when the Compliant Kubernetes cluster is unhealthy. As an on-call administrator I want \"break glass\" to investigate and recover an unhealthy Compliant Kubernetes cluster.","title":"Operator Manual Overview"},{"location":"operator-manual/access-control/","text":"Access control This guide describes how to set up and make use of group claims for applications. Note This guide assumes your group claim name is groups Kubernetes To set up kubelogin to fetch and use groups make sure that your kubeconfig looks something like this. users : - name : user@my-cluster user : exec : apiVersion : client.authentication.k8s.io/v1beta1 args : - oidc-login - get-token - --oidc-issuer-url=https://dex.my-cluster-domain.com - --oidc-client-id=my-client-id - --oidc-client-secret=my-client-secret - --oidc-extra-scope=email,groups # Make sure groups are here command : kubectl Tips Your token can be found in ~/.kube/cache/oidc-login/ . This is useful if you're trying to debug your claims since you can just paste the token to jwt.io and check it. Example: $ ls ~/.kube/cache/oidc-login/ $ kubectl get pod <log in> $ ls ~/.kube/cache/oidc-login/ 13b165965d8e80749ce3b8d442da3e4e9f5ff5e38900ef104eee99fde85a39d4 $ cat ~/.kube/cache/oidc-login/13b165965d8e80749ce3b8d442da3e4e9f5ff5e38900ef104eee99fde85a39d4 | jq -r .id_token eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2RleC5teS1jbHVzdGVyLWRvbWFpbi5jb20iLCJpYXQiOjE2MjE1MTUxNzcsImV4cCI6MTY1MzEzNzU3NywiYXVkIjoibXktY2xpZW50LWlkIiwic3ViIjoiSGlVUE92S1BKMmVwWUkwR1R1U0JYWGRxYTJTV2ZxRnc1ZjBXNVBQeThTWSIsIm5vdW5jZSI6IkNoVXhNRFk0TVRZNE1qRXpORFUzTURVM01ERXlNREFTQm1kdmIyZHNaUSIsImF0X2hhc2giOiI1aUZjbF9Sc1JvblhHekZaMU0xQ2JnIiwiZW1haWwiOiJ1c2VyQG15LWRvbWFpbi5jb20iLCJlbWFpbF92ZXJpZmllZCI6InRydWUiLCJncm91cHMiOlsibXktZ3JvdXAtb25lIiwibXktZ3JvdXAtdHdvIl19.s65Aowfn6B1PiyQvRGPRu9KgX7G39nkLtx6yCAEElao Copy the token to jwt.io and ensure that the payload includes the expected groups claim. Kibana To enable kibana to use the groups for kibana access. elasticsearch : sso : scope : \"... groups\" # Add groups to existing extraRoleMappings : - mapping_name : kibana_user definition : backend_roles : - my-group-name - mapping_name : kubernetes_log_reader definition : backend_roles : - my-group-name - mapping_name : readall_and_monitor definition : backend_roles : - my-group-name Harbor Set correct group claim name since the default scopes includes groups already. This groups can be assigned to projects or as admin group. harbor : oidc : groupClaimName : groups Grafana Note This section assumes that elastisys/compliantkubernetes-apps/pull/450 is merged OPS Grafana prometheus : grafana : oidc : enabled : true userGroups : grafanaAdmin : my-admin-group grafanaEditor : my-editor-group grafanaViewer : my-viewer-group scopes : \".... groups\" # Add groups to existing allowedDomains : - my-domain.com User Grafana user : grafana : oidc : scopes : \"... groups\" # Add groups to existing allowedDomains : - my-domain.com userGroups : grafanaAdmin : my-admin-group grafanaEditor : my-editor-group grafanaViewer : my-viewer-group","title":"Access Control"},{"location":"operator-manual/access-control/#access-control","text":"This guide describes how to set up and make use of group claims for applications. Note This guide assumes your group claim name is groups","title":"Access control"},{"location":"operator-manual/access-control/#kubernetes","text":"To set up kubelogin to fetch and use groups make sure that your kubeconfig looks something like this. users : - name : user@my-cluster user : exec : apiVersion : client.authentication.k8s.io/v1beta1 args : - oidc-login - get-token - --oidc-issuer-url=https://dex.my-cluster-domain.com - --oidc-client-id=my-client-id - --oidc-client-secret=my-client-secret - --oidc-extra-scope=email,groups # Make sure groups are here command : kubectl Tips Your token can be found in ~/.kube/cache/oidc-login/ . This is useful if you're trying to debug your claims since you can just paste the token to jwt.io and check it. Example: $ ls ~/.kube/cache/oidc-login/ $ kubectl get pod <log in> $ ls ~/.kube/cache/oidc-login/ 13b165965d8e80749ce3b8d442da3e4e9f5ff5e38900ef104eee99fde85a39d4 $ cat ~/.kube/cache/oidc-login/13b165965d8e80749ce3b8d442da3e4e9f5ff5e38900ef104eee99fde85a39d4 | jq -r .id_token eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2RleC5teS1jbHVzdGVyLWRvbWFpbi5jb20iLCJpYXQiOjE2MjE1MTUxNzcsImV4cCI6MTY1MzEzNzU3NywiYXVkIjoibXktY2xpZW50LWlkIiwic3ViIjoiSGlVUE92S1BKMmVwWUkwR1R1U0JYWGRxYTJTV2ZxRnc1ZjBXNVBQeThTWSIsIm5vdW5jZSI6IkNoVXhNRFk0TVRZNE1qRXpORFUzTURVM01ERXlNREFTQm1kdmIyZHNaUSIsImF0X2hhc2giOiI1aUZjbF9Sc1JvblhHekZaMU0xQ2JnIiwiZW1haWwiOiJ1c2VyQG15LWRvbWFpbi5jb20iLCJlbWFpbF92ZXJpZmllZCI6InRydWUiLCJncm91cHMiOlsibXktZ3JvdXAtb25lIiwibXktZ3JvdXAtdHdvIl19.s65Aowfn6B1PiyQvRGPRu9KgX7G39nkLtx6yCAEElao Copy the token to jwt.io and ensure that the payload includes the expected groups claim.","title":"Kubernetes"},{"location":"operator-manual/access-control/#kibana","text":"To enable kibana to use the groups for kibana access. elasticsearch : sso : scope : \"... groups\" # Add groups to existing extraRoleMappings : - mapping_name : kibana_user definition : backend_roles : - my-group-name - mapping_name : kubernetes_log_reader definition : backend_roles : - my-group-name - mapping_name : readall_and_monitor definition : backend_roles : - my-group-name","title":"Kibana"},{"location":"operator-manual/access-control/#harbor","text":"Set correct group claim name since the default scopes includes groups already. This groups can be assigned to projects or as admin group. harbor : oidc : groupClaimName : groups","title":"Harbor"},{"location":"operator-manual/access-control/#grafana","text":"Note This section assumes that elastisys/compliantkubernetes-apps/pull/450 is merged","title":"Grafana"},{"location":"operator-manual/access-control/#ops-grafana","text":"prometheus : grafana : oidc : enabled : true userGroups : grafanaAdmin : my-admin-group grafanaEditor : my-editor-group grafanaViewer : my-viewer-group scopes : \".... groups\" # Add groups to existing allowedDomains : - my-domain.com","title":"OPS Grafana"},{"location":"operator-manual/access-control/#user-grafana","text":"user : grafana : oidc : scopes : \"... groups\" # Add groups to existing allowedDomains : - my-domain.com userGroups : grafanaAdmin : my-admin-group grafanaEditor : my-editor-group grafanaViewer : my-viewer-group","title":"User Grafana"},{"location":"operator-manual/aws/","text":"Compliant Kubernetes Deployment on AWS This document describes how to set up Compliant Kubernetes on AWS. The setup has two major parts: Deploying at least two vanilla Kubernetes clusters Deploying Compliant Kubernetes apps Before starting, make sure you have all necessary tools . Note This guide is written for compliantkubernetes-apps v0.17.0 Setup Choose names for your service cluster and workload clusters, as well as the DNS domain to expose the services inside the service cluster: SERVICE_CLUSTER = \"testsc\" WORKLOAD_CLUSTERS =( \"testwc0\" ) BASE_DOMAIN = \"example.com\" Note If you want to set up multiple workload clusters you can add more names. E.g. WORKLOAD_CLUSTERS=( \"testwc0\" \"testwc1\" \"testwc2\" ) SERVICE_CLUSTER and each entry in WORKLOAD_CLUSTERS must be maximum 17 characters long. Deploying vanilla Kubernetes clusters We suggest to set up Kubernetes clusters using kubespray. If you haven't done so already, clone the Elastisys Compliant Kubernetes Kubespray repo as follows: git clone --recursive https://github.com/elastisys/compliantkubernetes-kubespray cd compliantkubernetes-kubespray Infrastructure Setup using Terraform Note This step will also create the necessary IAM Roles for control plane Nodes to make integration with the cloud provider work. This will ensure that both Service Type LoadBalancer and PersistentVolumes (backed by AWS EBS volumes) will work. The necessary credentials are pulled automatically by control plane Nodes via AWS EC2 instance metadata and require no other configuration. Expose AWS credentials to Terraform We suggest exposing AWS credentials to Terraform via environment variables, so they are not accidentally left on the file-system: export TF_VAR_AWS_ACCESS_KEY_ID = \"xyz\" # Access key for AWS export TF_VAR_AWS_SECRET_ACCESS_KEY = \"zyx\" # Secret key for AWS export TF_VAR_AWS_SSH_KEY_NAME = \"foo\" # Name of the AWS key pair to use for the EC2 instances export TF_VAR_AWS_DEFAULT_REGION = \"bar\" # Region to use for all AWS resources Tip We suggest generating the SSH key locally, then importing it to AWS. Customize your infrastructure Create a configuration for the service cluster and the workload cluster: pushd kubespray for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do cat contrib/terraform/aws/terraform.tfvars \\ | sed \\ -e \"s@^aws_cluster_name =.*@aws_cluster_name = \\\" $CLUSTER \\\"@\" \\ -e \"s@^inventory_file =.*@inventory_file = \\\"../../../inventory/hosts- $CLUSTER \\\"@\" \\ -e \"s@^aws_kube_worker_size =.*@aws_kube_worker_size = \\\"t3.large\\\"@\" \\ > inventory/terraform- $CLUSTER .tfvars done popd Review and, if needed, adjust the files in kubespray/inventory/ . Initialize and Apply Terraform pushd kubespray/contrib/terraform/aws terraform init for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do terraform apply \\ -var-file = ../../../inventory/terraform- $CLUSTER .tfvars \\ -auto-approve \\ -state = ../../../inventory/tfstate- $CLUSTER .tfstate done popd Important The Terraform state is stored in kubespray/inventory/tfstate-* . It is precious. Consider backing it up or using Terraform Cloud . Check that the Ansible inventory was properly generated ls -l kubespray/inventory/hosts-* You may also want to check the AWS Console if the infrastructure was created correctly: Deploying vanilla Kubernetes clusters using Kubespray With the infrastructure provisioned, we can now deploy both the sc and wc Kubernetes clusters using kubespray. Before trying any of the steps, make sure you are in the repo's root folder. Init the Kubespray config in your config path export CK8S_CONFIG_PATH = ~/.ck8s/aws export CK8S_PGP_FP = <your GPG key fingerprint> # retrieve with gpg --list-secret-keys for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ./bin/ck8s-kubespray init $CLUSTER aws $CK8S_PGP_FP done Copy the inventories generated by Terraform above in the right place for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do cp kubespray/inventory/hosts- $CLUSTER $CK8S_CONFIG_PATH / $CLUSTER -config/inventory.ini done Run kubespray to deploy the Kubernetes clusters for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ./bin/ck8s-kubespray apply $CLUSTER --flush-cache -e ansible_user = ubuntu done This may take up to 20 minutes per cluster. Correct the Kubernetes API IP addresses Find the DNS names of the load balancers fronting the API servers: grep apiserver_loadbalancer $CK8S_CONFIG_PATH /*-config/inventory.ini Locate the encrypted kubeconfigs kube_config_*.yaml and edit them using sops. Copy the URL of the load balancer from inventory files shown above into kube_config_*.yaml . Do not overwrite the port. for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops $CK8S_CONFIG_PATH /.state/kube_config_ $CLUSTER .yaml done Test access to the clusters as follows for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file $CK8S_CONFIG_PATH /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get nodes' done Deploying Compliant Kubernetes Apps Now that the Kubernetes clusters are up and running, we are ready to install the Compliant Kubernetes apps. Clone compliantkubernetes-apps and Install Pre-requisites If you haven't done so already, clone the compliantkubernetes-apps repo and install pre-requisites. git clone https://github.com/elastisys/compliantkubernetes-apps.git cd compliantkubernetes-apps ansible-playbook -e 'ansible_python_interpreter=/usr/bin/python3' --ask-become-pass --connection local --inventory 127 .0.0.1, get-requirements.yaml Initialize the apps configuration export CK8S_ENVIRONMENT_NAME = my-environment-name #export CK8S_FLAVOR=[dev|prod] # defaults to dev export CK8S_CONFIG_PATH = ~/.ck8s/my-cluster-path export CK8S_CLOUD_PROVIDER = # [exoscale|safespring|citycloud|aws|baremetal] export CK8S_PGP_FP = <your GPG key fingerprint> # retrieve with gpg --list-secret-keys ./bin/ck8s init Three files, sc-config.yaml and wc-config.yaml , and secrets.yaml , were generated in the ${CK8S_CONFIG_PATH} directory. ls -l $CK8S_CONFIG_PATH Configure the apps Edit the configuration files ${CK8S_CONFIG_PATH}/sc-config.yaml , ${CK8S_CONFIG_PATH}/wc-config.yaml and ${CK8S_CONFIG_PATH}/secrets.yaml and set the appropriate values for some of the configuration fields. Note that, the latter is encrypted. vim ${ CK8S_CONFIG_PATH } /sc-config.yaml vim ${ CK8S_CONFIG_PATH } /wc-config.yaml sops ${ CK8S_CONFIG_PATH } /secrets.yaml The following are the minimum change you should perform: # sc-config.yaml and wc-config.yaml global : baseDomain : \"set-me\" # set to $BASE_DOMAIN opsDomain : \"set-me\" # set to ops.$BASE_DOMAIN issuer : letsencrypt-prod objectStorage : type : \"s3\" s3 : region : \"set-me\" # Region for S3 buckets, e.g, eu-central-1 regionEndpoint : \"set-me\" # e.g., https://s3.us-west-1.amazonaws.com # sc-config.yaml harbor : oidc : groupClaimName : \"set-me\" # set to group claim name used by OIDC provider issuers : letsencrypt : prod : email : \"set-me\" # set this to an email to receive LetsEncrypt notifications staging : email : \"set-me\" # set this to an email to receive LetsEncrypt notifications # secrets.yaml objectStorage : s3 : accessKey : \"set-me\" #put your s3 accesskey secretKey : \"set-me\" #put your s3 secretKey Create placeholder DNS entries To avoid negative caching and other surprises. Create two placeholders as follows (feel free to use the \"Import zone\" feature of AWS Route53): echo \"\"\" *. $BASE_DOMAIN 60s A 203.0.113.123 *.ops. $BASE_DOMAIN 60s A 203.0.113.123 \"\"\" NOTE: 203.0.113.123 is in TEST-NET-3 and okey to use as placeholder. Install Compliant Kubernetes apps Start with the service cluster: ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ SERVICE_CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_sc.yaml ./bin/ck8s apply sc # Respond \"n\" if you get a WARN Then the workload clusters: for CLUSTER in \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_wc.yaml ./bin/ck8s apply wc # Respond \"n\" if you get a WARN done Settling Important Leave sufficient time for the system to settle, e.g., request TLS certificates from LetsEncrypt, perhaps as much as 20 minutes. You can check if the system settled as follows: for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get --all-namespaces pods' done Check the output of the command above. All Pods needs to be Running or Completed. for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get --all-namespaces issuers,clusterissuers,certificates' done Check the output of the command above. All resources need to have the Ready column True. Setup required DNS entries You will need to set up the following DNS entries. First, determine the public IP of the load-balancer fronting the Ingress controller of the service cluster : SC_INGRESS_LB_HOSTNAME = $( sops exec-file $CK8S_CONFIG_PATH /.state/kube_config_sc.yaml 'kubectl --kubeconfig {} get -n ingress-nginx svc ingress-nginx-controller -o jsonpath={.status.loadBalancer.ingress[0].hostname}' ) SC_INGRESS_LB_IP = $( dig +short $SC_INGRESS_LB_HOSTNAME | head -1 ) echo $SC_INGRESS_LB_IP Then, import the following zone in AWS Route53: echo \"\"\" *.ops. $BASE_DOMAIN 60s A $SC_INGRESS_LB_IP dex. $BASE_DOMAIN 60s A $SC_INGRESS_LB_IP grafana. $BASE_DOMAIN 60s A $SC_INGRESS_LB_IP harbor. $BASE_DOMAIN 60s A $SC_INGRESS_LB_IP kibana. $BASE_DOMAIN 60s A $SC_INGRESS_LB_IP \"\"\" Testing After completing the installation step you can test if the apps are properly installed and ready using the commands below. Start with the service cluster: ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ SERVICE_CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_sc.yaml ./bin/ck8s test sc # Respond \"n\" if you get a WARN Then the workload clusters: for CLUSTER in \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_wc.yaml ./bin/ck8s test wc # Respond \"n\" if you get a WARN done Done. Navigate to the endpoints, for example grafana.$BASE_DOMAIN , kibana.$BASE_DOMAIN , harbor.$BASE_DOMAIN , etc. to discover Compliant Kubernetes's features. Teardown Removing Compliant Kubernetes Apps from your cluster To remove the applications added by compliant kubernetes you can use the two scripts clean-sc.sh and clean-wc.sh , they are located here in the scripts folder . They perform the following actions: Delete the added helm charts Delete the added namespaces Delete any remaining PersistentVolumes Delete the added CustomResourceDefinitions Note: if user namespaces are managed by Compliant Kubernetes apps then they will also be deleted if you clean up the workload cluster. Remove infrastructure Note Even if you want to completely destroy the cluster with all its infrastructure, it is recommended to first execute the clean scripts described above, otherwise resources created by the cloud controller (e.g. volumes and loadbalancers) are not removed and terraform destroy might fail. pushd kubespray/contrib/terraform/aws for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do terraform destroy \\ -auto-approve \\ -state = ../../../inventory/tfstate- $CLUSTER .tfstate done popd Further Reading Compliant Kubernetes apps repo Configurations option","title":"On AWS"},{"location":"operator-manual/aws/#compliant-kubernetes-deployment-on-aws","text":"This document describes how to set up Compliant Kubernetes on AWS. The setup has two major parts: Deploying at least two vanilla Kubernetes clusters Deploying Compliant Kubernetes apps Before starting, make sure you have all necessary tools . Note This guide is written for compliantkubernetes-apps v0.17.0","title":"Compliant Kubernetes Deployment on AWS"},{"location":"operator-manual/aws/#setup","text":"Choose names for your service cluster and workload clusters, as well as the DNS domain to expose the services inside the service cluster: SERVICE_CLUSTER = \"testsc\" WORKLOAD_CLUSTERS =( \"testwc0\" ) BASE_DOMAIN = \"example.com\" Note If you want to set up multiple workload clusters you can add more names. E.g. WORKLOAD_CLUSTERS=( \"testwc0\" \"testwc1\" \"testwc2\" ) SERVICE_CLUSTER and each entry in WORKLOAD_CLUSTERS must be maximum 17 characters long.","title":"Setup"},{"location":"operator-manual/aws/#deploying-vanilla-kubernetes-clusters","text":"We suggest to set up Kubernetes clusters using kubespray. If you haven't done so already, clone the Elastisys Compliant Kubernetes Kubespray repo as follows: git clone --recursive https://github.com/elastisys/compliantkubernetes-kubespray cd compliantkubernetes-kubespray","title":"Deploying vanilla Kubernetes clusters"},{"location":"operator-manual/aws/#infrastructure-setup-using-terraform","text":"Note This step will also create the necessary IAM Roles for control plane Nodes to make integration with the cloud provider work. This will ensure that both Service Type LoadBalancer and PersistentVolumes (backed by AWS EBS volumes) will work. The necessary credentials are pulled automatically by control plane Nodes via AWS EC2 instance metadata and require no other configuration.","title":"Infrastructure Setup using Terraform"},{"location":"operator-manual/aws/#expose-aws-credentials-to-terraform","text":"We suggest exposing AWS credentials to Terraform via environment variables, so they are not accidentally left on the file-system: export TF_VAR_AWS_ACCESS_KEY_ID = \"xyz\" # Access key for AWS export TF_VAR_AWS_SECRET_ACCESS_KEY = \"zyx\" # Secret key for AWS export TF_VAR_AWS_SSH_KEY_NAME = \"foo\" # Name of the AWS key pair to use for the EC2 instances export TF_VAR_AWS_DEFAULT_REGION = \"bar\" # Region to use for all AWS resources Tip We suggest generating the SSH key locally, then importing it to AWS.","title":"Expose AWS credentials to Terraform"},{"location":"operator-manual/aws/#customize-your-infrastructure","text":"Create a configuration for the service cluster and the workload cluster: pushd kubespray for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do cat contrib/terraform/aws/terraform.tfvars \\ | sed \\ -e \"s@^aws_cluster_name =.*@aws_cluster_name = \\\" $CLUSTER \\\"@\" \\ -e \"s@^inventory_file =.*@inventory_file = \\\"../../../inventory/hosts- $CLUSTER \\\"@\" \\ -e \"s@^aws_kube_worker_size =.*@aws_kube_worker_size = \\\"t3.large\\\"@\" \\ > inventory/terraform- $CLUSTER .tfvars done popd Review and, if needed, adjust the files in kubespray/inventory/ .","title":"Customize your infrastructure"},{"location":"operator-manual/aws/#initialize-and-apply-terraform","text":"pushd kubespray/contrib/terraform/aws terraform init for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do terraform apply \\ -var-file = ../../../inventory/terraform- $CLUSTER .tfvars \\ -auto-approve \\ -state = ../../../inventory/tfstate- $CLUSTER .tfstate done popd Important The Terraform state is stored in kubespray/inventory/tfstate-* . It is precious. Consider backing it up or using Terraform Cloud .","title":"Initialize and Apply Terraform"},{"location":"operator-manual/aws/#check-that-the-ansible-inventory-was-properly-generated","text":"ls -l kubespray/inventory/hosts-* You may also want to check the AWS Console if the infrastructure was created correctly:","title":"Check that the Ansible inventory was properly generated"},{"location":"operator-manual/aws/#deploying-vanilla-kubernetes-clusters-using-kubespray","text":"With the infrastructure provisioned, we can now deploy both the sc and wc Kubernetes clusters using kubespray. Before trying any of the steps, make sure you are in the repo's root folder.","title":"Deploying vanilla Kubernetes clusters using Kubespray"},{"location":"operator-manual/aws/#init-the-kubespray-config-in-your-config-path","text":"export CK8S_CONFIG_PATH = ~/.ck8s/aws export CK8S_PGP_FP = <your GPG key fingerprint> # retrieve with gpg --list-secret-keys for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ./bin/ck8s-kubespray init $CLUSTER aws $CK8S_PGP_FP done","title":"Init the Kubespray config in your config path"},{"location":"operator-manual/aws/#copy-the-inventories-generated-by-terraform-above-in-the-right-place","text":"for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do cp kubespray/inventory/hosts- $CLUSTER $CK8S_CONFIG_PATH / $CLUSTER -config/inventory.ini done","title":"Copy the inventories generated by Terraform above in the right place"},{"location":"operator-manual/aws/#run-kubespray-to-deploy-the-kubernetes-clusters","text":"for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ./bin/ck8s-kubespray apply $CLUSTER --flush-cache -e ansible_user = ubuntu done This may take up to 20 minutes per cluster.","title":"Run kubespray to deploy the Kubernetes clusters"},{"location":"operator-manual/aws/#correct-the-kubernetes-api-ip-addresses","text":"Find the DNS names of the load balancers fronting the API servers: grep apiserver_loadbalancer $CK8S_CONFIG_PATH /*-config/inventory.ini Locate the encrypted kubeconfigs kube_config_*.yaml and edit them using sops. Copy the URL of the load balancer from inventory files shown above into kube_config_*.yaml . Do not overwrite the port. for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops $CK8S_CONFIG_PATH /.state/kube_config_ $CLUSTER .yaml done","title":"Correct the Kubernetes API IP addresses"},{"location":"operator-manual/aws/#test-access-to-the-clusters-as-follows","text":"for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file $CK8S_CONFIG_PATH /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get nodes' done","title":"Test access to the clusters as follows"},{"location":"operator-manual/aws/#deploying-compliant-kubernetes-apps","text":"Now that the Kubernetes clusters are up and running, we are ready to install the Compliant Kubernetes apps.","title":"Deploying Compliant Kubernetes Apps"},{"location":"operator-manual/aws/#clone-compliantkubernetes-apps-and-install-pre-requisites","text":"If you haven't done so already, clone the compliantkubernetes-apps repo and install pre-requisites. git clone https://github.com/elastisys/compliantkubernetes-apps.git cd compliantkubernetes-apps ansible-playbook -e 'ansible_python_interpreter=/usr/bin/python3' --ask-become-pass --connection local --inventory 127 .0.0.1, get-requirements.yaml","title":"Clone compliantkubernetes-apps and Install Pre-requisites"},{"location":"operator-manual/aws/#initialize-the-apps-configuration","text":"export CK8S_ENVIRONMENT_NAME = my-environment-name #export CK8S_FLAVOR=[dev|prod] # defaults to dev export CK8S_CONFIG_PATH = ~/.ck8s/my-cluster-path export CK8S_CLOUD_PROVIDER = # [exoscale|safespring|citycloud|aws|baremetal] export CK8S_PGP_FP = <your GPG key fingerprint> # retrieve with gpg --list-secret-keys ./bin/ck8s init Three files, sc-config.yaml and wc-config.yaml , and secrets.yaml , were generated in the ${CK8S_CONFIG_PATH} directory. ls -l $CK8S_CONFIG_PATH","title":"Initialize the apps configuration"},{"location":"operator-manual/aws/#configure-the-apps","text":"Edit the configuration files ${CK8S_CONFIG_PATH}/sc-config.yaml , ${CK8S_CONFIG_PATH}/wc-config.yaml and ${CK8S_CONFIG_PATH}/secrets.yaml and set the appropriate values for some of the configuration fields. Note that, the latter is encrypted. vim ${ CK8S_CONFIG_PATH } /sc-config.yaml vim ${ CK8S_CONFIG_PATH } /wc-config.yaml sops ${ CK8S_CONFIG_PATH } /secrets.yaml The following are the minimum change you should perform: # sc-config.yaml and wc-config.yaml global : baseDomain : \"set-me\" # set to $BASE_DOMAIN opsDomain : \"set-me\" # set to ops.$BASE_DOMAIN issuer : letsencrypt-prod objectStorage : type : \"s3\" s3 : region : \"set-me\" # Region for S3 buckets, e.g, eu-central-1 regionEndpoint : \"set-me\" # e.g., https://s3.us-west-1.amazonaws.com # sc-config.yaml harbor : oidc : groupClaimName : \"set-me\" # set to group claim name used by OIDC provider issuers : letsencrypt : prod : email : \"set-me\" # set this to an email to receive LetsEncrypt notifications staging : email : \"set-me\" # set this to an email to receive LetsEncrypt notifications # secrets.yaml objectStorage : s3 : accessKey : \"set-me\" #put your s3 accesskey secretKey : \"set-me\" #put your s3 secretKey","title":"Configure the apps"},{"location":"operator-manual/aws/#create-placeholder-dns-entries","text":"To avoid negative caching and other surprises. Create two placeholders as follows (feel free to use the \"Import zone\" feature of AWS Route53): echo \"\"\" *. $BASE_DOMAIN 60s A 203.0.113.123 *.ops. $BASE_DOMAIN 60s A 203.0.113.123 \"\"\" NOTE: 203.0.113.123 is in TEST-NET-3 and okey to use as placeholder.","title":"Create placeholder DNS entries"},{"location":"operator-manual/aws/#install-compliant-kubernetes-apps","text":"Start with the service cluster: ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ SERVICE_CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_sc.yaml ./bin/ck8s apply sc # Respond \"n\" if you get a WARN Then the workload clusters: for CLUSTER in \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_wc.yaml ./bin/ck8s apply wc # Respond \"n\" if you get a WARN done","title":"Install Compliant Kubernetes apps"},{"location":"operator-manual/aws/#settling","text":"Important Leave sufficient time for the system to settle, e.g., request TLS certificates from LetsEncrypt, perhaps as much as 20 minutes. You can check if the system settled as follows: for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get --all-namespaces pods' done Check the output of the command above. All Pods needs to be Running or Completed. for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get --all-namespaces issuers,clusterissuers,certificates' done Check the output of the command above. All resources need to have the Ready column True.","title":"Settling"},{"location":"operator-manual/aws/#setup-required-dns-entries","text":"You will need to set up the following DNS entries. First, determine the public IP of the load-balancer fronting the Ingress controller of the service cluster : SC_INGRESS_LB_HOSTNAME = $( sops exec-file $CK8S_CONFIG_PATH /.state/kube_config_sc.yaml 'kubectl --kubeconfig {} get -n ingress-nginx svc ingress-nginx-controller -o jsonpath={.status.loadBalancer.ingress[0].hostname}' ) SC_INGRESS_LB_IP = $( dig +short $SC_INGRESS_LB_HOSTNAME | head -1 ) echo $SC_INGRESS_LB_IP Then, import the following zone in AWS Route53: echo \"\"\" *.ops. $BASE_DOMAIN 60s A $SC_INGRESS_LB_IP dex. $BASE_DOMAIN 60s A $SC_INGRESS_LB_IP grafana. $BASE_DOMAIN 60s A $SC_INGRESS_LB_IP harbor. $BASE_DOMAIN 60s A $SC_INGRESS_LB_IP kibana. $BASE_DOMAIN 60s A $SC_INGRESS_LB_IP \"\"\"","title":"Setup required DNS entries"},{"location":"operator-manual/aws/#testing","text":"After completing the installation step you can test if the apps are properly installed and ready using the commands below. Start with the service cluster: ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ SERVICE_CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_sc.yaml ./bin/ck8s test sc # Respond \"n\" if you get a WARN Then the workload clusters: for CLUSTER in \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_wc.yaml ./bin/ck8s test wc # Respond \"n\" if you get a WARN done Done. Navigate to the endpoints, for example grafana.$BASE_DOMAIN , kibana.$BASE_DOMAIN , harbor.$BASE_DOMAIN , etc. to discover Compliant Kubernetes's features.","title":"Testing"},{"location":"operator-manual/aws/#teardown","text":"","title":"Teardown"},{"location":"operator-manual/aws/#removing-compliant-kubernetes-apps-from-your-cluster","text":"To remove the applications added by compliant kubernetes you can use the two scripts clean-sc.sh and clean-wc.sh , they are located here in the scripts folder . They perform the following actions: Delete the added helm charts Delete the added namespaces Delete any remaining PersistentVolumes Delete the added CustomResourceDefinitions Note: if user namespaces are managed by Compliant Kubernetes apps then they will also be deleted if you clean up the workload cluster.","title":"Removing Compliant Kubernetes Apps from your cluster"},{"location":"operator-manual/aws/#remove-infrastructure","text":"Note Even if you want to completely destroy the cluster with all its infrastructure, it is recommended to first execute the clean scripts described above, otherwise resources created by the cloud controller (e.g. volumes and loadbalancers) are not removed and terraform destroy might fail. pushd kubespray/contrib/terraform/aws for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do terraform destroy \\ -auto-approve \\ -state = ../../../inventory/tfstate- $CLUSTER .tfstate done popd","title":"Remove infrastructure"},{"location":"operator-manual/aws/#further-reading","text":"Compliant Kubernetes apps repo Configurations option","title":"Further Reading"},{"location":"operator-manual/azure/","text":"Compliant Kubernetes Deployment on Azure This document contains instructions on how to setup a service cluster and a workload cluster in Azure. The following are the main tasks addressed in this document: Infrastructure setup for two clusters: one service and one workload cluster Deploying Compliant Kubernetes on top of the two clusters. Creating DNS Records Deploying Rook Storage Orchestration Service Deploying Compliant Kubernetes apps Before starting, make sure you have all necessary tools . Note This guide is written for compliantkubernetes-apps v0.13.0 Setup Choose names for your service cluster and workload clusters, as well as the DNS domain to expose the services inside the service cluster: SERVICE_CLUSTER = \"sc-test\" WORKLOAD_CLUSTERS =( \"wc-test0\" ) BASE_DOMAIN = \"example.com\" Infrastructure Setup using AzureRM We suggest to set up Kubernetes clusters using kubespray. If you haven't done so already, clone the Elastisys Compliant Kubernetes Kubespray repo as follows: git clone --recursive https://github.com/elastisys/compliantkubernetes-kubespray cd compliantkubernetes-kubespray Install azure-cli If you haven't done so already, please install and configure azure-cli . Login with azure-cli az login Customize your infrastructure Create a configuration for the service and the workload clusters: pushd kubespray/contrib/azurerm/ for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do az group create -g $CLUSTER -l northeurope mkdir -p $CLUSTER /inventory done popd Note Please specify the value for the ssh_public_keys variable in kubespray/contrib/azurerm/group_vars/all . It must be your SSH public key to access your Azure virtual machines. Besides, the value for the cluster_name variable must be globally unique due to some restrictions in Azure. Make sure that $SERVICE_CLUSTER and $WORKLOAD_CLUSTERS are unique. Review and, if needed, adjust the files in kubespray/contrib/azurerm/group_vars/all accordingly. Generate and apply the templates pushd kubespray/contrib/azurerm/ tmp = \"\" for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do cat group_vars/all \\ | sed \\ -e \"s@^cluster_name:.*@cluster_name: \\\" $CLUSTER \\\"@\" \\ > group_vars/all1 cat group_vars/all1 > group_vars/all rm group_vars/all1 if [ -z $tmp ] then sed -i \"s/{{ playbook_dir }}/{{ playbook_dir }}\\/ $CLUSTER /g\" roles/generate-templates/tasks/main.yml ansible-playbook generate-templates.yml az deployment group create --template-file ./ $CLUSTER /.generated/network.json -g $CLUSTER az deployment group create --template-file ./ $CLUSTER /.generated/storage.json -g $CLUSTER az deployment group create --template-file ./ $CLUSTER /.generated/availability-sets.json -g $CLUSTER az deployment group create --template-file ./ $CLUSTER /.generated/bastion.json -g $CLUSTER az deployment group create --template-file ./ $CLUSTER /.generated/masters.json -g $CLUSTER az deployment group create --template-file ./ $CLUSTER /.generated/minions.json -g $CLUSTER else sed -i \"s/{{ playbook_dir }}\\/ $tmp /{{ playbook_dir }}\\/ $CLUSTER /g\" roles/generate-templates/tasks/main.yml ansible-playbook generate-templates.yml az deployment group create --template-file ./ $CLUSTER /.generated/network.json -g $CLUSTER az deployment group create --template-file ./ $CLUSTER /.generated/storage.json -g $CLUSTER az deployment group create --template-file ./ $CLUSTER /.generated/availability-sets.json -g $CLUSTER az deployment group create --template-file ./ $CLUSTER /.generated/bastion.json -g $CLUSTER az deployment group create --template-file ./ $CLUSTER /.generated/masters.json -g $CLUSTER az deployment group create --template-file ./ $CLUSTER /.generated/minions.json -g $CLUSTER fi tmp = $CLUSTER done sed -i \"s/{{ playbook_dir }}\\/ $tmp /{{ playbook_dir }}/g\" roles/generate-templates/tasks/main.yml sed -i \"s/{{ playbook_dir }}\\/ $tmp /{{ playbook_dir }}/g\" roles/generate-inventory_2/tasks/main.yml sed -i \"s/{{ playbook_dir }}\\/ $tmp /{{ playbook_dir }}/g\" roles/generate-inventory/tasks/main.yml popd Generating an inventory for kubespray pushd kubespray/contrib/azurerm/ tmp = \"\" for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do if [ -z $tmp ] then sed -i \"s/{{ playbook_dir }}/{{ playbook_dir }}\\/ $CLUSTER /g\" roles/generate-inventory_2/tasks/main.yml sed -i \"s/{{ playbook_dir }}/{{ playbook_dir }}\\/ $CLUSTER /g\" roles/generate-inventory/tasks/main.yml ./generate-inventory.sh $CLUSTER else sed -i \"s/{{ playbook_dir }}\\/ $tmp /{{ playbook_dir }}\\/ $CLUSTER /g\" roles/generate-inventory_2/tasks/main.yml sed -i \"s/{{ playbook_dir }}\\/ $tmp /{{ playbook_dir }}\\/ $CLUSTER /g\" roles/generate-inventory/tasks/main.yml ./generate-inventory.sh $CLUSTER fi tmp = $CLUSTER done sed -i \"s/{{ playbook_dir }}\\/ $tmp /{{ playbook_dir }}/g\" roles/generate-inventory_2/tasks/main.yml sed -i \"s/{{ playbook_dir }}\\/ $tmp /{{ playbook_dir }}/g\" roles/generate-inventory/tasks/main.yml popd The inventory files for for cluster will be created under */inventory/ . Besides, two loadBalancer_vars.yaml files will be created, one for each cluster. You may also want to check the Azure portal if the infrastructure was created correctly. The figure below shows for wc-test0 . Deploying vanilla Kubernetes clusters using Kubespray With the infrastructure provisioned, we can now deploy Kubernetes using kubespray. First, change to the compliantkubernetes-kubespray root directory. cd .. Init the Kubespray config in your config path export CK8S_CONFIG_PATH = ~/.ck8s/azure export CK8S_PGP_FP = <your GPG key fingerprint> # retrieve with gpg --list-secret-keys for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ./bin/ck8s-kubespray init $CLUSTER default $CK8S_PGP_FP done Copy the generated inventory files in the right location for CLUSTER in ${ SERVICE_CLUSTER } ${ WORKLOAD_CLUSTERS [@] } ; do #add calico to the inventory file cat kubespray/contrib/azurerm/ $CLUSTER /inventory/inventory.j2 \\ | sed '/\\[k8s_cluster:children\\]/i \\[calico-rr\\]' \\ > $CK8S_CONFIG_PATH / $CLUSTER -config/inventory.ini echo \"calico-rr\" >> $CK8S_CONFIG_PATH / $CLUSTER -config/inventory.ini $CK8S_CONFIG_PATH / $CLUSTER -config/inventory.ini # Add ansible_user ubuntu (note that this assumes you have set admin_username in azurerm/group_vars/all to ubuntu) echo -e 'ansible_user: ubuntu' >> $CK8S_CONFIG_PATH / $CLUSTER -config/group_vars/k8s_cluster/ck8s-k8s_cluster.yaml # Get the IP address of the loadbalancer (to be added in kubadmin certSANs list which will be used for kubectl) ip = $( grep -o '[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}' kubespray/contrib/azurerm/ $CLUSTER /loadbalancer_vars.yml ) echo 'supplementary_addresses_in_ssl_keys: [\"' $ip '\"]' >> $CK8S_CONFIG_PATH / $CLUSTER -config/group_vars/k8s_cluster/ck8s-k8s_cluster.yaml echo -e 'nameservers:\\n - 1.1.1.1' >> $CK8S_CONFIG_PATH / $CLUSTER -config/group_vars/k8s_cluster/ck8s-k8s_cluster.yaml echo 'resolvconf_mode: host_resolvconf' >> $CK8S_CONFIG_PATH / $CLUSTER -config/group_vars/k8s_cluster/ck8s-k8s_cluster.yaml done Run kubespray to deploy the Kubernetes clusters for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ./bin/ck8s-kubespray apply $CLUSTER --flush-cache done This may take up to 30 minutes per cluster. Please increase the value for timeout, e.g timeout=30 , in kubespray/ansible.cfg if you face the following issue while running step-3. TASK [bootstrap-os : Fetch /etc/os-release] **************************************************** fatal: [minion-0]: FAILED! => {\"msg\": \"Timeout (12s) waiting for privilege escalation prompt: \"} fatal: [minion-1]: FAILED! => {\"msg\": \"Timeout (12s) waiting for privilege escalation prompt: \"} fatal: [minion-2]: FAILED! => {\"msg\": \"Timeout (12s) waiting for privilege escalation prompt: \"} fatal: [master-0]: FAILED! => {\"msg\": \"Timeout (12s) waiting for privilege escalation prompt: \"} Correct the Kubernetes API IP addresses Get the public IP address of the loadbalancer: grep -o '[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}' kubespray/contrib/azurerm/ $CLUSTER /loadbalancer_vars.yml Locate the encrypted kubeconfigs kube_config_*.yaml and edit them using sops. Copy the IP shown above into kube_config_*.yaml . Do not overwrite the port. for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml done Test access to the clusters as follows for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get nodes' done Deploy Rook To deploy Rook, please go to the compliantkubernetes-kubespray repo root directory and run the following. for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops --decrypt ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml > $CLUSTER .yaml export KUBECONFIG = $CLUSTER .yaml ./rook/deploy-rook.sh shred -zu $CLUSTER .yaml done Please restart the operator pod, rook-ceph-operator* , if some pods stalls in initialization state as shown below: rook-ceph rook-ceph-crashcollector-minion-0-b75b9fc64-tv2vg 0/1 Init:0/2 0 24m rook-ceph rook-ceph-crashcollector-minion-1-5cfb88b66f-mggrh 0/1 Init:0/2 0 36m rook-ceph rook-ceph-crashcollector-minion-2-5c74ffffb6-jwk55 0/1 Init:0/2 0 14m Important Pods in pending state usually indicate resource shortage. In such cases you need to use bigger instances. Test Rook To test Rook, proceed as follows: for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml 'kubectl --kubeconfig {} apply -f https://raw.githubusercontent.com/rook/rook/release-1.5/cluster/examples/kubernetes/ceph/csi/rbd/pvc.yaml' ; done for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml 'kubectl --kubeconfig {} get pvc' ; done You should see PVCs in Bound state. If you want to clean the previously created PVCs: for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml 'kubectl --kubeconfig {} delete pvc rbd-pvc' ; done Deploying Compliant Kubernetes Apps Now that the Kubernetes clusters are up and running, we are ready to install the Compliant Kubernetes apps. Clone compliantkubernetes-apps and Install Pre-requisites If you haven't done so already, clone the compliantkubernetes-apps repo and install pre-requisites. git clone https://github.com/elastisys/compliantkubernetes-apps.git cd compliantkubernetes-apps ansible-playbook -e 'ansible_python_interpreter=/usr/bin/python3' --ask-become-pass --connection local --inventory 127 .0.0.1, get-requirements.yaml Initialize the apps configuration export CK8S_ENVIRONMENT_NAME = my-environment-name #export CK8S_FLAVOR=[dev|prod] # defaults to dev export CK8S_CONFIG_PATH = ~/.ck8s/my-cluster-path export CK8S_CLOUD_PROVIDER = # [exoscale|safespring|citycloud|aws|baremetal] export CK8S_PGP_FP = <your GPG key fingerprint> # retrieve with gpg --list-secret-keys ./bin/ck8s init Three files, sc-config.yaml and wc-config.yaml , and secrets.yaml , were generated in the ${CK8S_CONFIG_PATH} directory. ls -l $CK8S_CONFIG_PATH Configure the apps Edit the configuration files ${CK8S_CONFIG_PATH}/sc-config.yaml , ${CK8S_CONFIG_PATH}/wc-config.yaml and ${CK8S_CONFIG_PATH}/secrets.yaml and set the appropriate values for some of the configuration fields. Note that, the latter is encrypted. vim ${ CK8S_CONFIG_PATH } /sc-config.yaml vim ${ CK8S_CONFIG_PATH } /wc-config.yaml sops ${ CK8S_CONFIG_PATH } /secrets.yaml The following are the minimum change you should perform: # ${CK8S_CONFIG_PATH}/sc-config.yaml and ${CK8S_CONFIG_PATH}/wc-config.yaml global : baseDomain : \"set-me\" # set to <enovironment_name>.$DOMAIN opsDomain : \"set-me\" # set to ops.<environment_name>.$DOMAIN issuer : letsencrypt-prod objectStorage : type : \"s3\" s3 : region : \"set-me\" # Region for S3 buckets, e.g, west-1 regionEndpoint : \"set-me\" # e.g., https://s3.us-west-1.amazonaws.com storageClasses : default : rook-ceph-block nfs : enabled : false cinder : enabled : false local : enabled : false ebs : enabled : false # ${CK8S_CONFIG_PATH}/sc-config.yaml ingressNginx : controller : service : type : \"this-is-not-used\" annotations : \"this-is-not-used\" harbor : oidc : groupClaimName : \"set-me\" # set to group claim name used by OIDC provider issuers : letsencrypt : prod : email : \"set-me\" # set this to an email to receive LetsEncrypt notifications staging : email : \"set-me\" # set this to an email to receive LetsEncrypt notifications # ${CK8S_CONFIG_PATH}/secrets.yaml objectStorage : s3 : accessKey : \"set-me\" #set to your s3 accesskey secretKey : \"set-me\" #set to your s3 secretKey Install Compliant Kubernetes apps Start with the service cluster: ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ SERVICE_CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_sc.yaml ./bin/ck8s apply sc # Respond \"n\" if you get a WARN Then the workload clusters: for CLUSTER in \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_wc.yaml ./bin/ck8s apply wc # Respond \"n\" if you get a WARN done Settling Important Leave sufficient time for the system to settle, e.g., request TLS certificates from LetsEncrypt, perhaps as much as 20 minutes. You can check if the system settled as follows: for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get --all-namespaces pods' done Check the output of the command above. All Pods needs to be Running or Completed. for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get --all-namespaces issuers,clusterissuers,certificates' done Check the output of the command above. All resources need to have the Ready column True. Testing After completing the installation step you can test if the apps are properly installed and ready using the commands below. Start with the service cluster: ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ SERVICE_CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_sc.yaml ./bin/ck8s test sc # Respond \"n\" if you get a WARN Then the workload clusters: for CLUSTER in \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_wc.yaml ./bin/ck8s test wc # Respond \"n\" if you get a WARN done Done. Navigate to the endpoints, for example grafana.$BASE_DOMAIN , kibana.$BASE_DOMAIN , harbor.$BASE_DOMAIN , etc. to discover Compliant Kubernetes's features. Teardown Removing Compliant Kubernetes Apps from your cluster To remove the applications added by compliant kubernetes you can use the two scripts clean-sc.sh and clean-wc.sh , they are located here in the scripts folder . They perform the following actions: Delete the added helm charts Delete the added namespaces Delete any remaining PersistentVolumes Delete the added CustomResourceDefinitions Note: if user namespaces are managed by Compliant Kubernetes apps then they will also be deleted if you clean up the workload cluster. Remove infrastructure To teardown the cluster, please go to the compliantkubernetes-kubespray repo root directory and run the following. pushd kubespray/contrib/azurerm for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ansible-playbook generate-templates.yml az group deployment create -g \" $CLUSTER \" --template-file ./ $CLUSTER /.generated/clear-rg.json --mode Complete done popd Further Reading Elastisys Compliant Kubernetes Kubespray Compliant Kubernetes apps repo Configurations option","title":"On Azure"},{"location":"operator-manual/azure/#compliant-kubernetes-deployment-on-azure","text":"This document contains instructions on how to setup a service cluster and a workload cluster in Azure. The following are the main tasks addressed in this document: Infrastructure setup for two clusters: one service and one workload cluster Deploying Compliant Kubernetes on top of the two clusters. Creating DNS Records Deploying Rook Storage Orchestration Service Deploying Compliant Kubernetes apps Before starting, make sure you have all necessary tools . Note This guide is written for compliantkubernetes-apps v0.13.0","title":"Compliant Kubernetes Deployment on Azure"},{"location":"operator-manual/azure/#setup","text":"Choose names for your service cluster and workload clusters, as well as the DNS domain to expose the services inside the service cluster: SERVICE_CLUSTER = \"sc-test\" WORKLOAD_CLUSTERS =( \"wc-test0\" ) BASE_DOMAIN = \"example.com\"","title":"Setup"},{"location":"operator-manual/azure/#infrastructure-setup-using-azurerm","text":"We suggest to set up Kubernetes clusters using kubespray. If you haven't done so already, clone the Elastisys Compliant Kubernetes Kubespray repo as follows: git clone --recursive https://github.com/elastisys/compliantkubernetes-kubespray cd compliantkubernetes-kubespray","title":"Infrastructure Setup using AzureRM"},{"location":"operator-manual/azure/#install-azure-cli","text":"If you haven't done so already, please install and configure azure-cli .","title":"Install azure-cli"},{"location":"operator-manual/azure/#login-with-azure-cli","text":"az login","title":"Login with azure-cli"},{"location":"operator-manual/azure/#customize-your-infrastructure","text":"Create a configuration for the service and the workload clusters: pushd kubespray/contrib/azurerm/ for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do az group create -g $CLUSTER -l northeurope mkdir -p $CLUSTER /inventory done popd Note Please specify the value for the ssh_public_keys variable in kubespray/contrib/azurerm/group_vars/all . It must be your SSH public key to access your Azure virtual machines. Besides, the value for the cluster_name variable must be globally unique due to some restrictions in Azure. Make sure that $SERVICE_CLUSTER and $WORKLOAD_CLUSTERS are unique. Review and, if needed, adjust the files in kubespray/contrib/azurerm/group_vars/all accordingly.","title":"Customize your infrastructure"},{"location":"operator-manual/azure/#generate-and-apply-the-templates","text":"pushd kubespray/contrib/azurerm/ tmp = \"\" for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do cat group_vars/all \\ | sed \\ -e \"s@^cluster_name:.*@cluster_name: \\\" $CLUSTER \\\"@\" \\ > group_vars/all1 cat group_vars/all1 > group_vars/all rm group_vars/all1 if [ -z $tmp ] then sed -i \"s/{{ playbook_dir }}/{{ playbook_dir }}\\/ $CLUSTER /g\" roles/generate-templates/tasks/main.yml ansible-playbook generate-templates.yml az deployment group create --template-file ./ $CLUSTER /.generated/network.json -g $CLUSTER az deployment group create --template-file ./ $CLUSTER /.generated/storage.json -g $CLUSTER az deployment group create --template-file ./ $CLUSTER /.generated/availability-sets.json -g $CLUSTER az deployment group create --template-file ./ $CLUSTER /.generated/bastion.json -g $CLUSTER az deployment group create --template-file ./ $CLUSTER /.generated/masters.json -g $CLUSTER az deployment group create --template-file ./ $CLUSTER /.generated/minions.json -g $CLUSTER else sed -i \"s/{{ playbook_dir }}\\/ $tmp /{{ playbook_dir }}\\/ $CLUSTER /g\" roles/generate-templates/tasks/main.yml ansible-playbook generate-templates.yml az deployment group create --template-file ./ $CLUSTER /.generated/network.json -g $CLUSTER az deployment group create --template-file ./ $CLUSTER /.generated/storage.json -g $CLUSTER az deployment group create --template-file ./ $CLUSTER /.generated/availability-sets.json -g $CLUSTER az deployment group create --template-file ./ $CLUSTER /.generated/bastion.json -g $CLUSTER az deployment group create --template-file ./ $CLUSTER /.generated/masters.json -g $CLUSTER az deployment group create --template-file ./ $CLUSTER /.generated/minions.json -g $CLUSTER fi tmp = $CLUSTER done sed -i \"s/{{ playbook_dir }}\\/ $tmp /{{ playbook_dir }}/g\" roles/generate-templates/tasks/main.yml sed -i \"s/{{ playbook_dir }}\\/ $tmp /{{ playbook_dir }}/g\" roles/generate-inventory_2/tasks/main.yml sed -i \"s/{{ playbook_dir }}\\/ $tmp /{{ playbook_dir }}/g\" roles/generate-inventory/tasks/main.yml popd","title":"Generate and apply the templates"},{"location":"operator-manual/azure/#generating-an-inventory-for-kubespray","text":"pushd kubespray/contrib/azurerm/ tmp = \"\" for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do if [ -z $tmp ] then sed -i \"s/{{ playbook_dir }}/{{ playbook_dir }}\\/ $CLUSTER /g\" roles/generate-inventory_2/tasks/main.yml sed -i \"s/{{ playbook_dir }}/{{ playbook_dir }}\\/ $CLUSTER /g\" roles/generate-inventory/tasks/main.yml ./generate-inventory.sh $CLUSTER else sed -i \"s/{{ playbook_dir }}\\/ $tmp /{{ playbook_dir }}\\/ $CLUSTER /g\" roles/generate-inventory_2/tasks/main.yml sed -i \"s/{{ playbook_dir }}\\/ $tmp /{{ playbook_dir }}\\/ $CLUSTER /g\" roles/generate-inventory/tasks/main.yml ./generate-inventory.sh $CLUSTER fi tmp = $CLUSTER done sed -i \"s/{{ playbook_dir }}\\/ $tmp /{{ playbook_dir }}/g\" roles/generate-inventory_2/tasks/main.yml sed -i \"s/{{ playbook_dir }}\\/ $tmp /{{ playbook_dir }}/g\" roles/generate-inventory/tasks/main.yml popd The inventory files for for cluster will be created under */inventory/ . Besides, two loadBalancer_vars.yaml files will be created, one for each cluster. You may also want to check the Azure portal if the infrastructure was created correctly. The figure below shows for wc-test0 .","title":"Generating an inventory for kubespray"},{"location":"operator-manual/azure/#deploying-vanilla-kubernetes-clusters-using-kubespray","text":"With the infrastructure provisioned, we can now deploy Kubernetes using kubespray. First, change to the compliantkubernetes-kubespray root directory. cd ..","title":"Deploying vanilla Kubernetes clusters using Kubespray"},{"location":"operator-manual/azure/#init-the-kubespray-config-in-your-config-path","text":"export CK8S_CONFIG_PATH = ~/.ck8s/azure export CK8S_PGP_FP = <your GPG key fingerprint> # retrieve with gpg --list-secret-keys for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ./bin/ck8s-kubespray init $CLUSTER default $CK8S_PGP_FP done","title":"Init the Kubespray config in your config path"},{"location":"operator-manual/azure/#copy-the-generated-inventory-files-in-the-right-location","text":"for CLUSTER in ${ SERVICE_CLUSTER } ${ WORKLOAD_CLUSTERS [@] } ; do #add calico to the inventory file cat kubespray/contrib/azurerm/ $CLUSTER /inventory/inventory.j2 \\ | sed '/\\[k8s_cluster:children\\]/i \\[calico-rr\\]' \\ > $CK8S_CONFIG_PATH / $CLUSTER -config/inventory.ini echo \"calico-rr\" >> $CK8S_CONFIG_PATH / $CLUSTER -config/inventory.ini $CK8S_CONFIG_PATH / $CLUSTER -config/inventory.ini # Add ansible_user ubuntu (note that this assumes you have set admin_username in azurerm/group_vars/all to ubuntu) echo -e 'ansible_user: ubuntu' >> $CK8S_CONFIG_PATH / $CLUSTER -config/group_vars/k8s_cluster/ck8s-k8s_cluster.yaml # Get the IP address of the loadbalancer (to be added in kubadmin certSANs list which will be used for kubectl) ip = $( grep -o '[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}' kubespray/contrib/azurerm/ $CLUSTER /loadbalancer_vars.yml ) echo 'supplementary_addresses_in_ssl_keys: [\"' $ip '\"]' >> $CK8S_CONFIG_PATH / $CLUSTER -config/group_vars/k8s_cluster/ck8s-k8s_cluster.yaml echo -e 'nameservers:\\n - 1.1.1.1' >> $CK8S_CONFIG_PATH / $CLUSTER -config/group_vars/k8s_cluster/ck8s-k8s_cluster.yaml echo 'resolvconf_mode: host_resolvconf' >> $CK8S_CONFIG_PATH / $CLUSTER -config/group_vars/k8s_cluster/ck8s-k8s_cluster.yaml done","title":"Copy the generated inventory files in the right location"},{"location":"operator-manual/azure/#run-kubespray-to-deploy-the-kubernetes-clusters","text":"for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ./bin/ck8s-kubespray apply $CLUSTER --flush-cache done This may take up to 30 minutes per cluster. Please increase the value for timeout, e.g timeout=30 , in kubespray/ansible.cfg if you face the following issue while running step-3. TASK [bootstrap-os : Fetch /etc/os-release] **************************************************** fatal: [minion-0]: FAILED! => {\"msg\": \"Timeout (12s) waiting for privilege escalation prompt: \"} fatal: [minion-1]: FAILED! => {\"msg\": \"Timeout (12s) waiting for privilege escalation prompt: \"} fatal: [minion-2]: FAILED! => {\"msg\": \"Timeout (12s) waiting for privilege escalation prompt: \"} fatal: [master-0]: FAILED! => {\"msg\": \"Timeout (12s) waiting for privilege escalation prompt: \"}","title":"Run kubespray to deploy the Kubernetes clusters"},{"location":"operator-manual/azure/#correct-the-kubernetes-api-ip-addresses","text":"Get the public IP address of the loadbalancer: grep -o '[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}' kubespray/contrib/azurerm/ $CLUSTER /loadbalancer_vars.yml Locate the encrypted kubeconfigs kube_config_*.yaml and edit them using sops. Copy the IP shown above into kube_config_*.yaml . Do not overwrite the port. for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml done","title":"Correct the Kubernetes API IP addresses"},{"location":"operator-manual/azure/#test-access-to-the-clusters-as-follows","text":"for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get nodes' done","title":"Test access to the clusters as follows"},{"location":"operator-manual/azure/#deploy-rook","text":"To deploy Rook, please go to the compliantkubernetes-kubespray repo root directory and run the following. for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops --decrypt ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml > $CLUSTER .yaml export KUBECONFIG = $CLUSTER .yaml ./rook/deploy-rook.sh shred -zu $CLUSTER .yaml done Please restart the operator pod, rook-ceph-operator* , if some pods stalls in initialization state as shown below: rook-ceph rook-ceph-crashcollector-minion-0-b75b9fc64-tv2vg 0/1 Init:0/2 0 24m rook-ceph rook-ceph-crashcollector-minion-1-5cfb88b66f-mggrh 0/1 Init:0/2 0 36m rook-ceph rook-ceph-crashcollector-minion-2-5c74ffffb6-jwk55 0/1 Init:0/2 0 14m Important Pods in pending state usually indicate resource shortage. In such cases you need to use bigger instances.","title":"Deploy Rook"},{"location":"operator-manual/azure/#test-rook","text":"To test Rook, proceed as follows: for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml 'kubectl --kubeconfig {} apply -f https://raw.githubusercontent.com/rook/rook/release-1.5/cluster/examples/kubernetes/ceph/csi/rbd/pvc.yaml' ; done for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml 'kubectl --kubeconfig {} get pvc' ; done You should see PVCs in Bound state. If you want to clean the previously created PVCs: for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml 'kubectl --kubeconfig {} delete pvc rbd-pvc' ; done","title":"Test Rook"},{"location":"operator-manual/azure/#deploying-compliant-kubernetes-apps","text":"Now that the Kubernetes clusters are up and running, we are ready to install the Compliant Kubernetes apps.","title":"Deploying Compliant Kubernetes Apps"},{"location":"operator-manual/azure/#clone-compliantkubernetes-apps-and-install-pre-requisites","text":"If you haven't done so already, clone the compliantkubernetes-apps repo and install pre-requisites. git clone https://github.com/elastisys/compliantkubernetes-apps.git cd compliantkubernetes-apps ansible-playbook -e 'ansible_python_interpreter=/usr/bin/python3' --ask-become-pass --connection local --inventory 127 .0.0.1, get-requirements.yaml","title":"Clone compliantkubernetes-apps and Install Pre-requisites"},{"location":"operator-manual/azure/#initialize-the-apps-configuration","text":"export CK8S_ENVIRONMENT_NAME = my-environment-name #export CK8S_FLAVOR=[dev|prod] # defaults to dev export CK8S_CONFIG_PATH = ~/.ck8s/my-cluster-path export CK8S_CLOUD_PROVIDER = # [exoscale|safespring|citycloud|aws|baremetal] export CK8S_PGP_FP = <your GPG key fingerprint> # retrieve with gpg --list-secret-keys ./bin/ck8s init Three files, sc-config.yaml and wc-config.yaml , and secrets.yaml , were generated in the ${CK8S_CONFIG_PATH} directory. ls -l $CK8S_CONFIG_PATH","title":"Initialize the apps configuration"},{"location":"operator-manual/azure/#configure-the-apps","text":"Edit the configuration files ${CK8S_CONFIG_PATH}/sc-config.yaml , ${CK8S_CONFIG_PATH}/wc-config.yaml and ${CK8S_CONFIG_PATH}/secrets.yaml and set the appropriate values for some of the configuration fields. Note that, the latter is encrypted. vim ${ CK8S_CONFIG_PATH } /sc-config.yaml vim ${ CK8S_CONFIG_PATH } /wc-config.yaml sops ${ CK8S_CONFIG_PATH } /secrets.yaml The following are the minimum change you should perform: # ${CK8S_CONFIG_PATH}/sc-config.yaml and ${CK8S_CONFIG_PATH}/wc-config.yaml global : baseDomain : \"set-me\" # set to <enovironment_name>.$DOMAIN opsDomain : \"set-me\" # set to ops.<environment_name>.$DOMAIN issuer : letsencrypt-prod objectStorage : type : \"s3\" s3 : region : \"set-me\" # Region for S3 buckets, e.g, west-1 regionEndpoint : \"set-me\" # e.g., https://s3.us-west-1.amazonaws.com storageClasses : default : rook-ceph-block nfs : enabled : false cinder : enabled : false local : enabled : false ebs : enabled : false # ${CK8S_CONFIG_PATH}/sc-config.yaml ingressNginx : controller : service : type : \"this-is-not-used\" annotations : \"this-is-not-used\" harbor : oidc : groupClaimName : \"set-me\" # set to group claim name used by OIDC provider issuers : letsencrypt : prod : email : \"set-me\" # set this to an email to receive LetsEncrypt notifications staging : email : \"set-me\" # set this to an email to receive LetsEncrypt notifications # ${CK8S_CONFIG_PATH}/secrets.yaml objectStorage : s3 : accessKey : \"set-me\" #set to your s3 accesskey secretKey : \"set-me\" #set to your s3 secretKey","title":"Configure the apps"},{"location":"operator-manual/azure/#install-compliant-kubernetes-apps","text":"Start with the service cluster: ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ SERVICE_CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_sc.yaml ./bin/ck8s apply sc # Respond \"n\" if you get a WARN Then the workload clusters: for CLUSTER in \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_wc.yaml ./bin/ck8s apply wc # Respond \"n\" if you get a WARN done","title":"Install Compliant Kubernetes apps"},{"location":"operator-manual/azure/#settling","text":"Important Leave sufficient time for the system to settle, e.g., request TLS certificates from LetsEncrypt, perhaps as much as 20 minutes. You can check if the system settled as follows: for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get --all-namespaces pods' done Check the output of the command above. All Pods needs to be Running or Completed. for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get --all-namespaces issuers,clusterissuers,certificates' done Check the output of the command above. All resources need to have the Ready column True.","title":"Settling"},{"location":"operator-manual/azure/#testing","text":"After completing the installation step you can test if the apps are properly installed and ready using the commands below. Start with the service cluster: ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ SERVICE_CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_sc.yaml ./bin/ck8s test sc # Respond \"n\" if you get a WARN Then the workload clusters: for CLUSTER in \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_wc.yaml ./bin/ck8s test wc # Respond \"n\" if you get a WARN done Done. Navigate to the endpoints, for example grafana.$BASE_DOMAIN , kibana.$BASE_DOMAIN , harbor.$BASE_DOMAIN , etc. to discover Compliant Kubernetes's features.","title":"Testing"},{"location":"operator-manual/azure/#teardown","text":"","title":"Teardown"},{"location":"operator-manual/azure/#removing-compliant-kubernetes-apps-from-your-cluster","text":"To remove the applications added by compliant kubernetes you can use the two scripts clean-sc.sh and clean-wc.sh , they are located here in the scripts folder . They perform the following actions: Delete the added helm charts Delete the added namespaces Delete any remaining PersistentVolumes Delete the added CustomResourceDefinitions Note: if user namespaces are managed by Compliant Kubernetes apps then they will also be deleted if you clean up the workload cluster.","title":"Removing Compliant Kubernetes Apps from your cluster"},{"location":"operator-manual/azure/#remove-infrastructure","text":"To teardown the cluster, please go to the compliantkubernetes-kubespray repo root directory and run the following. pushd kubespray/contrib/azurerm for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ansible-playbook generate-templates.yml az group deployment create -g \" $CLUSTER \" --template-file ./ $CLUSTER /.generated/clear-rg.json --mode Complete done popd","title":"Remove infrastructure"},{"location":"operator-manual/azure/#further-reading","text":"Elastisys Compliant Kubernetes Kubespray Compliant Kubernetes apps repo Configurations option","title":"Further Reading"},{"location":"operator-manual/break-glass/","text":"Work in progress Please check back soon!","title":"Breaking the Glass"},{"location":"operator-manual/break-glass/#work-in-progress","text":"Please check back soon!","title":"Work in progress"},{"location":"operator-manual/clean-up/","text":"Removing Compliant Kubernetes Apps from your cluster To remove the applications added by Compliant Kubernetes you can use the two scripts clean-sc.sh and clean-wc.sh , they are located here in the scripts folder . They perform the following actions: Delete the added helm charts Delete the added namespaces Delete any remaining PersistentVolumes Delete the added CustomResourceDefinitions Note If user namespaces are managed by Compliant Kubernetes apps then they will also be deleted if you clean up the workload cluster.","title":"Remove Compliant Kubernetes Apps"},{"location":"operator-manual/clean-up/#removing-compliant-kubernetes-apps-from-your-cluster","text":"To remove the applications added by Compliant Kubernetes you can use the two scripts clean-sc.sh and clean-wc.sh , they are located here in the scripts folder . They perform the following actions: Delete the added helm charts Delete the added namespaces Delete any remaining PersistentVolumes Delete the added CustomResourceDefinitions Note If user namespaces are managed by Compliant Kubernetes apps then they will also be deleted if you clean up the workload cluster.","title":"Removing Compliant Kubernetes Apps from your cluster"},{"location":"operator-manual/cluster-sizing/","text":"Cluster Sizing A full Compliant Kubernetes deployment requires a cluster with at least 40 CPUs and 82 GB of memory in total. Monitoring Monitoring stack (InfluxDB) can handle 2500 metrics per second while provisioned with 4 CPUs and 16 GB of memory. Logging Logging stack (Elasticsearch) can take 100 records per second while provisioned with 12 CPUs and 24 GB of memory.","title":"Sizing"},{"location":"operator-manual/cluster-sizing/#cluster-sizing","text":"A full Compliant Kubernetes deployment requires a cluster with at least 40 CPUs and 82 GB of memory in total.","title":"Cluster Sizing"},{"location":"operator-manual/cluster-sizing/#monitoring","text":"Monitoring stack (InfluxDB) can handle 2500 metrics per second while provisioned with 4 CPUs and 16 GB of memory.","title":"Monitoring"},{"location":"operator-manual/cluster-sizing/#logging","text":"Logging stack (Elasticsearch) can take 100 records per second while provisioned with 12 CPUs and 24 GB of memory.","title":"Logging"},{"location":"operator-manual/common/","text":"Deploy Rook To deploy Rook, please go to the compliantkubernetes-kubespray repo root directory and run the following. for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops --decrypt ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml > $CLUSTER .yaml export KUBECONFIG = $CLUSTER .yaml ./rook/deploy-rook.sh shred -zu $CLUSTER .yaml done Please restart the operator pod, rook-ceph-operator* , if some pods stalls in initialization state as shown below: rook-ceph rook-ceph-crashcollector-minion-0-b75b9fc64-tv2vg 0/1 Init:0/2 0 24m rook-ceph rook-ceph-crashcollector-minion-1-5cfb88b66f-mggrh 0/1 Init:0/2 0 36m rook-ceph rook-ceph-crashcollector-minion-2-5c74ffffb6-jwk55 0/1 Init:0/2 0 14m Important Pods in pending state usually indicate resource shortage. In such cases you need to use bigger instances. Test Rook To test Rook, proceed as follows: for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml 'kubectl --kubeconfig {} apply -f https://raw.githubusercontent.com/rook/rook/release-1.5/cluster/examples/kubernetes/ceph/csi/rbd/pvc.yaml' ; done for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml 'kubectl --kubeconfig {} get pvc' ; done You should see PVCs in Bound state. If you want to clean the previously created PVCs: for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml 'kubectl --kubeconfig {} delete pvc rbd-pvc' ; done Now that the Kubernetes clusters are up and running, we are ready to install the Compliant Kubernetes apps. Clone compliantkubernetes-apps and Install Pre-requisites If you haven't done so already, clone the compliantkubernetes-apps repo and install pre-requisites. git clone https://github.com/elastisys/compliantkubernetes-apps.git cd compliantkubernetes-apps ansible-playbook -e 'ansible_python_interpreter=/usr/bin/python3' --ask-become-pass --connection local --inventory 127 .0.0.1, get-requirements.yaml Initialize the apps configuration export CK8S_ENVIRONMENT_NAME = my-environment-name #export CK8S_FLAVOR=[dev|prod] # defaults to dev export CK8S_CONFIG_PATH = ~/.ck8s/my-cluster-path export CK8S_CLOUD_PROVIDER = # [exoscale|safespring|citycloud|aws|baremetal] export CK8S_PGP_FP = <your GPG key fingerprint> # retrieve with gpg --list-secret-keys ./bin/ck8s init Three files, sc-config.yaml and wc-config.yaml , and secrets.yaml , were generated in the ${CK8S_CONFIG_PATH} directory. ls -l $CK8S_CONFIG_PATH Configure the apps Edit the configuration files ${CK8S_CONFIG_PATH}/sc-config.yaml , ${CK8S_CONFIG_PATH}/wc-config.yaml and ${CK8S_CONFIG_PATH}/secrets.yaml and set the appropriate values for some of the configuration fields. Note that, the latter is encrypted. vim ${ CK8S_CONFIG_PATH } /sc-config.yaml vim ${ CK8S_CONFIG_PATH } /wc-config.yaml sops ${ CK8S_CONFIG_PATH } /secrets.yaml Install Compliant Kubernetes apps Start with the service cluster: ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ SERVICE_CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_sc.yaml ./bin/ck8s apply sc # Respond \"n\" if you get a WARN Then the workload clusters: for CLUSTER in \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_wc.yaml ./bin/ck8s apply wc # Respond \"n\" if you get a WARN done Settling Important Leave sufficient time for the system to settle, e.g., request TLS certificates from LetsEncrypt, perhaps as much as 20 minutes. You can check if the system settled as follows: for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get --all-namespaces pods' done Check the output of the command above. All Pods needs to be Running or Completed. for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get --all-namespaces issuers,clusterissuers,certificates' done Check the output of the command above. All resources need to have the Ready column True. Testing After completing the installation step you can test if the apps are properly installed and ready using the commands below. Start with the service cluster: ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ SERVICE_CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_sc.yaml ./bin/ck8s test sc # Respond \"n\" if you get a WARN Then the workload clusters: for CLUSTER in \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_wc.yaml ./bin/ck8s test wc # Respond \"n\" if you get a WARN done Done. Navigate to the endpoints, for example grafana.$BASE_DOMAIN , kibana.$BASE_DOMAIN , harbor.$BASE_DOMAIN , etc. to discover Compliant Kubernetes's features. Removing Compliant Kubernetes Apps from your cluster To remove the applications added by compliant kubernetes you can use the two scripts clean-sc.sh and clean-wc.sh , they are located here in the scripts folder . They perform the following actions: Delete the added helm charts Delete the added namespaces Delete any remaining PersistentVolumes Delete the added CustomResourceDefinitions Note: if user namespaces are managed by Compliant Kubernetes apps then they will also be deleted if you clean up the workload cluster. Create S3 buckets You can use the following script to create required S3 buckets. The script uses s3cmd in the background and gets configuration and credentials for your S3 provider from ${HOME}/.s3cfg file. # Use your default s3cmd config file: ${HOME}/.s3cfg scripts/S3/entry.sh create Important You should not use your own credentials for S3. Rather create a new set of credentials with write-only access, when supported by the object storage provider ( check a feature matrix ). Test S3 To ensure that you have configured S3 correctly, run the following snippet: ( access_key = $( sops exec-file ${ CK8S_CONFIG_PATH } /secrets.yaml 'yq r {} \"objectStorage.s3.accessKey\"' ) secret_key = $( sops exec-file ${ CK8S_CONFIG_PATH } /secrets.yaml 'yq r {} \"objectStorage.s3.secretKey\"' ) region = $( yq r ${ CK8S_CONFIG_PATH } /sc-config.yaml 'objectStorage.s3.region' ) host = $( yq r ${ CK8S_CONFIG_PATH } /sc-config.yaml 'objectStorage.s3.regionEndpoint' ) for bucket in $( yq r ${ CK8S_CONFIG_PATH } /sc-config.yaml 'objectStorage.buckets.*' ) ; do s3cmd --access_key = ${ access_key } --secret_key = ${ secret_key } \\ --region = ${ region } --host = ${ host } \\ ls s3:// ${ bucket } > /dev/null [ ${ ? } = 0 ] && echo \"Bucket ${ bucket } exists!\" done )","title":"Common"},{"location":"operator-manual/common/#deploy-rook","text":"To deploy Rook, please go to the compliantkubernetes-kubespray repo root directory and run the following. for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops --decrypt ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml > $CLUSTER .yaml export KUBECONFIG = $CLUSTER .yaml ./rook/deploy-rook.sh shred -zu $CLUSTER .yaml done Please restart the operator pod, rook-ceph-operator* , if some pods stalls in initialization state as shown below: rook-ceph rook-ceph-crashcollector-minion-0-b75b9fc64-tv2vg 0/1 Init:0/2 0 24m rook-ceph rook-ceph-crashcollector-minion-1-5cfb88b66f-mggrh 0/1 Init:0/2 0 36m rook-ceph rook-ceph-crashcollector-minion-2-5c74ffffb6-jwk55 0/1 Init:0/2 0 14m Important Pods in pending state usually indicate resource shortage. In such cases you need to use bigger instances.","title":"Deploy Rook"},{"location":"operator-manual/common/#test-rook","text":"To test Rook, proceed as follows: for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml 'kubectl --kubeconfig {} apply -f https://raw.githubusercontent.com/rook/rook/release-1.5/cluster/examples/kubernetes/ceph/csi/rbd/pvc.yaml' ; done for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml 'kubectl --kubeconfig {} get pvc' ; done You should see PVCs in Bound state. If you want to clean the previously created PVCs: for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml 'kubectl --kubeconfig {} delete pvc rbd-pvc' ; done Now that the Kubernetes clusters are up and running, we are ready to install the Compliant Kubernetes apps.","title":"Test Rook"},{"location":"operator-manual/common/#clone-compliantkubernetes-apps-and-install-pre-requisites","text":"If you haven't done so already, clone the compliantkubernetes-apps repo and install pre-requisites. git clone https://github.com/elastisys/compliantkubernetes-apps.git cd compliantkubernetes-apps ansible-playbook -e 'ansible_python_interpreter=/usr/bin/python3' --ask-become-pass --connection local --inventory 127 .0.0.1, get-requirements.yaml","title":"Clone compliantkubernetes-apps and Install Pre-requisites"},{"location":"operator-manual/common/#initialize-the-apps-configuration","text":"export CK8S_ENVIRONMENT_NAME = my-environment-name #export CK8S_FLAVOR=[dev|prod] # defaults to dev export CK8S_CONFIG_PATH = ~/.ck8s/my-cluster-path export CK8S_CLOUD_PROVIDER = # [exoscale|safespring|citycloud|aws|baremetal] export CK8S_PGP_FP = <your GPG key fingerprint> # retrieve with gpg --list-secret-keys ./bin/ck8s init Three files, sc-config.yaml and wc-config.yaml , and secrets.yaml , were generated in the ${CK8S_CONFIG_PATH} directory. ls -l $CK8S_CONFIG_PATH","title":"Initialize the apps configuration"},{"location":"operator-manual/common/#configure-the-apps","text":"Edit the configuration files ${CK8S_CONFIG_PATH}/sc-config.yaml , ${CK8S_CONFIG_PATH}/wc-config.yaml and ${CK8S_CONFIG_PATH}/secrets.yaml and set the appropriate values for some of the configuration fields. Note that, the latter is encrypted. vim ${ CK8S_CONFIG_PATH } /sc-config.yaml vim ${ CK8S_CONFIG_PATH } /wc-config.yaml sops ${ CK8S_CONFIG_PATH } /secrets.yaml","title":"Configure the apps"},{"location":"operator-manual/common/#install-compliant-kubernetes-apps","text":"Start with the service cluster: ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ SERVICE_CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_sc.yaml ./bin/ck8s apply sc # Respond \"n\" if you get a WARN Then the workload clusters: for CLUSTER in \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_wc.yaml ./bin/ck8s apply wc # Respond \"n\" if you get a WARN done","title":"Install Compliant Kubernetes apps"},{"location":"operator-manual/common/#settling","text":"Important Leave sufficient time for the system to settle, e.g., request TLS certificates from LetsEncrypt, perhaps as much as 20 minutes. You can check if the system settled as follows: for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get --all-namespaces pods' done Check the output of the command above. All Pods needs to be Running or Completed. for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get --all-namespaces issuers,clusterissuers,certificates' done Check the output of the command above. All resources need to have the Ready column True.","title":"Settling"},{"location":"operator-manual/common/#testing","text":"After completing the installation step you can test if the apps are properly installed and ready using the commands below. Start with the service cluster: ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ SERVICE_CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_sc.yaml ./bin/ck8s test sc # Respond \"n\" if you get a WARN Then the workload clusters: for CLUSTER in \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_wc.yaml ./bin/ck8s test wc # Respond \"n\" if you get a WARN done Done. Navigate to the endpoints, for example grafana.$BASE_DOMAIN , kibana.$BASE_DOMAIN , harbor.$BASE_DOMAIN , etc. to discover Compliant Kubernetes's features.","title":"Testing"},{"location":"operator-manual/common/#removing-compliant-kubernetes-apps-from-your-cluster","text":"To remove the applications added by compliant kubernetes you can use the two scripts clean-sc.sh and clean-wc.sh , they are located here in the scripts folder . They perform the following actions: Delete the added helm charts Delete the added namespaces Delete any remaining PersistentVolumes Delete the added CustomResourceDefinitions Note: if user namespaces are managed by Compliant Kubernetes apps then they will also be deleted if you clean up the workload cluster.","title":"Removing Compliant Kubernetes Apps from your cluster"},{"location":"operator-manual/common/#create-s3-buckets","text":"You can use the following script to create required S3 buckets. The script uses s3cmd in the background and gets configuration and credentials for your S3 provider from ${HOME}/.s3cfg file. # Use your default s3cmd config file: ${HOME}/.s3cfg scripts/S3/entry.sh create Important You should not use your own credentials for S3. Rather create a new set of credentials with write-only access, when supported by the object storage provider ( check a feature matrix ).","title":"Create S3 buckets"},{"location":"operator-manual/common/#test-s3","text":"To ensure that you have configured S3 correctly, run the following snippet: ( access_key = $( sops exec-file ${ CK8S_CONFIG_PATH } /secrets.yaml 'yq r {} \"objectStorage.s3.accessKey\"' ) secret_key = $( sops exec-file ${ CK8S_CONFIG_PATH } /secrets.yaml 'yq r {} \"objectStorage.s3.secretKey\"' ) region = $( yq r ${ CK8S_CONFIG_PATH } /sc-config.yaml 'objectStorage.s3.region' ) host = $( yq r ${ CK8S_CONFIG_PATH } /sc-config.yaml 'objectStorage.s3.regionEndpoint' ) for bucket in $( yq r ${ CK8S_CONFIG_PATH } /sc-config.yaml 'objectStorage.buckets.*' ) ; do s3cmd --access_key = ${ access_key } --secret_key = ${ secret_key } \\ --region = ${ region } --host = ${ host } \\ ls s3:// ${ bucket } > /dev/null [ ${ ? } = 0 ] && echo \"Bucket ${ bucket } exists!\" done )","title":"Test S3"},{"location":"operator-manual/credentials/","text":"Use of Credentials Compliant Kubernetes interacts with a lot of credentials. This document captures all of them in an orderly fashion, layer-by-layer. Terminology Purpose: Why are these credentials necessary, what can be done with them. Owner: The person (e.g., John Smith) or computing system (e.g., control plane Node, Pod) who controls the credentials, and is responsible for their safe storage and usage. Type: Individual credentials identify a person, while service accounts identify a computing system. Use for: What should these credentials be used for. Do not use for: When should these credentials NOT be used, although they technically could. Single Sign-On (SSO) Credentials Example: Company Google Accounts Purpose: authenticate a person with various system, in particular Kubernetes API via Dex Grafana via Dex Kibana via Dex Harbor via Dex Owner: individual person (user or administrator) Type: individual credentials Use for: identifying yourself Do not use for: These credentials are super valuable and should not be shared with anyone, not even family, friends, workmates, etc., even if requested. Report such sharing requests. Misc: Protect using 2FA Cloud Provider (Infrastructure) Credentials Purpose: create infrastructure, e.g., VMs, load balancers, networks, buckets. Owner: administrator Type: individual credentials Use for: Terraform layer in Kubespray Creating and destroying buckets via helper scripts Do not use for: Kubernetes cloud-controller integration , use Cloud Controller Credentials instead. Access to object storage / S3 bucket, use backup credentials instead. SSH Keys Purpose: access Nodes for setup, break glass or disaster recovery Owner: administrator Type: individual credentials Use for: Accessing Nodes via SSH Do not use for: Giving a system access to a Git repository. Create a separate SSH key only for that purpose instead. PGP Keys Purpose: encrypt/decrypt sensitive information, e.g., service account credentials, customer names, incident reports, financial information, etc. Owner: administrator Type: individual credentials Use for: Encrypting/decrypting sensitive information Do not use for: Encrypting/decrypting individual credentials. These are meant to be individual and never shared. Encrypting/decrypting SSH key. These are meant to be individual and never shared. Prefer protecting your SSH key with a passphrase or storing it on a YubiKey . Encrypting non-sensitive information. This leads to a culture of \"security by obscurity\" in which people over-rely on encryption. Prefer being mindful about what data you store and why. If unsure, prefer not storing credentials, as Cloud Provider Credentials and SSH keys should be enough to restore any access. Cloud Controller (Integration) Credentials Purpose: allow Kubernetes control Nodes, specifically the cloud-controller-manager , to create LoadBalancers and PersistentVolumes Owner: each Kubernetes cluster should have their own Type: service account Use for: Configuring Kubespray to set up a Kubernetes cluster with cloud integration Do not use for: AWS. Use AWS IAM Node Roles instead. Exoscale. We currently don't integrate with Exoscale for LoadBalancer or PersistentVolumes. Terraform layer in Kubespray Backup and Long-Term Logging Credentials Purpose: Allow backup of various components, e.g., PVCs via Velero, InfluxDB metrics, Elasticsearch Indexes, PostgreSQL databases. Allow long-term logging, e.g., Service Cluster logs Owner: each Compliant Kubernetes cluster should have their own Type: service account Use for: Backup Logging Do not use for: Other object storage, e.g., Harbor container images Disaster recovery, investigations. Use Cloud Provider credentials instead. Misc: Ensure these credentials are write-only , if supported by the underlying cloud provider, to comply with ISO 27001 A.12.3.1 Information Backup and ISO 27001 A.12.4.2 Protection of Log Information . As of 2021-05-20, this is supported by AWS S3, Exoscale S3, GCP and SafeSpring S3. OpsGenie Credentials Purpose: Allow the Cluster to issue alerts to OpsGenie. Owner: each Compliant Kubernetes cluster should have their own Type: service account Use for: alerting Do not use for: Operator access to OpsGenie. Prefer Single Sign-On (SSO) . Dex OpenID Client Secret Purpose: Complete the \"OAuth dance\" between Grafana, OpenSearch Dashboard, Harbor and kubectl, on one side, and Dex, on the other side. Used both by administrators and users. Owner: each Compliant Kubernetes cluster should have their own Type: not secret Misc: We have determined that the OpenID client secret should not be treated as a secret. See risk analysis here and here . Kubeconfig with OpenID Authentication Purpose: access the Kubernetes API in normal situations Owner: shared between administrators and users Type: not secret Use for: Routine checks Routine maintenance Investigations \"Simple\" recovery Misc: If these credentials become unusable, you are in a \"break glass\" situation. Use cloud provider credentials or SSH keys to initiate disaster recovery. Kubeconfig with Client Certificate Key Purpose: access the Kubernetes API for disaster recovery, break glass or initial setup Owner: shared between administrators Type: special Use for: Initial setup Break glass Disaster recovery Do not use for: Routine maintenance or investigation. Use Kubeconfig with OpenID Authentication Misc: Such a Kubeconfig is available on all control plane Nodes at /etc/kubernetes/admin.conf . SSH into a control plane Node then type sudo su and you can readily use kubectl commands. Unless absolutely necessary, avoid storing this file outside the control plane Nodes. If, for some good reason, you downloaded this file, shred it after usage.","title":"Use of Credentials"},{"location":"operator-manual/credentials/#use-of-credentials","text":"Compliant Kubernetes interacts with a lot of credentials. This document captures all of them in an orderly fashion, layer-by-layer.","title":"Use of Credentials"},{"location":"operator-manual/credentials/#terminology","text":"Purpose: Why are these credentials necessary, what can be done with them. Owner: The person (e.g., John Smith) or computing system (e.g., control plane Node, Pod) who controls the credentials, and is responsible for their safe storage and usage. Type: Individual credentials identify a person, while service accounts identify a computing system. Use for: What should these credentials be used for. Do not use for: When should these credentials NOT be used, although they technically could.","title":"Terminology"},{"location":"operator-manual/credentials/#single-sign-on-sso-credentials","text":"Example: Company Google Accounts Purpose: authenticate a person with various system, in particular Kubernetes API via Dex Grafana via Dex Kibana via Dex Harbor via Dex Owner: individual person (user or administrator) Type: individual credentials Use for: identifying yourself Do not use for: These credentials are super valuable and should not be shared with anyone, not even family, friends, workmates, etc., even if requested. Report such sharing requests. Misc: Protect using 2FA","title":"Single Sign-On (SSO) Credentials"},{"location":"operator-manual/credentials/#cloud-provider-infrastructure-credentials","text":"Purpose: create infrastructure, e.g., VMs, load balancers, networks, buckets. Owner: administrator Type: individual credentials Use for: Terraform layer in Kubespray Creating and destroying buckets via helper scripts Do not use for: Kubernetes cloud-controller integration , use Cloud Controller Credentials instead. Access to object storage / S3 bucket, use backup credentials instead.","title":"Cloud Provider (Infrastructure) Credentials"},{"location":"operator-manual/credentials/#ssh-keys","text":"Purpose: access Nodes for setup, break glass or disaster recovery Owner: administrator Type: individual credentials Use for: Accessing Nodes via SSH Do not use for: Giving a system access to a Git repository. Create a separate SSH key only for that purpose instead.","title":"SSH Keys"},{"location":"operator-manual/credentials/#pgp-keys","text":"Purpose: encrypt/decrypt sensitive information, e.g., service account credentials, customer names, incident reports, financial information, etc. Owner: administrator Type: individual credentials Use for: Encrypting/decrypting sensitive information Do not use for: Encrypting/decrypting individual credentials. These are meant to be individual and never shared. Encrypting/decrypting SSH key. These are meant to be individual and never shared. Prefer protecting your SSH key with a passphrase or storing it on a YubiKey . Encrypting non-sensitive information. This leads to a culture of \"security by obscurity\" in which people over-rely on encryption. Prefer being mindful about what data you store and why. If unsure, prefer not storing credentials, as Cloud Provider Credentials and SSH keys should be enough to restore any access.","title":"PGP Keys"},{"location":"operator-manual/credentials/#cloud-controller-integration-credentials","text":"Purpose: allow Kubernetes control Nodes, specifically the cloud-controller-manager , to create LoadBalancers and PersistentVolumes Owner: each Kubernetes cluster should have their own Type: service account Use for: Configuring Kubespray to set up a Kubernetes cluster with cloud integration Do not use for: AWS. Use AWS IAM Node Roles instead. Exoscale. We currently don't integrate with Exoscale for LoadBalancer or PersistentVolumes. Terraform layer in Kubespray","title":"Cloud Controller (Integration) Credentials"},{"location":"operator-manual/credentials/#backup-and-long-term-logging-credentials","text":"Purpose: Allow backup of various components, e.g., PVCs via Velero, InfluxDB metrics, Elasticsearch Indexes, PostgreSQL databases. Allow long-term logging, e.g., Service Cluster logs Owner: each Compliant Kubernetes cluster should have their own Type: service account Use for: Backup Logging Do not use for: Other object storage, e.g., Harbor container images Disaster recovery, investigations. Use Cloud Provider credentials instead. Misc: Ensure these credentials are write-only , if supported by the underlying cloud provider, to comply with ISO 27001 A.12.3.1 Information Backup and ISO 27001 A.12.4.2 Protection of Log Information . As of 2021-05-20, this is supported by AWS S3, Exoscale S3, GCP and SafeSpring S3.","title":"Backup and Long-Term Logging Credentials"},{"location":"operator-manual/credentials/#opsgenie-credentials","text":"Purpose: Allow the Cluster to issue alerts to OpsGenie. Owner: each Compliant Kubernetes cluster should have their own Type: service account Use for: alerting Do not use for: Operator access to OpsGenie. Prefer Single Sign-On (SSO) .","title":"OpsGenie Credentials"},{"location":"operator-manual/credentials/#dex-openid-client-secret","text":"Purpose: Complete the \"OAuth dance\" between Grafana, OpenSearch Dashboard, Harbor and kubectl, on one side, and Dex, on the other side. Used both by administrators and users. Owner: each Compliant Kubernetes cluster should have their own Type: not secret Misc: We have determined that the OpenID client secret should not be treated as a secret. See risk analysis here and here .","title":"Dex OpenID Client Secret"},{"location":"operator-manual/credentials/#kubeconfig-with-openid-authentication","text":"Purpose: access the Kubernetes API in normal situations Owner: shared between administrators and users Type: not secret Use for: Routine checks Routine maintenance Investigations \"Simple\" recovery Misc: If these credentials become unusable, you are in a \"break glass\" situation. Use cloud provider credentials or SSH keys to initiate disaster recovery.","title":"Kubeconfig with OpenID Authentication"},{"location":"operator-manual/credentials/#kubeconfig-with-client-certificate-key","text":"Purpose: access the Kubernetes API for disaster recovery, break glass or initial setup Owner: shared between administrators Type: special Use for: Initial setup Break glass Disaster recovery Do not use for: Routine maintenance or investigation. Use Kubeconfig with OpenID Authentication Misc: Such a Kubeconfig is available on all control plane Nodes at /etc/kubernetes/admin.conf . SSH into a control plane Node then type sudo su and you can readily use kubectl commands. Unless absolutely necessary, avoid storing this file outside the control plane Nodes. If, for some good reason, you downloaded this file, shred it after usage.","title":"Kubeconfig with Client Certificate Key"},{"location":"operator-manual/disaster-recovery/","text":"Disaster Recovery This document details disaster recovery procedures for Compliant Kubernetes. These procedures must be executed by the administrator. Compliant Need Disaster recovery is mandated by several regulations and information security standards. For example, in ISO 27001:2013, the annexes that mostly concerns disaster recovery are: A.12.3.1 Information Backup A.17.1.1 Planning Information Security Continuity Object storage providers Feature matrix Provider Write-only credentials AWS S3 Yes Citycloud S3 No Exoscale S3 Yes GCP Yes Safespring S3 Yes Elasticsearch Backup Elasticsearch is set up to store backups in an S3 bucket. There is a CronJob called elasticsearch-backup in the cluster that is invoking the snapshot process in Elasticsearch. To take a snapshot on-demand, execute ./bin/ck8s ops kubectl sc -n elastic-system create job --from=cronjob/elasticsearch-backup <name-of-job> Restore Set the following variables user - Elasticsearch user with permissions to manage snapshots, usually snapshotter password - password for the above user es_url - url to Elasticsearch List snapshot repositories # Simple \u276f curl -kL -u \" ${ user } : ${ password } \" \" ${ es_url } /_cat/repositories?v\" id type s3_exoscale_7.x s3 # Detailed \u276f curl -kL -u \" ${ user } : ${ password } \" \" ${ es_url } /_snapshot/?pretty\" { \"s3_exoscale_7.x\" : { \"type\" : \"s3\" , \"settings\" : { \"bucket\" : \"es-backup\" , \"client\" : \"default\" } } } List available snapshots snapshot_repo = <name/id from previous step> # Simple \u276f curl -kL -u \" ${ user } : ${ password } \" \" ${ es_url } /_cat/snapshots/ ${ snapshot_repo } ?v&s=id\" id status start_epoch start_time end_epoch end_time duration indices successful_shards failed_shards total_shards snapshot-20200929_093941z SUCCESS 1601372382 09 :39:42 1601372390 09 :39:50 8 .4s 6 6 0 6 snapshot-20200930_000008z SUCCESS 1601424008 00 :00:08 1601424035 00 :00:35 27 .4s 20 20 0 20 snapshot-20201001_000006z SUCCESS 1601510407 00 :00:07 1601510530 00 :02:10 2m 75 75 0 75 # Detailed list of all snapshots curl -kL -u \" ${ user } : ${ password } \" \" ${ es_url } /_snapshot/ ${ snapshot_repo } /_all?pretty\" # Detailed list of specific snapshot \u276f curl -kL -u \" ${ user } : ${ password } \" \" ${ es_url } /_snapshot/ ${ snapshot_repo } /snapshot-20201001_000006z?pretty\" { \"snapshots\" : [ { \"snapshot\" : \"snapshot-20201001_000006z\" , \"uuid\" : \"Fq0EusFYRV2nI9G9F1DX1A\" , \"version_id\" : 7080099 , \"version\" : \"7.8.0\" , \"indices\" : [ \"kubernetes-default-2020.09.30-000032\" , \"other-default-2020.09.30-000005\" , ..<redacted>.. \"kubeaudit-default-2020.09.30-000009\" ] , \"include_global_state\" : false, \"state\" : \"SUCCESS\" , \"start_time\" : \"2020-10-01T00:00:07.344Z\" , \"start_time_in_millis\" : 1601510407344 , \"end_time\" : \"2020-10-01T00:02:10.828Z\" , \"end_time_in_millis\" : 1601510530828 , \"duration_in_millis\" : 123484 , \"failures\" : [ ] , \"shards\" : { \"total\" : 75 , \"failed\" : 0 , \"successful\" : 75 } } ] } You usually select the latest snapshot containing the indices you want to restore. Restore one or multiple indices from a snapshot Note You cannot restore a write index (the latest index) if you already have a write index connected to the same index alias (which will happen if you have started to receive logs). snapshot_name = <Snapshot name from previous step> indices = \"<list of comma separated indices/index patterns>\" curl -kL -u \" ${ user } : ${ password } \" -X POST \" ${ es_url } /_snapshot/ ${ snapshot_repo } / ${ snapshot_name } /_restore?pretty\" -H 'Content-Type: application/json' -d ' { \"indices\": \"' ${ indices } '\" } ' Read the API to see all parameters and their explanations. Restoring Kibana data Data in Kibana (saved searches, visualizations, dashboards, etc) is stored in the index .kibana_1 . To restore that data you first need to delete the index and then do a restore. This will overwrite anything in the current .kibana_1 index. If there is something new that should be saved, then export the saved objects and import them after the restore. snapshot_name = <Snapshot name from previous step> curl -kL -u \" ${ user } : ${ password } \" -X DELETE \" ${ es_url } /.kibana_1?pretty\" curl -kL -u \" ${ user } : ${ password } \" -X POST \" ${ es_url } /_snapshot/ ${ snapshot_repo } / ${ snapshot_name } /_restore?pretty\" -H 'Content-Type: application/json' -d ' { \"indices\": \"' .kibana_1 '\" } ' Start new cluster from snapshot This process is very similar to the one described above, but there are a few extra steps to carry out. Before you install Elasticsearch you can preferably disable the initial index creation by setting configurer.createIndices: false in the values file for opendistro. This will make the restore process leaner. Install the Elasticsearch suite: ./bin/ck8s ops helmfile sc -l app = opendistro apply Wait for the the installation to complete. After the installation, go back up to the Restore section to proceed with the restore. If you want to restore all indices, use the following indices variable indices = \"kubernetes-*,kubeaudit-*,other-*\" Note This process assumes that you are using the same S3 bucket as your previous cluster. If you aren't: Register a new S3 snapshot repository to the old bucket as described here Use the newly registered snapshot repository in the restore process Harbor Backup Harbor is set up to store backups of the database in an S3 bucket (note that this does not include the actual images, since those are already stored in S3 by default). There is a CronJob called harbor-backup-cronjob in the cluster that is taking a database dump and uploading it to a S3 bucket. To take a backup on-demand, execute ./bin/ck8s ops kubectl sc -n harbor create job --from = cronjob/harbor-backup-cronjob <name-of-job> Restore Instructions for how to restore Harbor can be found in compliantkubernetes-apps : https://github.com/elastisys/compliantkubernetes-apps/tree/main/scripts/restore#restore-harbor InfluxDB Backup InfluxDB is set up to store backups of the data in an S3 bucket. There is a CronJob called influxdb-backup in the cluster that is invoking influxDB's backup function and uploading the backup to a S3 bucket. To take a backup on-demand, execute ./bin/ck8s ops kubectl sc -n influxdb-prometheus create job --from = cronjob/influxdb-backup <name-of-job> Restore When restoring the data, InfluxDB must not already contain the databases that are going to be restored. This will make sure that the databases are not created. If you are planning on restoring InfluxDB on a new installation, then before installing InfluxDB set influxDB.createdb: false in sc-config.yaml . If you are restoring to an existing InfluxDB instance, then first drop the databases: # Enter the InfluxDB container ./bin/ck8s ops kubectl sc exec -n influxdb-prometheus influxdb-0 -it -- bash # Start the influx CLI influx -username ${ INFLUXDB_ADMIN_USER } -password ${ INFLUXDB_ADMIN_PASSWORD } -precision rfc3339 # Drop the databases DROP DATABASE service_cluster DROP DATABASE workload_cluster # Exit the influx CLI exit # Exit the InfluxDB container exit You will then create a kubernetes job that will restore InfluxDB. Run the following from root of comliantkubernetes-apps .: export INFLUX_BACKUP_NAME = \"<name of your backup>\" export INFLUX_ADDR = \"influxdb.influxdb-prometheus.svc:8088\" export S3_INFLUX_BUCKET_NAME = $( yq read \" ${ CK8S_CONFIG_PATH } /sc-config.yaml\" objectStorage.buckets.influxDB ) export S3_REGION_ENDPOINT = $( yq read \" ${ CK8S_CONFIG_PATH } /sc-config.yaml\" objectStorage.s3.regionEndpoint ) export S3_REGION = $( yq read \" ${ CK8S_CONFIG_PATH } /sc-config.yaml\" objectStorage.s3.region ) export S3_ACCESS_KEY = $( sops -d \" ${ CK8S_CONFIG_PATH } /secrets.yaml\" | yq read - objectStorage.s3.accessKey ) export S3_SECRET_KEY = $( sops -d \" ${ CK8S_CONFIG_PATH } /secrets.yaml\" | yq read - objectStorage.s3.secretKey ) envsubst < manifests/restore/restore-influx.yaml | ./bin/ck8s ops kubectl sc apply -n influxdb-prometheus -f - Then make sure that out influxDB users have the correct permissions (this assumes you are using default usernames for the prometheus writers): # Enter the InfluxDB container ./bin/ck8s ops kubectl sc exec -n influxdb-prometheus influxdb-0 -it -- bash # Start the influx CLI influx -username ${ INFLUXDB_ADMIN_USER } -password ${ INFLUXDB_ADMIN_PASSWORD } -precision rfc3339 # Update permissions GRANT WRITE ON service_cluster TO scWriter GRANT WRITE ON workload_cluster TO wcWriter # Exit the influx CLI exit # Exit the InfluxDB container exit Velero These instructions make use of the Velero CLI, you can download it here: https://github.com/vmware-tanzu/velero/releases/tag/v1.5.3 (version 1.5.3). The CLI needs the env variable KUBECONFIG set to the path of a decrypted kubeconfig. Read more about Velero here: https://compliantkubernetes.io/user-guide/backup/ Note This documentation uses the Velero CLI, as opposed to Velero CRDs, since that is what is encouraged by upstream documentation. Backup Velero is set up to take daily backups and store them in an S3 bucket. The daily backup will not take backups of everything in a kubernetes cluster, it will instead look for certain labels and annotations. Read more about those labels and annotations here: https://compliantkubernetes.io/user-guide/backup/#backing-up It is also possible to take on-demand backups. Then you can freely chose what to backup and do not have to base it on the same labels. A basic example with the Velero CLI would be velero backup create manual-backup , which would take a backup of all kubernetes resources (though not the data in the volumes by default). Check which arguments you can use by running velero backup create --help . Restore Restoring from a backup with Velero is meant to be a type of disaster recovery. Velero will not overwrite existing Resources when restoring. As such, if you want to restore the state of a Resource that is still running, the Resource must be deleted first. To restore the state from the latest daily backup, run: velero restore create --from-schedule velero-daily-backup --wait This command will wait until the restore has finished. You can also do partial restorations, e.g. just restoring one namespace, by using different arguments. You can also restore from manual backups by using the flag --from-backup <backup-name> Persistent Volumes are only restored if a Pod with the backup annotation is restored. Multiple Pods can have an annotation for the same Persistent Volume. When restoring the Persistent Volume it will overwrite any existing files with the same names as the files to be restored. Any other files will be left as they were before the restoration started. So a restore will not wipe the volume clean and then restore. If a clean wipe is the desired behavior, then the volume must be wiped manually before restoring. Grafana This refers to the user Grafana, not the ops Grafana. Backup Grafana is set up to be included in the daily Velero backup. We then include the Grafana deployment, pod, and PVC (including the data). Manual backups can be taken using velero (include the same resources). Restore To restore the Grafana backup you must: Have Grafana installed Delete the grafana deployment, PVC and PV kubectl delete deploy -n monitoring user-grafana kubectl delete pvc -n monitoring user-grafana Restore the velero backup velero restore create --from-schedule velero-daily-backup --wait You can also restore Grafana by setting restore.velero in your {CK8S_CONFIG_PATH}/sc-config.yaml to true , and then reapply the service cluster apps: .bin/ck8s apply sc This will go through the same steps as above. By default, the latest daily backup is chosen; to restore from a different backup, set restore.veleroBackupName to the desired backup name.","title":"Disaster Recovery"},{"location":"operator-manual/disaster-recovery/#disaster-recovery","text":"This document details disaster recovery procedures for Compliant Kubernetes. These procedures must be executed by the administrator.","title":"Disaster Recovery"},{"location":"operator-manual/disaster-recovery/#compliant-need","text":"Disaster recovery is mandated by several regulations and information security standards. For example, in ISO 27001:2013, the annexes that mostly concerns disaster recovery are: A.12.3.1 Information Backup A.17.1.1 Planning Information Security Continuity","title":"Compliant Need"},{"location":"operator-manual/disaster-recovery/#object-storage-providers","text":"","title":"Object storage providers"},{"location":"operator-manual/disaster-recovery/#feature-matrix","text":"Provider Write-only credentials AWS S3 Yes Citycloud S3 No Exoscale S3 Yes GCP Yes Safespring S3 Yes","title":"Feature matrix"},{"location":"operator-manual/disaster-recovery/#elasticsearch","text":"","title":"Elasticsearch"},{"location":"operator-manual/disaster-recovery/#backup","text":"Elasticsearch is set up to store backups in an S3 bucket. There is a CronJob called elasticsearch-backup in the cluster that is invoking the snapshot process in Elasticsearch. To take a snapshot on-demand, execute ./bin/ck8s ops kubectl sc -n elastic-system create job --from=cronjob/elasticsearch-backup <name-of-job>","title":"Backup"},{"location":"operator-manual/disaster-recovery/#restore","text":"Set the following variables user - Elasticsearch user with permissions to manage snapshots, usually snapshotter password - password for the above user es_url - url to Elasticsearch List snapshot repositories # Simple \u276f curl -kL -u \" ${ user } : ${ password } \" \" ${ es_url } /_cat/repositories?v\" id type s3_exoscale_7.x s3 # Detailed \u276f curl -kL -u \" ${ user } : ${ password } \" \" ${ es_url } /_snapshot/?pretty\" { \"s3_exoscale_7.x\" : { \"type\" : \"s3\" , \"settings\" : { \"bucket\" : \"es-backup\" , \"client\" : \"default\" } } } List available snapshots snapshot_repo = <name/id from previous step> # Simple \u276f curl -kL -u \" ${ user } : ${ password } \" \" ${ es_url } /_cat/snapshots/ ${ snapshot_repo } ?v&s=id\" id status start_epoch start_time end_epoch end_time duration indices successful_shards failed_shards total_shards snapshot-20200929_093941z SUCCESS 1601372382 09 :39:42 1601372390 09 :39:50 8 .4s 6 6 0 6 snapshot-20200930_000008z SUCCESS 1601424008 00 :00:08 1601424035 00 :00:35 27 .4s 20 20 0 20 snapshot-20201001_000006z SUCCESS 1601510407 00 :00:07 1601510530 00 :02:10 2m 75 75 0 75 # Detailed list of all snapshots curl -kL -u \" ${ user } : ${ password } \" \" ${ es_url } /_snapshot/ ${ snapshot_repo } /_all?pretty\" # Detailed list of specific snapshot \u276f curl -kL -u \" ${ user } : ${ password } \" \" ${ es_url } /_snapshot/ ${ snapshot_repo } /snapshot-20201001_000006z?pretty\" { \"snapshots\" : [ { \"snapshot\" : \"snapshot-20201001_000006z\" , \"uuid\" : \"Fq0EusFYRV2nI9G9F1DX1A\" , \"version_id\" : 7080099 , \"version\" : \"7.8.0\" , \"indices\" : [ \"kubernetes-default-2020.09.30-000032\" , \"other-default-2020.09.30-000005\" , ..<redacted>.. \"kubeaudit-default-2020.09.30-000009\" ] , \"include_global_state\" : false, \"state\" : \"SUCCESS\" , \"start_time\" : \"2020-10-01T00:00:07.344Z\" , \"start_time_in_millis\" : 1601510407344 , \"end_time\" : \"2020-10-01T00:02:10.828Z\" , \"end_time_in_millis\" : 1601510530828 , \"duration_in_millis\" : 123484 , \"failures\" : [ ] , \"shards\" : { \"total\" : 75 , \"failed\" : 0 , \"successful\" : 75 } } ] } You usually select the latest snapshot containing the indices you want to restore. Restore one or multiple indices from a snapshot Note You cannot restore a write index (the latest index) if you already have a write index connected to the same index alias (which will happen if you have started to receive logs). snapshot_name = <Snapshot name from previous step> indices = \"<list of comma separated indices/index patterns>\" curl -kL -u \" ${ user } : ${ password } \" -X POST \" ${ es_url } /_snapshot/ ${ snapshot_repo } / ${ snapshot_name } /_restore?pretty\" -H 'Content-Type: application/json' -d ' { \"indices\": \"' ${ indices } '\" } ' Read the API to see all parameters and their explanations.","title":"Restore"},{"location":"operator-manual/disaster-recovery/#restoring-kibana-data","text":"Data in Kibana (saved searches, visualizations, dashboards, etc) is stored in the index .kibana_1 . To restore that data you first need to delete the index and then do a restore. This will overwrite anything in the current .kibana_1 index. If there is something new that should be saved, then export the saved objects and import them after the restore. snapshot_name = <Snapshot name from previous step> curl -kL -u \" ${ user } : ${ password } \" -X DELETE \" ${ es_url } /.kibana_1?pretty\" curl -kL -u \" ${ user } : ${ password } \" -X POST \" ${ es_url } /_snapshot/ ${ snapshot_repo } / ${ snapshot_name } /_restore?pretty\" -H 'Content-Type: application/json' -d ' { \"indices\": \"' .kibana_1 '\" } '","title":"Restoring Kibana data"},{"location":"operator-manual/disaster-recovery/#start-new-cluster-from-snapshot","text":"This process is very similar to the one described above, but there are a few extra steps to carry out. Before you install Elasticsearch you can preferably disable the initial index creation by setting configurer.createIndices: false in the values file for opendistro. This will make the restore process leaner. Install the Elasticsearch suite: ./bin/ck8s ops helmfile sc -l app = opendistro apply Wait for the the installation to complete. After the installation, go back up to the Restore section to proceed with the restore. If you want to restore all indices, use the following indices variable indices = \"kubernetes-*,kubeaudit-*,other-*\" Note This process assumes that you are using the same S3 bucket as your previous cluster. If you aren't: Register a new S3 snapshot repository to the old bucket as described here Use the newly registered snapshot repository in the restore process","title":"Start new cluster from snapshot"},{"location":"operator-manual/disaster-recovery/#harbor","text":"","title":"Harbor"},{"location":"operator-manual/disaster-recovery/#backup_1","text":"Harbor is set up to store backups of the database in an S3 bucket (note that this does not include the actual images, since those are already stored in S3 by default). There is a CronJob called harbor-backup-cronjob in the cluster that is taking a database dump and uploading it to a S3 bucket. To take a backup on-demand, execute ./bin/ck8s ops kubectl sc -n harbor create job --from = cronjob/harbor-backup-cronjob <name-of-job>","title":"Backup"},{"location":"operator-manual/disaster-recovery/#restore_1","text":"Instructions for how to restore Harbor can be found in compliantkubernetes-apps : https://github.com/elastisys/compliantkubernetes-apps/tree/main/scripts/restore#restore-harbor","title":"Restore"},{"location":"operator-manual/disaster-recovery/#influxdb","text":"","title":"InfluxDB"},{"location":"operator-manual/disaster-recovery/#backup_2","text":"InfluxDB is set up to store backups of the data in an S3 bucket. There is a CronJob called influxdb-backup in the cluster that is invoking influxDB's backup function and uploading the backup to a S3 bucket. To take a backup on-demand, execute ./bin/ck8s ops kubectl sc -n influxdb-prometheus create job --from = cronjob/influxdb-backup <name-of-job>","title":"Backup"},{"location":"operator-manual/disaster-recovery/#restore_2","text":"When restoring the data, InfluxDB must not already contain the databases that are going to be restored. This will make sure that the databases are not created. If you are planning on restoring InfluxDB on a new installation, then before installing InfluxDB set influxDB.createdb: false in sc-config.yaml . If you are restoring to an existing InfluxDB instance, then first drop the databases: # Enter the InfluxDB container ./bin/ck8s ops kubectl sc exec -n influxdb-prometheus influxdb-0 -it -- bash # Start the influx CLI influx -username ${ INFLUXDB_ADMIN_USER } -password ${ INFLUXDB_ADMIN_PASSWORD } -precision rfc3339 # Drop the databases DROP DATABASE service_cluster DROP DATABASE workload_cluster # Exit the influx CLI exit # Exit the InfluxDB container exit You will then create a kubernetes job that will restore InfluxDB. Run the following from root of comliantkubernetes-apps .: export INFLUX_BACKUP_NAME = \"<name of your backup>\" export INFLUX_ADDR = \"influxdb.influxdb-prometheus.svc:8088\" export S3_INFLUX_BUCKET_NAME = $( yq read \" ${ CK8S_CONFIG_PATH } /sc-config.yaml\" objectStorage.buckets.influxDB ) export S3_REGION_ENDPOINT = $( yq read \" ${ CK8S_CONFIG_PATH } /sc-config.yaml\" objectStorage.s3.regionEndpoint ) export S3_REGION = $( yq read \" ${ CK8S_CONFIG_PATH } /sc-config.yaml\" objectStorage.s3.region ) export S3_ACCESS_KEY = $( sops -d \" ${ CK8S_CONFIG_PATH } /secrets.yaml\" | yq read - objectStorage.s3.accessKey ) export S3_SECRET_KEY = $( sops -d \" ${ CK8S_CONFIG_PATH } /secrets.yaml\" | yq read - objectStorage.s3.secretKey ) envsubst < manifests/restore/restore-influx.yaml | ./bin/ck8s ops kubectl sc apply -n influxdb-prometheus -f - Then make sure that out influxDB users have the correct permissions (this assumes you are using default usernames for the prometheus writers): # Enter the InfluxDB container ./bin/ck8s ops kubectl sc exec -n influxdb-prometheus influxdb-0 -it -- bash # Start the influx CLI influx -username ${ INFLUXDB_ADMIN_USER } -password ${ INFLUXDB_ADMIN_PASSWORD } -precision rfc3339 # Update permissions GRANT WRITE ON service_cluster TO scWriter GRANT WRITE ON workload_cluster TO wcWriter # Exit the influx CLI exit # Exit the InfluxDB container exit","title":"Restore"},{"location":"operator-manual/disaster-recovery/#velero","text":"These instructions make use of the Velero CLI, you can download it here: https://github.com/vmware-tanzu/velero/releases/tag/v1.5.3 (version 1.5.3). The CLI needs the env variable KUBECONFIG set to the path of a decrypted kubeconfig. Read more about Velero here: https://compliantkubernetes.io/user-guide/backup/ Note This documentation uses the Velero CLI, as opposed to Velero CRDs, since that is what is encouraged by upstream documentation.","title":"Velero"},{"location":"operator-manual/disaster-recovery/#backup_3","text":"Velero is set up to take daily backups and store them in an S3 bucket. The daily backup will not take backups of everything in a kubernetes cluster, it will instead look for certain labels and annotations. Read more about those labels and annotations here: https://compliantkubernetes.io/user-guide/backup/#backing-up It is also possible to take on-demand backups. Then you can freely chose what to backup and do not have to base it on the same labels. A basic example with the Velero CLI would be velero backup create manual-backup , which would take a backup of all kubernetes resources (though not the data in the volumes by default). Check which arguments you can use by running velero backup create --help .","title":"Backup"},{"location":"operator-manual/disaster-recovery/#restore_3","text":"Restoring from a backup with Velero is meant to be a type of disaster recovery. Velero will not overwrite existing Resources when restoring. As such, if you want to restore the state of a Resource that is still running, the Resource must be deleted first. To restore the state from the latest daily backup, run: velero restore create --from-schedule velero-daily-backup --wait This command will wait until the restore has finished. You can also do partial restorations, e.g. just restoring one namespace, by using different arguments. You can also restore from manual backups by using the flag --from-backup <backup-name> Persistent Volumes are only restored if a Pod with the backup annotation is restored. Multiple Pods can have an annotation for the same Persistent Volume. When restoring the Persistent Volume it will overwrite any existing files with the same names as the files to be restored. Any other files will be left as they were before the restoration started. So a restore will not wipe the volume clean and then restore. If a clean wipe is the desired behavior, then the volume must be wiped manually before restoring.","title":"Restore"},{"location":"operator-manual/disaster-recovery/#grafana","text":"This refers to the user Grafana, not the ops Grafana.","title":"Grafana"},{"location":"operator-manual/disaster-recovery/#backup_4","text":"Grafana is set up to be included in the daily Velero backup. We then include the Grafana deployment, pod, and PVC (including the data). Manual backups can be taken using velero (include the same resources).","title":"Backup"},{"location":"operator-manual/disaster-recovery/#restore_4","text":"To restore the Grafana backup you must: Have Grafana installed Delete the grafana deployment, PVC and PV kubectl delete deploy -n monitoring user-grafana kubectl delete pvc -n monitoring user-grafana Restore the velero backup velero restore create --from-schedule velero-daily-backup --wait You can also restore Grafana by setting restore.velero in your {CK8S_CONFIG_PATH}/sc-config.yaml to true , and then reapply the service cluster apps: .bin/ck8s apply sc This will go through the same steps as above. By default, the latest daily backup is chosen; to restore from a different backup, set restore.veleroBackupName to the desired backup name.","title":"Restore"},{"location":"operator-manual/eksd/","text":"Compliant Kubernetes on EKS-D based clusters This document contains instructions on how to install Compliant Kubernetes on AWS using EKS-D . Note This guide is written for compliantkubernetes-apps v0.13.0 Requirements An AWS account with billing enabled. A hosted zone in Route53. yq v3.4.1 installed on you machine. gpg2 installed on your machine with at least one key available. kubectl installed on your machine. Infrastructure and Kubernetes Get EKS-D git clone https://github.com/aws/eks-distro.git cd eks-distro/development/kops git checkout v1-19-eks-1 Configure your AWS environment Follow the instructions in Getting Started with kOps on AWS up until you reach Creating your first cluster . Unless you have very specific requirements you shouldn't need to take any action when it comes to the DNS configuration . If you followed the instructions you should have: An IAM user for kOps with the correct permissions. Set AWS credentials and any other AWS environment variables you require in your shell. An S3 bucket for storing the kOps cluster state. Create initial kOps cluster configurations export AWS_REGION=<region where you want the infrastructure to be created> export KOPS_STATE_STORE=s3://<name of the bucket you created in previous step> SERVICE_CLUSTER=\"<xyz, e.g. test-sc>.<your hosted zone in Route53, e.g. example.com>\" WORKLOAD_CLUSTER=\"<xyz, e.g. test-wc>.<your hosted zone in Route53, e.g. example.com>\" for CLUSTER in ${SERVICE_CLUSTER} ${WORKLOAD_CLUSTER}; do export KOPS_CLUSTER_NAME=${CLUSTER} ./create_values_yaml.sh ./create_configuration.sh done Modify kOps cluster configurations for CLUSTER in ${SERVICE_CLUSTER} ${WORKLOAD_CLUSTER}; do echo ' --- - command: update path: spec.etcdClusters[0].manager value: env: - name: ETCD_LISTEN_METRICS_URLS value: http://0.0.0.0:8081 - name: ETCD_METRICS value: basic - command: update path: spec.networking value: calico: encapsulationMode: ipip - command: update path: spec.metricsServer.enabled value: false - command: update path: spec.kubeAPIServer value: image: public.ecr.aws/eks-distro/kubernetes/kube-apiserver:v1.19.6-eks-1-19-1 auditLogMaxAge: 7 auditLogMaxBackups: 1 auditLogMaxSize: 100 auditLogPath: /var/log/kubernetes/audit/kube-apiserver-audit.log auditPolicyFile: /srv/kubernetes/audit/policy-config.yaml enableAdmissionPlugins: - \"PodSecurityPolicy\" - \"NamespaceLifecycle\" - \"LimitRanger\" - \"ServiceAccount\" - \"DefaultStorageClass\" - \"DefaultTolerationSeconds\" - \"MutatingAdmissionWebhook\" - \"ValidatingAdmissionWebhook\" - \"ResourceQuota\" - \"NodeRestriction\" - command: update path: spec.fileAssets value: - name: audit-policy-config path: /srv/kubernetes/audit/policy-config.yaml roles: - Master content: | apiVersion: audit.k8s.io/v1 kind: Policy rules: - level: RequestResponse resources: - group: \"\" resources: [\"pods\"] - level: Metadata resources: - group: \"\" resources: [\"pods/log\", \"pods/status\"] - level: None resources: - group: \"\" resources: [\"configmaps\"] resourceNames: [\"controller-leader\"] - level: None users: [\"system:kube-proxy\"] verbs: [\"watch\"] resources: - group: \"\" # core API group resources: [\"endpoints\", \"services\"] - level: None userGroups: [\"system:authenticated\"] nonResourceURLs: - \"/api*\" # Wildcard matching. - \"/version\" - level: Request resources: - group: \"\" # core API group resources: [\"configmaps\"] namespaces: [\"kube-system\"] - level: Metadata resources: - group: \"\" # core API group resources: [\"secrets\", \"configmaps\"] - level: Request resources: - group: \"\" # core API group - group: \"extensions\" # Version of group should NOT be included. - level: Metadata omitStages: - \"RequestReceived\" ' | yq w -i -s - ${CLUSTER}/${CLUSTER}.yaml done # Configure OIDC flags for kube-apiserver. for CLUSTER in ${WORKLOAD_CLUSTERS}; do yq w -i ${CLUSTER}/${CLUSTER}.yaml 'spec.kubeAPIServer.oidcIssuerURL' https://dex.${SERVICE_CLUSTER} yq w -i ${CLUSTER}/${CLUSTER}.yaml 'spec.kubeAPIServer.oidcUsernameClaim' email yq w -i ${CLUSTER}/${CLUSTER}.yaml 'spec.kubeAPIServer.oidcClientID' kubelogin done # Use bigger machines for service cluster worker nodes. yq w -i -d2 ${SERVICE_CLUSTER}/${SERVICE_CLUSTER}.yaml 'spec.machineType' t3.large # Update kOps cluster configurations in state bucket. for CLUSTER in ${SERVICE_CLUSTER} ${WORKLOAD_CLUSTER}; do ./bin/kops-1-19 replace -f \"./${CLUSTER}/${CLUSTER}.yaml\" done Create clusters for CLUSTER in ${SERVICE_CLUSTER} ${WORKLOAD_CLUSTER}; do export KOPS_CLUSTER_NAME=${CLUSTER} ./create_cluster.sh done The creation of the clusters might take anywhere from 5 minutes to 20 minutes. You should run the ./cluster_wait.sh script against all of your clusters as it creates a configmap needed by the aws-iam-authenticator pod, e.g. for CLUSTER in ${SERVICE_CLUSTER} ${WORKLOAD_CLUSTER}; do export KOPS_CLUSTER_NAME=${CLUSTER} kubectl config use-context ${CLUSTER} timeout 600 ./cluster_wait.sh done Compliant Kubernetes Apps Get Compliant Kubernetes Apps git clone git@github.com:elastisys/compliantkubernetes-apps cd compliantkubernetes-apps git checkout v0.13.0 Install requirements ansible-playbook -e 'ansible_python_interpreter=/usr/bin/python3' --ask-become-pass --connection local --inventory 127.0.0.1, get-requirements.yaml Initialize configuration export CK8S_ENVIRONMENT_NAME=aws-eks-d #export CK8S_FLAVOR=[dev|prod] # defaults to dev export CK8S_CONFIG_PATH=~/.ck8s/aws-eks-d export CK8S_CLOUD_PROVIDER=aws export CK8S_PGP_FP=<your GPG key ID> # retrieve with gpg --list-secret-keys ./bin/ck8s init Three files, sc-config.yaml and wc-config.yaml , and secrets.yaml , were generated in the ${CK8S_CONFIG_PATH} directory. ls -l ${CK8S_CONFIG_PATH} Edit configuration files Edit the configuration files sc-config.yaml , wc-config.yaml and secrets.yaml and set the appropriate values for some of the configuration fields. Note that, the latter is encrypted. vim ${CK8S_CONFIG_PATH}/sc-config.yaml vim ${CK8S_CONFIG_PATH}/wc-config.yaml sops ${CK8S_CONFIG_PATH}/secrets.yaml You should perform the following changes: # sc-config.yaml global: baseDomain: \"set-me\" # Set to ${SERVICE_CLUSTER} opsDomain: \"set-me\" # Set to ops.${SERVICE_CLUSTER} issuer: letsencrypt-prod verifyTls: true clusterDNS: 100.64.0.10 storageClasses: default: kops-ssd-1-17 nfs: enabled: false cinder: enabled: false local: enabled: false ebs: enabled: false objectStorage: type: \"s3\" s3: region: \"set-me\" # e.g. eu-north-1 regionEndpoint: \"set-me\" # e.g. https://s3.eu-north-1.amazonaws.com issuers: letsencrypt: prod: email: \"set-me\" # Set to a valid email address staging: email: \"set-me\" # Set to a valid email address # wc-config.yaml global: baseDomain: \"set-me\" # Set to ${WORKLOAD_CLUSTER} opsDomain: \"set-me\" # Set to ops.${SERVICE_CLUSTER} issuer: letsencrypt-prod verifyTls: true clusterDNS: 100.64.0.10 storageClasses: default: kops-ssd-1-17 nfs: enabled: false cinder: enabled: false local: enabled: false ebs: enabled: false objectStorage: type: \"s3\" s3: region: \"set-me\" # e.g. eu-north-1 regionEndpoint: \"set-me\" # e.g. https://s3.eu-north-1.amazonaws.com opa: enabled: false # Does not work with k8s 1.19+ # secrets.yaml objectStorage: s3: accessKey: \"set-me\" # Set to your AWS S3 accesskey secretKey: \"set-me\" # Set to your AWS S3 secretKey PSP and RBAC Since we've enabled the PodSecurityPolicy admission plugin in the kube-apiserver we'll need to create some basic PSPs and RBAC rules that both you and Compliant Kubernetes Apps will need to run workloads. for CLUSTER in ${SERVICE_CLUSTER} ${WORKLOAD_CLUSTER}; do kubectl config use-context ${CLUSTER} # Install 'restricted' and 'privileged' podSecurityPolicies. kubectl apply -f https://raw.githubusercontent.com/kubernetes/website/master/content/en/examples/policy/privileged-psp.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes/website/master/content/en/examples/policy/restricted-psp.yaml # Install RBAC so authenticated users are be able to use the 'restricted' psp. echo ' --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: addonmanager.kubernetes.io/mode: Reconcile name: psp:restricted rules: - apiGroups: - policy resourceNames: - restricted resources: - podsecuritypolicies verbs: - use --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: psp:any:restricted roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: psp:restricted subjects: - apiGroup: rbac.authorization.k8s.io kind: Group name: system:authenticated ' | kubectl apply -f - done Create placeholder DNS records To avoid negative caching and other surprises. Create the following records using your favorite tool or you can use the Import zone file feature in Route53: echo \" *.${SERVICE_CLUSTER} 60s A 203.0.113.123 *.${WORKLOAD_CLUSTER} 60s A 203.0.113.123 *.ops.${SERVICE_CLUSTER} 60s A 203.0.113.123 \" Create S3 buckets Depending on you configuration you may want to create S3 buckets. Create the following buckets using your favorite tool or via the AWS console: # List bucket names. { yq r ${CK8S_CONFIG_PATH}/wc-config.yaml 'objectStorage.buckets.*';\\ yq r ${CK8S_CONFIG_PATH}/sc-config.yaml 'objectStorage.buckets.*'; } | sort | uniq # Create buckets using the AWS CLI. # Assumes that the same bucket is used for velero in both service and workload cluster. for BUCKET in $(yq r ${CK8S_CONFIG_PATH}/sc-config.yaml 'objectStorage.buckets.*'); do aws s3api create-bucket\\ --bucket ${BUCKET} \\ --create-bucket-configuration LocationConstraint=${AWS_REGION} done Prepare kubeconfigs Compliant Kubernetes Apps demands that the kube contexts for the workload and service cluster are found in separate files encrypted with sops. kubectl config view --minify --flatten --context=${SERVICE_CLUSTER} > ${CK8S_CONFIG_PATH}/.state/kube_config_sc.yaml sops -e -i --config ${CK8S_CONFIG_PATH}/.sops.yaml ${CK8S_CONFIG_PATH}/.state/kube_config_sc.yaml kubectl config view --minify --flatten --context=${WORKLOAD_CLUSTER} > ${CK8S_CONFIG_PATH}/.state/kube_config_wc.yaml sops -e -i --config ${CK8S_CONFIG_PATH}/.sops.yaml ${CK8S_CONFIG_PATH}/.state/kube_config_wc.yaml Install apps You can install apps in parallel, although it is recommended to install the service cluster before the workload cluster. # Service cluster ./bin/ck8s apply sc # Respond \"n\" if you get a WARN # Workload cluster ./bin/ck8s apply wc # Respond \"n\" if you get a WARN Run the following to get metrics from etcd-manager # Service cluster ./bin/ck8s ops helmfile sc -l app=kube-prometheus-stack apply --skip-deps --set kubeEtcd.service.selector.k8s-app=etcd-manager-main --set kubeEtcd.service.targetPort=8081 # Workload cluster ./bin/ck8s ops helmfile wc -l app=kube-prometheus-stack apply --skip-deps --set kubeEtcd.service.selector.k8s-app=etcd-manager-main --set kubeEtcd.service.targetPort=8081 Update DNS records Now that we've installed all applications, the loadbalancer fronting the ingress controller should be ready. Run the following commands and update the A records in Route53. sc_lb=$(./bin/ck8s ops kubectl sc -n ingress-nginx get svc ingress-nginx-controller -ojsonpath={.status.loadBalancer.ingress[0].hostname}) wc_lb=$(./bin/ck8s ops kubectl wc -n ingress-nginx get svc ingress-nginx-controller -ojsonpath={.status.loadBalancer.ingress[0].hostname}) sc_lb_ip=$(dig +short ${sc_lb} | head -1) wc_lb_ip=$(dig +short ${wc_lb} | head -1) echo \" *.${SERVICE_CLUSTER} 60s A ${sc_lb_ip} *.${WORKLOAD_CLUSTER} 60s A ${wc_lb_ip} *.ops.${SERVICE_CLUSTER} 60s A ${sc_lb_ip} \" Teardown Compliant Kubernetes Apps This step is optional. If this is not run you'll have to check and manually remove any leftover cloud resources like S3 buckets, ELBs, and EBS volumes. git checkout 6f2e386 timeout 180 ./scripts/clean-wc.sh timeout 180 ./scripts/clean-sc.sh # Delete buckets for BUCKET in $(yq r ${CK8S_CONFIG_PATH}/sc-config.yaml 'objectStorage.buckets.*'); do aws s3 rb --force s3://${BUCKET} done # Delete config repo rm -rf ${CK8S_CONFIG_PATH} Remember to also remove the A records from Route53. Infrastructure and Kubernetes Enter eks-distro/development/kops and run: # Destroy clusters and local cluster configurations. for CLUSTER in ${SERVICE_CLUSTER} ${WORKLOAD_CLUSTER}; do export KOPS_CLUSTER_NAME=${CLUSTER} ./delete_cluster.sh rm -rf ${CLUSTER} done You'll have to manually remove the leftover kOps A records from Route53. # Get names of the A records to be removed. for CLUSTER in ${SERVICE_CLUSTER} ${WORKLOAD_CLUSTER}; do echo kops-controller.internal.${CLUSTER} done Finally, you'll also need to remove the ${KOPS_STATE_STORE} from S3 and the IAM user that you used for this guide.","title":"On EKS-D"},{"location":"operator-manual/eksd/#compliant-kubernetes-on-eks-d-based-clusters","text":"This document contains instructions on how to install Compliant Kubernetes on AWS using EKS-D . Note This guide is written for compliantkubernetes-apps v0.13.0","title":"Compliant Kubernetes on EKS-D based clusters"},{"location":"operator-manual/eksd/#requirements","text":"An AWS account with billing enabled. A hosted zone in Route53. yq v3.4.1 installed on you machine. gpg2 installed on your machine with at least one key available. kubectl installed on your machine.","title":"Requirements"},{"location":"operator-manual/eksd/#infrastructure-and-kubernetes","text":"","title":"Infrastructure and Kubernetes"},{"location":"operator-manual/eksd/#get-eks-d","text":"git clone https://github.com/aws/eks-distro.git cd eks-distro/development/kops git checkout v1-19-eks-1","title":"Get EKS-D"},{"location":"operator-manual/eksd/#configure-your-aws-environment","text":"Follow the instructions in Getting Started with kOps on AWS up until you reach Creating your first cluster . Unless you have very specific requirements you shouldn't need to take any action when it comes to the DNS configuration . If you followed the instructions you should have: An IAM user for kOps with the correct permissions. Set AWS credentials and any other AWS environment variables you require in your shell. An S3 bucket for storing the kOps cluster state.","title":"Configure your AWS environment"},{"location":"operator-manual/eksd/#create-initial-kops-cluster-configurations","text":"export AWS_REGION=<region where you want the infrastructure to be created> export KOPS_STATE_STORE=s3://<name of the bucket you created in previous step> SERVICE_CLUSTER=\"<xyz, e.g. test-sc>.<your hosted zone in Route53, e.g. example.com>\" WORKLOAD_CLUSTER=\"<xyz, e.g. test-wc>.<your hosted zone in Route53, e.g. example.com>\" for CLUSTER in ${SERVICE_CLUSTER} ${WORKLOAD_CLUSTER}; do export KOPS_CLUSTER_NAME=${CLUSTER} ./create_values_yaml.sh ./create_configuration.sh done","title":"Create initial kOps cluster configurations"},{"location":"operator-manual/eksd/#modify-kops-cluster-configurations","text":"for CLUSTER in ${SERVICE_CLUSTER} ${WORKLOAD_CLUSTER}; do echo ' --- - command: update path: spec.etcdClusters[0].manager value: env: - name: ETCD_LISTEN_METRICS_URLS value: http://0.0.0.0:8081 - name: ETCD_METRICS value: basic - command: update path: spec.networking value: calico: encapsulationMode: ipip - command: update path: spec.metricsServer.enabled value: false - command: update path: spec.kubeAPIServer value: image: public.ecr.aws/eks-distro/kubernetes/kube-apiserver:v1.19.6-eks-1-19-1 auditLogMaxAge: 7 auditLogMaxBackups: 1 auditLogMaxSize: 100 auditLogPath: /var/log/kubernetes/audit/kube-apiserver-audit.log auditPolicyFile: /srv/kubernetes/audit/policy-config.yaml enableAdmissionPlugins: - \"PodSecurityPolicy\" - \"NamespaceLifecycle\" - \"LimitRanger\" - \"ServiceAccount\" - \"DefaultStorageClass\" - \"DefaultTolerationSeconds\" - \"MutatingAdmissionWebhook\" - \"ValidatingAdmissionWebhook\" - \"ResourceQuota\" - \"NodeRestriction\" - command: update path: spec.fileAssets value: - name: audit-policy-config path: /srv/kubernetes/audit/policy-config.yaml roles: - Master content: | apiVersion: audit.k8s.io/v1 kind: Policy rules: - level: RequestResponse resources: - group: \"\" resources: [\"pods\"] - level: Metadata resources: - group: \"\" resources: [\"pods/log\", \"pods/status\"] - level: None resources: - group: \"\" resources: [\"configmaps\"] resourceNames: [\"controller-leader\"] - level: None users: [\"system:kube-proxy\"] verbs: [\"watch\"] resources: - group: \"\" # core API group resources: [\"endpoints\", \"services\"] - level: None userGroups: [\"system:authenticated\"] nonResourceURLs: - \"/api*\" # Wildcard matching. - \"/version\" - level: Request resources: - group: \"\" # core API group resources: [\"configmaps\"] namespaces: [\"kube-system\"] - level: Metadata resources: - group: \"\" # core API group resources: [\"secrets\", \"configmaps\"] - level: Request resources: - group: \"\" # core API group - group: \"extensions\" # Version of group should NOT be included. - level: Metadata omitStages: - \"RequestReceived\" ' | yq w -i -s - ${CLUSTER}/${CLUSTER}.yaml done # Configure OIDC flags for kube-apiserver. for CLUSTER in ${WORKLOAD_CLUSTERS}; do yq w -i ${CLUSTER}/${CLUSTER}.yaml 'spec.kubeAPIServer.oidcIssuerURL' https://dex.${SERVICE_CLUSTER} yq w -i ${CLUSTER}/${CLUSTER}.yaml 'spec.kubeAPIServer.oidcUsernameClaim' email yq w -i ${CLUSTER}/${CLUSTER}.yaml 'spec.kubeAPIServer.oidcClientID' kubelogin done # Use bigger machines for service cluster worker nodes. yq w -i -d2 ${SERVICE_CLUSTER}/${SERVICE_CLUSTER}.yaml 'spec.machineType' t3.large # Update kOps cluster configurations in state bucket. for CLUSTER in ${SERVICE_CLUSTER} ${WORKLOAD_CLUSTER}; do ./bin/kops-1-19 replace -f \"./${CLUSTER}/${CLUSTER}.yaml\" done","title":"Modify kOps cluster configurations"},{"location":"operator-manual/eksd/#create-clusters","text":"for CLUSTER in ${SERVICE_CLUSTER} ${WORKLOAD_CLUSTER}; do export KOPS_CLUSTER_NAME=${CLUSTER} ./create_cluster.sh done The creation of the clusters might take anywhere from 5 minutes to 20 minutes. You should run the ./cluster_wait.sh script against all of your clusters as it creates a configmap needed by the aws-iam-authenticator pod, e.g. for CLUSTER in ${SERVICE_CLUSTER} ${WORKLOAD_CLUSTER}; do export KOPS_CLUSTER_NAME=${CLUSTER} kubectl config use-context ${CLUSTER} timeout 600 ./cluster_wait.sh done","title":"Create clusters"},{"location":"operator-manual/eksd/#compliant-kubernetes-apps","text":"","title":"Compliant Kubernetes Apps"},{"location":"operator-manual/eksd/#get-compliant-kubernetes-apps","text":"git clone git@github.com:elastisys/compliantkubernetes-apps cd compliantkubernetes-apps git checkout v0.13.0","title":"Get Compliant Kubernetes Apps"},{"location":"operator-manual/eksd/#install-requirements","text":"ansible-playbook -e 'ansible_python_interpreter=/usr/bin/python3' --ask-become-pass --connection local --inventory 127.0.0.1, get-requirements.yaml","title":"Install requirements"},{"location":"operator-manual/eksd/#initialize-configuration","text":"export CK8S_ENVIRONMENT_NAME=aws-eks-d #export CK8S_FLAVOR=[dev|prod] # defaults to dev export CK8S_CONFIG_PATH=~/.ck8s/aws-eks-d export CK8S_CLOUD_PROVIDER=aws export CK8S_PGP_FP=<your GPG key ID> # retrieve with gpg --list-secret-keys ./bin/ck8s init Three files, sc-config.yaml and wc-config.yaml , and secrets.yaml , were generated in the ${CK8S_CONFIG_PATH} directory. ls -l ${CK8S_CONFIG_PATH}","title":"Initialize configuration"},{"location":"operator-manual/eksd/#edit-configuration-files","text":"Edit the configuration files sc-config.yaml , wc-config.yaml and secrets.yaml and set the appropriate values for some of the configuration fields. Note that, the latter is encrypted. vim ${CK8S_CONFIG_PATH}/sc-config.yaml vim ${CK8S_CONFIG_PATH}/wc-config.yaml sops ${CK8S_CONFIG_PATH}/secrets.yaml You should perform the following changes: # sc-config.yaml global: baseDomain: \"set-me\" # Set to ${SERVICE_CLUSTER} opsDomain: \"set-me\" # Set to ops.${SERVICE_CLUSTER} issuer: letsencrypt-prod verifyTls: true clusterDNS: 100.64.0.10 storageClasses: default: kops-ssd-1-17 nfs: enabled: false cinder: enabled: false local: enabled: false ebs: enabled: false objectStorage: type: \"s3\" s3: region: \"set-me\" # e.g. eu-north-1 regionEndpoint: \"set-me\" # e.g. https://s3.eu-north-1.amazonaws.com issuers: letsencrypt: prod: email: \"set-me\" # Set to a valid email address staging: email: \"set-me\" # Set to a valid email address # wc-config.yaml global: baseDomain: \"set-me\" # Set to ${WORKLOAD_CLUSTER} opsDomain: \"set-me\" # Set to ops.${SERVICE_CLUSTER} issuer: letsencrypt-prod verifyTls: true clusterDNS: 100.64.0.10 storageClasses: default: kops-ssd-1-17 nfs: enabled: false cinder: enabled: false local: enabled: false ebs: enabled: false objectStorage: type: \"s3\" s3: region: \"set-me\" # e.g. eu-north-1 regionEndpoint: \"set-me\" # e.g. https://s3.eu-north-1.amazonaws.com opa: enabled: false # Does not work with k8s 1.19+ # secrets.yaml objectStorage: s3: accessKey: \"set-me\" # Set to your AWS S3 accesskey secretKey: \"set-me\" # Set to your AWS S3 secretKey","title":"Edit configuration files"},{"location":"operator-manual/eksd/#psp-and-rbac","text":"Since we've enabled the PodSecurityPolicy admission plugin in the kube-apiserver we'll need to create some basic PSPs and RBAC rules that both you and Compliant Kubernetes Apps will need to run workloads. for CLUSTER in ${SERVICE_CLUSTER} ${WORKLOAD_CLUSTER}; do kubectl config use-context ${CLUSTER} # Install 'restricted' and 'privileged' podSecurityPolicies. kubectl apply -f https://raw.githubusercontent.com/kubernetes/website/master/content/en/examples/policy/privileged-psp.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes/website/master/content/en/examples/policy/restricted-psp.yaml # Install RBAC so authenticated users are be able to use the 'restricted' psp. echo ' --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: addonmanager.kubernetes.io/mode: Reconcile name: psp:restricted rules: - apiGroups: - policy resourceNames: - restricted resources: - podsecuritypolicies verbs: - use --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: psp:any:restricted roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: psp:restricted subjects: - apiGroup: rbac.authorization.k8s.io kind: Group name: system:authenticated ' | kubectl apply -f - done","title":"PSP and RBAC"},{"location":"operator-manual/eksd/#create-placeholder-dns-records","text":"To avoid negative caching and other surprises. Create the following records using your favorite tool or you can use the Import zone file feature in Route53: echo \" *.${SERVICE_CLUSTER} 60s A 203.0.113.123 *.${WORKLOAD_CLUSTER} 60s A 203.0.113.123 *.ops.${SERVICE_CLUSTER} 60s A 203.0.113.123 \"","title":"Create placeholder DNS records"},{"location":"operator-manual/eksd/#create-s3-buckets","text":"Depending on you configuration you may want to create S3 buckets. Create the following buckets using your favorite tool or via the AWS console: # List bucket names. { yq r ${CK8S_CONFIG_PATH}/wc-config.yaml 'objectStorage.buckets.*';\\ yq r ${CK8S_CONFIG_PATH}/sc-config.yaml 'objectStorage.buckets.*'; } | sort | uniq # Create buckets using the AWS CLI. # Assumes that the same bucket is used for velero in both service and workload cluster. for BUCKET in $(yq r ${CK8S_CONFIG_PATH}/sc-config.yaml 'objectStorage.buckets.*'); do aws s3api create-bucket\\ --bucket ${BUCKET} \\ --create-bucket-configuration LocationConstraint=${AWS_REGION} done","title":"Create S3 buckets"},{"location":"operator-manual/eksd/#prepare-kubeconfigs","text":"Compliant Kubernetes Apps demands that the kube contexts for the workload and service cluster are found in separate files encrypted with sops. kubectl config view --minify --flatten --context=${SERVICE_CLUSTER} > ${CK8S_CONFIG_PATH}/.state/kube_config_sc.yaml sops -e -i --config ${CK8S_CONFIG_PATH}/.sops.yaml ${CK8S_CONFIG_PATH}/.state/kube_config_sc.yaml kubectl config view --minify --flatten --context=${WORKLOAD_CLUSTER} > ${CK8S_CONFIG_PATH}/.state/kube_config_wc.yaml sops -e -i --config ${CK8S_CONFIG_PATH}/.sops.yaml ${CK8S_CONFIG_PATH}/.state/kube_config_wc.yaml","title":"Prepare kubeconfigs"},{"location":"operator-manual/eksd/#install-apps","text":"You can install apps in parallel, although it is recommended to install the service cluster before the workload cluster. # Service cluster ./bin/ck8s apply sc # Respond \"n\" if you get a WARN # Workload cluster ./bin/ck8s apply wc # Respond \"n\" if you get a WARN Run the following to get metrics from etcd-manager # Service cluster ./bin/ck8s ops helmfile sc -l app=kube-prometheus-stack apply --skip-deps --set kubeEtcd.service.selector.k8s-app=etcd-manager-main --set kubeEtcd.service.targetPort=8081 # Workload cluster ./bin/ck8s ops helmfile wc -l app=kube-prometheus-stack apply --skip-deps --set kubeEtcd.service.selector.k8s-app=etcd-manager-main --set kubeEtcd.service.targetPort=8081","title":"Install apps"},{"location":"operator-manual/eksd/#update-dns-records","text":"Now that we've installed all applications, the loadbalancer fronting the ingress controller should be ready. Run the following commands and update the A records in Route53. sc_lb=$(./bin/ck8s ops kubectl sc -n ingress-nginx get svc ingress-nginx-controller -ojsonpath={.status.loadBalancer.ingress[0].hostname}) wc_lb=$(./bin/ck8s ops kubectl wc -n ingress-nginx get svc ingress-nginx-controller -ojsonpath={.status.loadBalancer.ingress[0].hostname}) sc_lb_ip=$(dig +short ${sc_lb} | head -1) wc_lb_ip=$(dig +short ${wc_lb} | head -1) echo \" *.${SERVICE_CLUSTER} 60s A ${sc_lb_ip} *.${WORKLOAD_CLUSTER} 60s A ${wc_lb_ip} *.ops.${SERVICE_CLUSTER} 60s A ${sc_lb_ip} \"","title":"Update DNS records"},{"location":"operator-manual/eksd/#teardown","text":"","title":"Teardown"},{"location":"operator-manual/eksd/#compliant-kubernetes-apps_1","text":"This step is optional. If this is not run you'll have to check and manually remove any leftover cloud resources like S3 buckets, ELBs, and EBS volumes. git checkout 6f2e386 timeout 180 ./scripts/clean-wc.sh timeout 180 ./scripts/clean-sc.sh # Delete buckets for BUCKET in $(yq r ${CK8S_CONFIG_PATH}/sc-config.yaml 'objectStorage.buckets.*'); do aws s3 rb --force s3://${BUCKET} done # Delete config repo rm -rf ${CK8S_CONFIG_PATH} Remember to also remove the A records from Route53.","title":"Compliant Kubernetes Apps"},{"location":"operator-manual/eksd/#infrastructure-and-kubernetes_1","text":"Enter eks-distro/development/kops and run: # Destroy clusters and local cluster configurations. for CLUSTER in ${SERVICE_CLUSTER} ${WORKLOAD_CLUSTER}; do export KOPS_CLUSTER_NAME=${CLUSTER} ./delete_cluster.sh rm -rf ${CLUSTER} done You'll have to manually remove the leftover kOps A records from Route53. # Get names of the A records to be removed. for CLUSTER in ${SERVICE_CLUSTER} ${WORKLOAD_CLUSTER}; do echo kops-controller.internal.${CLUSTER} done Finally, you'll also need to remove the ${KOPS_STATE_STORE} from S3 and the IAM user that you used for this guide.","title":"Infrastructure and Kubernetes"},{"location":"operator-manual/exoscale/","text":"Compliant Kubernetes Deployment on Exoscale This document contains instructions on how to setup a service cluster and a workload cluster in Exoscale. The following are the main tasks addressed in this document: Infrastructure setup for two clusters: one service and one workload cluster Deploying Compliant Kubernetes on top of the two clusters. Creating DNS Records Deploying Rook Storage Orchestration Service Deploying Compliant Kubernetes apps The instructions below are just samples, you need to update them according to your requirements. Besides, the exoscale cli is used to manage DNS. If you are using any other DNS service provider for managing your DNS you can skip it. Before starting, make sure you have all necessary tools . Note This guide is written for compliantkubernetes-apps v0.17.0 Setup Choose names for your service cluster and workload cluster, as well as a name for your environment: SERVICE_CLUSTER = \"testsc\" WORKLOAD_CLUSTERS =( \"testwc0\" ) CK8S_ENVIRONMENT_NAME = my-environment-name Infrastructure Setup using Terraform Before trying any of the steps, clone the Elastisys Compliant Kubernetes Kubespray repo as follows: git clone --recursive https://github.com/elastisys/compliantkubernetes-kubespray cd compliantkubernetes-kubespray/kubespray Expose Exoscale credentials to Terraform For authentication create the file ~/.cloudstack.ini and put your Exoscale credentials in it. The file should look like something like this: [cloudstack] key = <API key> secret = <API secret> Customize your infrastructure Create a configuration for the service and the workload clusters: for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do cp -r inventory/sample inventory/ $CLUSTER cp contrib/terraform/exoscale/default.tfvars inventory/ $CLUSTER / done Review and, if needed, adjust the files in inventory/$CLUSTER/default.tfvars , where $CLUSTER is the cluster name: Use different value for the prefix field in /default.tfvars for the two clusters. Failing to do so will result in a name conflict. Set a non-zero value for ceph_partition_size field, e.g., \"ceph_partition_size\": 50 , as it will be used by Rook storage service to provide local disk storage. To security harden your cluster, set ssh_whitelist and api_server_whitelist to the IP addresses from which you expect to operate the cluster. Make sure you configure your SSH keys in ssh_public_keys . Important The Linux Ubuntu 20.04 LTS 64-bit image on Exoscale is regularly upgraded, which might cause unexpected changes during terraform apply . Consider uploading your own dated Ubuntu image to reduce the risk of downtime. Initialize and Apply Terraform for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do pushd contrib/terraform/exoscale terraform init terraform apply \\ -var-file = ../../../inventory/ $CLUSTER /default.tfvars \\ -state = ../../../inventory/ $CLUSTER /tfstate- $CLUSTER .tfstate cp inventory.ini ../../../inventory/ $CLUSTER / popd done Important The Terraform state is stored in inventory/$CLUSTER/tfstate-$CLUSTER.tfstate , where $CLUSTER is the cluster name. It is precious. Consider backing it up or using Terraform Cloud . You should now have inventory file named inventory/$CLUSTER/inventory.ini for each cluster that you can use with kubespray. Test access to all nodes for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do pushd inventory/ $CLUSTER ANSIBLE_HOST_KEY_CHECKING = False ansible all -i inventory.ini -m ping popd done Deploying vanilla Kubernetes clusters using Kubespray With the infrastructure provisioned, we can now deploy Kubernetes using kubespray. First, if you haven't done so already, install the pre-requisites and change to the compliantkubernetes-kubespray root directory. pip3 install -r requirements.txt cd .. Init the Kubespray config in your config path export DOMAIN = <your_domain> # DNS domain to expose the services inside the service cluster i.e. \"example.com\" export CK8S_CONFIG_PATH = ~/.ck8s/exoscale export CK8S_PGP_FP = <your GPG key fingerprint> # retrieve with gpg --list-secret-keys for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ./bin/ck8s-kubespray init $CLUSTER default $CK8S_PGP_FP done Copy the generated inventory files in the right location Please copy the two inventory files, kubespray/inventory/$CLUSTER/inventory.ini , generated by Terraform to ${CK8S_CONFIG_PATH}/$CLUSTER-config/ , where $CLUSTER the name of each cluster (i.e., testsc , testwc0 ). for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do cp kubespray/inventory/$CLUSTER/inventory.ini ${CK8S_CONFIG_PATH}/$CLUSTER-config/ done Run kubespray to deploy the Kubernetes clusters for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ./bin/ck8s-kubespray apply $CLUSTER --flush-cache done This may take up to 10 minutes for each cluster, 20 minutes in total. Correct the Kubernetes API IP addresses Locate the encrypted kubeconfigs in ${CK8S_CONFIG_PATH}/.state/kube_config_*.yaml and edit them using sops. Copy the public IP address of the load balancer from inventory files ${CK8S_CONFIG_PATH}/*-config/inventory.ini and replace the private IP address for the server field in ${CK8S_CONFIG_PATH}/.state/kube_config_*.yaml . for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml done Test access to the clusters as follows for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get nodes' done Create the DNS Records You will need to setup a number of DNS entries for traffic to be routed correctly. Determine the public IP of the load-balancer fronting the Ingress controller of the clusters from the Terraform state file generated during infrastructure setup. To get the load-balancer IP, run the following command: SC_INGRESS_LB_IP_ADDRESS = $( terraform output -state kubespray/inventory/ $SERVICE_CLUSTER /tfstate- $SERVICE_CLUSTER .tfstate -raw ingress_controller_lb_ip_address ) echo $SC_INGRESS_LB_IP_ADDRESS Configure the exoscale CLI: exo config Then point these domains to the service cluster using 'exoscale cli' as follows: exo dns add A $DOMAIN -a $SC_INGRESS_LB_IP_ADDRESS -n *.ops. $CK8S_ENVIRONMENT_NAME exo dns add A $DOMAIN -a $SC_INGRESS_LB_IP_ADDRESS -n *. $CK8S_ENVIRONMENT_NAME Deploy Rook To deploy Rook, please go to the compliantkubernetes-kubespray repo root directory and run the following. for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops --decrypt ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml > $CLUSTER .yaml export KUBECONFIG = $CLUSTER .yaml ./rook/deploy-rook.sh shred -zu $CLUSTER .yaml done Please restart the operator pod, rook-ceph-operator* , if some pods stalls in initialization state as shown below: rook-ceph rook-ceph-crashcollector-minion-0-b75b9fc64-tv2vg 0/1 Init:0/2 0 24m rook-ceph rook-ceph-crashcollector-minion-1-5cfb88b66f-mggrh 0/1 Init:0/2 0 36m rook-ceph rook-ceph-crashcollector-minion-2-5c74ffffb6-jwk55 0/1 Init:0/2 0 14m Important Pods in pending state usually indicate resource shortage. In such cases you need to use bigger instances. Test Rook To test Rook, proceed as follows: for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml 'kubectl --kubeconfig {} apply -f https://raw.githubusercontent.com/rook/rook/release-1.5/cluster/examples/kubernetes/ceph/csi/rbd/pvc.yaml' ; done for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml 'kubectl --kubeconfig {} get pvc' ; done You should see PVCs in Bound state. If you want to clean the previously created PVCs: for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml 'kubectl --kubeconfig {} delete pvc rbd-pvc' ; done Deploying Compliant Kubernetes Apps Now that the Kubernetes clusters are up and running, we are ready to install the Compliant Kubernetes apps. Clone compliantkubernetes-apps and Install Pre-requisites If you haven't done so already, clone the compliantkubernetes-apps repo and install pre-requisites. git clone https://github.com/elastisys/compliantkubernetes-apps.git cd compliantkubernetes-apps ansible-playbook -e 'ansible_python_interpreter=/usr/bin/python3' --ask-become-pass --connection local --inventory 127 .0.0.1, get-requirements.yaml Initialize the apps configuration export CK8S_ENVIRONMENT_NAME = my-environment-name #export CK8S_FLAVOR=[dev|prod] # defaults to dev export CK8S_CONFIG_PATH = ~/.ck8s/my-cluster-path export CK8S_CLOUD_PROVIDER = # [exoscale|safespring|citycloud|aws|baremetal] export CK8S_PGP_FP = <your GPG key fingerprint> # retrieve with gpg --list-secret-keys ./bin/ck8s init Three files, sc-config.yaml and wc-config.yaml , and secrets.yaml , were generated in the ${CK8S_CONFIG_PATH} directory. ls -l $CK8S_CONFIG_PATH Configure the apps Edit the configuration files ${CK8S_CONFIG_PATH}/sc-config.yaml , ${CK8S_CONFIG_PATH}/wc-config.yaml and ${CK8S_CONFIG_PATH}/secrets.yaml and set the appropriate values for some of the configuration fields. Note that, the latter is encrypted. vim ${ CK8S_CONFIG_PATH } /sc-config.yaml vim ${ CK8S_CONFIG_PATH } /wc-config.yaml sops ${ CK8S_CONFIG_PATH } /secrets.yaml The following are the minimum change you should perform: # ${CK8S_CONFIG_PATH}/sc-config.yaml and ${CK8S_CONFIG_PATH}/wc-config.yaml global : baseDomain : \"set-me\" # set to $CK8S_ENVIRONMENT_NAME.$DOMAIN opsDomain : \"set-me\" # set to ops.$CK8S_ENVIRONMENT_NAME.$DOMAIN issuer : letsencrypt-prod objectStorage : type : \"s3\" s3 : region : \"set-me\" # Region for S3 buckets, e.g, west-1 regionEndpoint : \"set-me\" # e.g., https://s3.us-west-1.amazonaws.com storageClasses : default : rook-ceph-block nfs : enabled : false cinder : enabled : false local : enabled : false ebs : enabled : false # ${CK8S_CONFIG_PATH}/sc-config.yaml (in addition to the changes above) ingressNginx : controller : service : type : \"this-is-not-used\" annotations : \"this-is-not-used\" harbor : oidc : groupClaimName : \"set-me\" # set to group claim name used by OIDC provider issuers : letsencrypt : prod : email : \"set-me\" # set this to an email to receive LetsEncrypt notifications staging : email : \"set-me\" # set this to an email to receive LetsEncrypt notifications # ${CK8S_CONFIG_PATH}/secrets.yaml objectStorage : s3 : accessKey : \"set-me\" # set to your s3 accesskey secretKey : \"set-me\" # set to your s3 secretKey Create S3 buckets You can use the following script to create required S3 buckets. The script uses s3cmd in the background and gets configuration and credentials for your S3 provider from ${HOME}/.s3cfg file. # Use your default s3cmd config file: ${HOME}/.s3cfg scripts/S3/entry.sh create Important You should not use your own credentials for S3. Rather create a new set of credentials with write-only access, when supported by the object storage provider ( check a feature matrix ). Test S3 To ensure that you have configured S3 correctly, run the following snippet: ( access_key = $( sops exec-file ${ CK8S_CONFIG_PATH } /secrets.yaml 'yq r {} \"objectStorage.s3.accessKey\"' ) secret_key = $( sops exec-file ${ CK8S_CONFIG_PATH } /secrets.yaml 'yq r {} \"objectStorage.s3.secretKey\"' ) region = $( yq r ${ CK8S_CONFIG_PATH } /sc-config.yaml 'objectStorage.s3.region' ) host = $( yq r ${ CK8S_CONFIG_PATH } /sc-config.yaml 'objectStorage.s3.regionEndpoint' ) for bucket in $( yq r ${ CK8S_CONFIG_PATH } /sc-config.yaml 'objectStorage.buckets.*' ) ; do s3cmd --access_key = ${ access_key } --secret_key = ${ secret_key } \\ --region = ${ region } --host = ${ host } \\ ls s3:// ${ bucket } > /dev/null [ ${ ? } = 0 ] && echo \"Bucket ${ bucket } exists!\" done ) Install Compliant Kubernetes apps Start with the service cluster: ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ SERVICE_CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_sc.yaml ./bin/ck8s apply sc # Respond \"n\" if you get a WARN Then the workload clusters: for CLUSTER in \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_wc.yaml ./bin/ck8s apply wc # Respond \"n\" if you get a WARN done Settling Important Leave sufficient time for the system to settle, e.g., request TLS certificates from LetsEncrypt, perhaps as much as 20 minutes. You can check if the system settled as follows: for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get --all-namespaces pods' done Check the output of the command above. All Pods needs to be Running or Completed. for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get --all-namespaces issuers,clusterissuers,certificates' done Check the output of the command above. All resources need to have the Ready column True. Testing After completing the installation step you can test if the apps are properly installed and ready using the commands below. Start with the service cluster: ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ SERVICE_CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_sc.yaml ./bin/ck8s test sc # Respond \"n\" if you get a WARN Then the workload clusters: for CLUSTER in \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_wc.yaml ./bin/ck8s test wc # Respond \"n\" if you get a WARN done Done. Navigate to the endpoints, for example grafana.$BASE_DOMAIN , kibana.$BASE_DOMAIN , harbor.$BASE_DOMAIN , etc. to discover Compliant Kubernetes's features. Teardown Removing Compliant Kubernetes Apps from your cluster To remove the applications added by compliant kubernetes you can use the two scripts clean-sc.sh and clean-wc.sh , they are located here in the scripts folder . They perform the following actions: Delete the added helm charts Delete the added namespaces Delete any remaining PersistentVolumes Delete the added CustomResourceDefinitions Note: if user namespaces are managed by Compliant Kubernetes apps then they will also be deleted if you clean up the workload cluster. Remove infrastructure To teardown the infrastructure, please switch to the root directory of the exoscale branch of the Kubespray repo (see the Terraform section). for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do pushd contrib/terraform/exoscale terraform init terraform destroy \\ -var-file = ../../../inventory/ $CLUSTER /default.tfvars \\ -state = ../../../inventory/ $CLUSTER /tfstate- $CLUSTER .tfstate popd done # Remove DNS records exo dns remove $DOMAIN *.ops. $CK8S_ENVIRONMENT_NAME exo dns remove $DOMAIN *. $CK8S_ENVIRONMENT_NAME Further Reading Elastisys Compliant Kubernetes Kubespray Kubernetes on Exoscale with Terraform Compliant Kubernetes apps repo Configurations option","title":"On Exoscale"},{"location":"operator-manual/exoscale/#compliant-kubernetes-deployment-on-exoscale","text":"This document contains instructions on how to setup a service cluster and a workload cluster in Exoscale. The following are the main tasks addressed in this document: Infrastructure setup for two clusters: one service and one workload cluster Deploying Compliant Kubernetes on top of the two clusters. Creating DNS Records Deploying Rook Storage Orchestration Service Deploying Compliant Kubernetes apps The instructions below are just samples, you need to update them according to your requirements. Besides, the exoscale cli is used to manage DNS. If you are using any other DNS service provider for managing your DNS you can skip it. Before starting, make sure you have all necessary tools . Note This guide is written for compliantkubernetes-apps v0.17.0","title":"Compliant Kubernetes Deployment on Exoscale"},{"location":"operator-manual/exoscale/#setup","text":"Choose names for your service cluster and workload cluster, as well as a name for your environment: SERVICE_CLUSTER = \"testsc\" WORKLOAD_CLUSTERS =( \"testwc0\" ) CK8S_ENVIRONMENT_NAME = my-environment-name","title":"Setup"},{"location":"operator-manual/exoscale/#infrastructure-setup-using-terraform","text":"Before trying any of the steps, clone the Elastisys Compliant Kubernetes Kubespray repo as follows: git clone --recursive https://github.com/elastisys/compliantkubernetes-kubespray cd compliantkubernetes-kubespray/kubespray","title":"Infrastructure Setup using Terraform"},{"location":"operator-manual/exoscale/#expose-exoscale-credentials-to-terraform","text":"For authentication create the file ~/.cloudstack.ini and put your Exoscale credentials in it. The file should look like something like this: [cloudstack] key = <API key> secret = <API secret>","title":"Expose Exoscale credentials to Terraform"},{"location":"operator-manual/exoscale/#customize-your-infrastructure","text":"Create a configuration for the service and the workload clusters: for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do cp -r inventory/sample inventory/ $CLUSTER cp contrib/terraform/exoscale/default.tfvars inventory/ $CLUSTER / done Review and, if needed, adjust the files in inventory/$CLUSTER/default.tfvars , where $CLUSTER is the cluster name: Use different value for the prefix field in /default.tfvars for the two clusters. Failing to do so will result in a name conflict. Set a non-zero value for ceph_partition_size field, e.g., \"ceph_partition_size\": 50 , as it will be used by Rook storage service to provide local disk storage. To security harden your cluster, set ssh_whitelist and api_server_whitelist to the IP addresses from which you expect to operate the cluster. Make sure you configure your SSH keys in ssh_public_keys . Important The Linux Ubuntu 20.04 LTS 64-bit image on Exoscale is regularly upgraded, which might cause unexpected changes during terraform apply . Consider uploading your own dated Ubuntu image to reduce the risk of downtime.","title":"Customize your infrastructure"},{"location":"operator-manual/exoscale/#initialize-and-apply-terraform","text":"for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do pushd contrib/terraform/exoscale terraform init terraform apply \\ -var-file = ../../../inventory/ $CLUSTER /default.tfvars \\ -state = ../../../inventory/ $CLUSTER /tfstate- $CLUSTER .tfstate cp inventory.ini ../../../inventory/ $CLUSTER / popd done Important The Terraform state is stored in inventory/$CLUSTER/tfstate-$CLUSTER.tfstate , where $CLUSTER is the cluster name. It is precious. Consider backing it up or using Terraform Cloud . You should now have inventory file named inventory/$CLUSTER/inventory.ini for each cluster that you can use with kubespray.","title":"Initialize and Apply Terraform"},{"location":"operator-manual/exoscale/#test-access-to-all-nodes","text":"for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do pushd inventory/ $CLUSTER ANSIBLE_HOST_KEY_CHECKING = False ansible all -i inventory.ini -m ping popd done","title":"Test access to all nodes"},{"location":"operator-manual/exoscale/#deploying-vanilla-kubernetes-clusters-using-kubespray","text":"With the infrastructure provisioned, we can now deploy Kubernetes using kubespray. First, if you haven't done so already, install the pre-requisites and change to the compliantkubernetes-kubespray root directory. pip3 install -r requirements.txt cd ..","title":"Deploying vanilla Kubernetes clusters using Kubespray"},{"location":"operator-manual/exoscale/#init-the-kubespray-config-in-your-config-path","text":"export DOMAIN = <your_domain> # DNS domain to expose the services inside the service cluster i.e. \"example.com\" export CK8S_CONFIG_PATH = ~/.ck8s/exoscale export CK8S_PGP_FP = <your GPG key fingerprint> # retrieve with gpg --list-secret-keys for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ./bin/ck8s-kubespray init $CLUSTER default $CK8S_PGP_FP done","title":"Init the Kubespray config in your config path"},{"location":"operator-manual/exoscale/#copy-the-generated-inventory-files-in-the-right-location","text":"Please copy the two inventory files, kubespray/inventory/$CLUSTER/inventory.ini , generated by Terraform to ${CK8S_CONFIG_PATH}/$CLUSTER-config/ , where $CLUSTER the name of each cluster (i.e., testsc , testwc0 ). for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do cp kubespray/inventory/$CLUSTER/inventory.ini ${CK8S_CONFIG_PATH}/$CLUSTER-config/ done","title":"Copy the generated inventory files in the right location"},{"location":"operator-manual/exoscale/#run-kubespray-to-deploy-the-kubernetes-clusters","text":"for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ./bin/ck8s-kubespray apply $CLUSTER --flush-cache done This may take up to 10 minutes for each cluster, 20 minutes in total.","title":"Run kubespray to deploy the Kubernetes clusters"},{"location":"operator-manual/exoscale/#correct-the-kubernetes-api-ip-addresses","text":"Locate the encrypted kubeconfigs in ${CK8S_CONFIG_PATH}/.state/kube_config_*.yaml and edit them using sops. Copy the public IP address of the load balancer from inventory files ${CK8S_CONFIG_PATH}/*-config/inventory.ini and replace the private IP address for the server field in ${CK8S_CONFIG_PATH}/.state/kube_config_*.yaml . for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml done","title":"Correct the Kubernetes API IP addresses"},{"location":"operator-manual/exoscale/#test-access-to-the-clusters-as-follows","text":"for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get nodes' done","title":"Test access to the clusters as follows"},{"location":"operator-manual/exoscale/#create-the-dns-records","text":"You will need to setup a number of DNS entries for traffic to be routed correctly. Determine the public IP of the load-balancer fronting the Ingress controller of the clusters from the Terraform state file generated during infrastructure setup. To get the load-balancer IP, run the following command: SC_INGRESS_LB_IP_ADDRESS = $( terraform output -state kubespray/inventory/ $SERVICE_CLUSTER /tfstate- $SERVICE_CLUSTER .tfstate -raw ingress_controller_lb_ip_address ) echo $SC_INGRESS_LB_IP_ADDRESS Configure the exoscale CLI: exo config Then point these domains to the service cluster using 'exoscale cli' as follows: exo dns add A $DOMAIN -a $SC_INGRESS_LB_IP_ADDRESS -n *.ops. $CK8S_ENVIRONMENT_NAME exo dns add A $DOMAIN -a $SC_INGRESS_LB_IP_ADDRESS -n *. $CK8S_ENVIRONMENT_NAME","title":"Create the DNS Records"},{"location":"operator-manual/exoscale/#deploy-rook","text":"To deploy Rook, please go to the compliantkubernetes-kubespray repo root directory and run the following. for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops --decrypt ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml > $CLUSTER .yaml export KUBECONFIG = $CLUSTER .yaml ./rook/deploy-rook.sh shred -zu $CLUSTER .yaml done Please restart the operator pod, rook-ceph-operator* , if some pods stalls in initialization state as shown below: rook-ceph rook-ceph-crashcollector-minion-0-b75b9fc64-tv2vg 0/1 Init:0/2 0 24m rook-ceph rook-ceph-crashcollector-minion-1-5cfb88b66f-mggrh 0/1 Init:0/2 0 36m rook-ceph rook-ceph-crashcollector-minion-2-5c74ffffb6-jwk55 0/1 Init:0/2 0 14m Important Pods in pending state usually indicate resource shortage. In such cases you need to use bigger instances.","title":"Deploy Rook"},{"location":"operator-manual/exoscale/#test-rook","text":"To test Rook, proceed as follows: for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml 'kubectl --kubeconfig {} apply -f https://raw.githubusercontent.com/rook/rook/release-1.5/cluster/examples/kubernetes/ceph/csi/rbd/pvc.yaml' ; done for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml 'kubectl --kubeconfig {} get pvc' ; done You should see PVCs in Bound state. If you want to clean the previously created PVCs: for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml 'kubectl --kubeconfig {} delete pvc rbd-pvc' ; done","title":"Test Rook"},{"location":"operator-manual/exoscale/#deploying-compliant-kubernetes-apps","text":"Now that the Kubernetes clusters are up and running, we are ready to install the Compliant Kubernetes apps.","title":"Deploying Compliant Kubernetes Apps"},{"location":"operator-manual/exoscale/#clone-compliantkubernetes-apps-and-install-pre-requisites","text":"If you haven't done so already, clone the compliantkubernetes-apps repo and install pre-requisites. git clone https://github.com/elastisys/compliantkubernetes-apps.git cd compliantkubernetes-apps ansible-playbook -e 'ansible_python_interpreter=/usr/bin/python3' --ask-become-pass --connection local --inventory 127 .0.0.1, get-requirements.yaml","title":"Clone compliantkubernetes-apps and Install Pre-requisites"},{"location":"operator-manual/exoscale/#initialize-the-apps-configuration","text":"export CK8S_ENVIRONMENT_NAME = my-environment-name #export CK8S_FLAVOR=[dev|prod] # defaults to dev export CK8S_CONFIG_PATH = ~/.ck8s/my-cluster-path export CK8S_CLOUD_PROVIDER = # [exoscale|safespring|citycloud|aws|baremetal] export CK8S_PGP_FP = <your GPG key fingerprint> # retrieve with gpg --list-secret-keys ./bin/ck8s init Three files, sc-config.yaml and wc-config.yaml , and secrets.yaml , were generated in the ${CK8S_CONFIG_PATH} directory. ls -l $CK8S_CONFIG_PATH","title":"Initialize the apps configuration"},{"location":"operator-manual/exoscale/#configure-the-apps","text":"Edit the configuration files ${CK8S_CONFIG_PATH}/sc-config.yaml , ${CK8S_CONFIG_PATH}/wc-config.yaml and ${CK8S_CONFIG_PATH}/secrets.yaml and set the appropriate values for some of the configuration fields. Note that, the latter is encrypted. vim ${ CK8S_CONFIG_PATH } /sc-config.yaml vim ${ CK8S_CONFIG_PATH } /wc-config.yaml sops ${ CK8S_CONFIG_PATH } /secrets.yaml The following are the minimum change you should perform: # ${CK8S_CONFIG_PATH}/sc-config.yaml and ${CK8S_CONFIG_PATH}/wc-config.yaml global : baseDomain : \"set-me\" # set to $CK8S_ENVIRONMENT_NAME.$DOMAIN opsDomain : \"set-me\" # set to ops.$CK8S_ENVIRONMENT_NAME.$DOMAIN issuer : letsencrypt-prod objectStorage : type : \"s3\" s3 : region : \"set-me\" # Region for S3 buckets, e.g, west-1 regionEndpoint : \"set-me\" # e.g., https://s3.us-west-1.amazonaws.com storageClasses : default : rook-ceph-block nfs : enabled : false cinder : enabled : false local : enabled : false ebs : enabled : false # ${CK8S_CONFIG_PATH}/sc-config.yaml (in addition to the changes above) ingressNginx : controller : service : type : \"this-is-not-used\" annotations : \"this-is-not-used\" harbor : oidc : groupClaimName : \"set-me\" # set to group claim name used by OIDC provider issuers : letsencrypt : prod : email : \"set-me\" # set this to an email to receive LetsEncrypt notifications staging : email : \"set-me\" # set this to an email to receive LetsEncrypt notifications # ${CK8S_CONFIG_PATH}/secrets.yaml objectStorage : s3 : accessKey : \"set-me\" # set to your s3 accesskey secretKey : \"set-me\" # set to your s3 secretKey","title":"Configure the apps"},{"location":"operator-manual/exoscale/#create-s3-buckets","text":"You can use the following script to create required S3 buckets. The script uses s3cmd in the background and gets configuration and credentials for your S3 provider from ${HOME}/.s3cfg file. # Use your default s3cmd config file: ${HOME}/.s3cfg scripts/S3/entry.sh create Important You should not use your own credentials for S3. Rather create a new set of credentials with write-only access, when supported by the object storage provider ( check a feature matrix ).","title":"Create S3 buckets"},{"location":"operator-manual/exoscale/#test-s3","text":"To ensure that you have configured S3 correctly, run the following snippet: ( access_key = $( sops exec-file ${ CK8S_CONFIG_PATH } /secrets.yaml 'yq r {} \"objectStorage.s3.accessKey\"' ) secret_key = $( sops exec-file ${ CK8S_CONFIG_PATH } /secrets.yaml 'yq r {} \"objectStorage.s3.secretKey\"' ) region = $( yq r ${ CK8S_CONFIG_PATH } /sc-config.yaml 'objectStorage.s3.region' ) host = $( yq r ${ CK8S_CONFIG_PATH } /sc-config.yaml 'objectStorage.s3.regionEndpoint' ) for bucket in $( yq r ${ CK8S_CONFIG_PATH } /sc-config.yaml 'objectStorage.buckets.*' ) ; do s3cmd --access_key = ${ access_key } --secret_key = ${ secret_key } \\ --region = ${ region } --host = ${ host } \\ ls s3:// ${ bucket } > /dev/null [ ${ ? } = 0 ] && echo \"Bucket ${ bucket } exists!\" done )","title":"Test S3"},{"location":"operator-manual/exoscale/#install-compliant-kubernetes-apps","text":"Start with the service cluster: ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ SERVICE_CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_sc.yaml ./bin/ck8s apply sc # Respond \"n\" if you get a WARN Then the workload clusters: for CLUSTER in \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_wc.yaml ./bin/ck8s apply wc # Respond \"n\" if you get a WARN done","title":"Install Compliant Kubernetes apps"},{"location":"operator-manual/exoscale/#settling","text":"Important Leave sufficient time for the system to settle, e.g., request TLS certificates from LetsEncrypt, perhaps as much as 20 minutes. You can check if the system settled as follows: for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get --all-namespaces pods' done Check the output of the command above. All Pods needs to be Running or Completed. for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get --all-namespaces issuers,clusterissuers,certificates' done Check the output of the command above. All resources need to have the Ready column True.","title":"Settling"},{"location":"operator-manual/exoscale/#testing","text":"After completing the installation step you can test if the apps are properly installed and ready using the commands below. Start with the service cluster: ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ SERVICE_CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_sc.yaml ./bin/ck8s test sc # Respond \"n\" if you get a WARN Then the workload clusters: for CLUSTER in \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_wc.yaml ./bin/ck8s test wc # Respond \"n\" if you get a WARN done Done. Navigate to the endpoints, for example grafana.$BASE_DOMAIN , kibana.$BASE_DOMAIN , harbor.$BASE_DOMAIN , etc. to discover Compliant Kubernetes's features.","title":"Testing"},{"location":"operator-manual/exoscale/#teardown","text":"","title":"Teardown"},{"location":"operator-manual/exoscale/#removing-compliant-kubernetes-apps-from-your-cluster","text":"To remove the applications added by compliant kubernetes you can use the two scripts clean-sc.sh and clean-wc.sh , they are located here in the scripts folder . They perform the following actions: Delete the added helm charts Delete the added namespaces Delete any remaining PersistentVolumes Delete the added CustomResourceDefinitions Note: if user namespaces are managed by Compliant Kubernetes apps then they will also be deleted if you clean up the workload cluster.","title":"Removing Compliant Kubernetes Apps from your cluster"},{"location":"operator-manual/exoscale/#remove-infrastructure","text":"To teardown the infrastructure, please switch to the root directory of the exoscale branch of the Kubespray repo (see the Terraform section). for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do pushd contrib/terraform/exoscale terraform init terraform destroy \\ -var-file = ../../../inventory/ $CLUSTER /default.tfvars \\ -state = ../../../inventory/ $CLUSTER /tfstate- $CLUSTER .tfstate popd done # Remove DNS records exo dns remove $DOMAIN *.ops. $CK8S_ENVIRONMENT_NAME exo dns remove $DOMAIN *. $CK8S_ENVIRONMENT_NAME","title":"Remove infrastructure"},{"location":"operator-manual/exoscale/#further-reading","text":"Elastisys Compliant Kubernetes Kubespray Kubernetes on Exoscale with Terraform Compliant Kubernetes apps repo Configurations option","title":"Further Reading"},{"location":"operator-manual/gcp/","text":"Compliant Kubernetes Deployment on GCP This document contains instructions on how to set up a Compliant Kubernetes environment (consisting of a service cluster and one or more workload clusters) on GCP. The document is split into two parts: Cluster setup (Setting up infrastructure and the Kubernetes clusters) Apps setup (including information about limitations) Before starting, make sure you have all necessary tools . In addition to these general tools, you will also need: A GCP project A JSON keyfile for running Terraform . SSH key that you will use to access GCP, which you have added to the metadata in your GCP project . (Optional) Another JSON keyfile for the GCP Persistent Disk CSI Driver . It is possible (but not recommended) to reuse the same JSON keyfile as you use for Terraform. Note This guide is written for compliantkubernetes-apps v0.13.0 Initial setup Choose names for your service cluster and workload cluster(s): SERVICE_CLUSTER = \"testsc\" WORKLOAD_CLUSTERS =( \"testwc0\" ) Cluster setup Clone the compliantkubernetes-kubespray repository: git clone --recursive https://github.com/elastisys/compliantkubernetes-kubespray cd compliantkubernetes-kubespray For all commands in the cluster setup part of this guide, your working directory is assumed to be the root directory of this repository. In config/gcp/group_vars/all/ck8s-gcp.yml , set the value of gcp_pd_csi_sa_cred_file to the path of your JSON keyfile for GCP Persistent Disk CSI Driver. Modify kubespray/contrib/terraform/gcp/tfvars.json in the following way: Set gcp_project_id to the ID of your GCP project. Set keyfile_location to the location of your JSON keyfile. This will be used as credentials for accessing the GCP API when running Terraform. Set ssh_pub_key to the path of your public SSH key. In ssh_whitelist , api_server_whitelist and nodeport_whitelist , add IP address(es) that you want to be able to access the cluster. Set up the nodes by performing the following steps: Make copies of the Terraform variables, one for each cluster: pushd kubespray/contrib/terraform/gcp for CLUSTER in ${ SERVICE_CLUSTER } ${ WORKLOAD_CLUSTERS [@] } ; do cp tfvars.json $CLUSTER -tfvars.json done popd Set up the nodes with Terraform. If desired, first modify \"machines\" in kubespray/contrib/terraform/gcp/$CLUSTER-tfvars.json to add/remove nodes, change node sizes, etc. (For setting up compliantkubernetes-apps in the service cluster, one n1-standard-8 worker and one n1-standard-4 worker is enough.) pushd kubespray/contrib/terraform/gcp for CLUSTER in ${ SERVICE_CLUSTER } ${ WORKLOAD_CLUSTERS [@] } ; do terraform init terraform apply -var-file $CLUSTER -tfvars.json -auto-approve -state $CLUSTER .tfstate -var prefix = $CLUSTER done popd Save the outputs from apply . (Alternatively, get them later by running terraform output -state $CLUSTER.tfstate in the folder with the state file) Generate inventory file: pushd kubespray/contrib/terraform/gcp for CLUSTER in ${ SERVICE_CLUSTER } ${ WORKLOAD_CLUSTERS [@] } ; do ./generate-inventory.sh $CLUSTER .tfstate > $CLUSTER -inventory.ini done popd Initialize the config: export CK8S_CONFIG_PATH = ~/.ck8s/<environment-name> ./bin/ck8s-kubespray init $CLUSTER gcp <path to SSH key> [ <SOPS fingerprint> ] path to SSH key should point to your private SSH key. It will be copied into your config path and encrypted with SOPS, the original file left as it were. SOPS fingerprint is the gpg fingerprint that will be used for SOPS encryption. You need to set this or the environment variable CK8S_PGP_FP the first time SOPS is used in your specified config path. Copy the inventory files: pushd kubespray/contrib/terraform/gcp for CLUSTER in ${ SERVICE_CLUSTER } ${ WORKLOAD_CLUSTERS [@] } ; do mv $CLUSTER -inventory.ini $CK8S_CONFIG_PATH / $CLUSTER -config/inventory.ini done popd Run kubespray to set up the kubernetes cluster: ./bin/ck8s-kubespray apply $CLUSTER In your config path, open .state/kube_config_$CLUSTER.yaml with SOPS and change clusters.cluster.server to the control_plane_lb_ip_address you got from terraform apply . Apps setup The following instructions were made for release v0.13.0 of compliantkubernetes-apps. There may be discrepancies with newer versions. Limitations Note that there are a few limitations when using compliantkubernetes-apps on GCP at the moment, due to lack of support for certain features: Backup retention for InfluxDB is disabled due to it only being supported with S3 as object storage. If you want to circumvent this, consider using S3 as object storage or deploying Minio as a gateway. Fluentd does not work due to a missing output plugin. If you want to circumvent this, consider using S3 as object storage or deploying Minio as a gateway. Alternatively, Fluentd can be disabled in the compliantkubernetes-apps configuration, which has the consequence of no logs being saved from the service cluster. For information on how to modify the configuration to use S3 as object storage, refer to the administrator manual for AWS or Exoscale , in the section for apps configuration. Setup Set up your DNS entries on a provider of your choice, using the ingress_controller_lb_ip_address from terraform apply as your loadbalancer IPs. You need the following entries: *.ops.<environment_name>. $DOMAIN A <service_cluster_lb_ip> grafana.<environment_name>. $DOMAIN A <service_cluster_lb_ip> harbor.<environment_name>. $DOMAIN A <service_cluster_lb_ip> kibana.<environment_name>. $DOMAIN A <service_cluster_lb_ip> dex.<environment_name>. $DOMAIN A <service_cluster_lb_ip> notary.harbor.<environment_name>. $DOMAIN A <service_cluster_lb_ip> Optionally, if alertmanager is enabled in the workload cluster, create the following DNS record: *.<environment_name>. $DOMAIN A <workload_cluster_lb_ip> In compliantkubernetes-apps , run: export CK8S_ENVIRONMENT_NAME = <environment-name> export CK8S_CLOUD_PROVIDER = baremetal ./bin/ck8s init You will need to modify secrets.yaml , sc-config.yaml and wc-config.yaml in your config path. secrets.yaml Uncomment objectStorage.gcs.keyfileData and paste the contents of your JSON keyfile as the value. sc-config.yaml AND wc-config.yaml Set global.baseDomain to <environment-name>.<dns-domain> and global.opsDomain to ops.<environment-name>.<dns-domain> . Set global.issuer to letsencrypt-prod . Set storageClasses.default to csi-gce-pd . Also set all storageClasses.*.enabled to false . Set objectStorage.type to \"gcs\" . Uncomment objectStorage.gcs.project and set it to the name of your GCP project. sc-config.yaml Set influxDB.backupRetention.enabled to false . Set ingressNginx.controller.service.type to this-is-not-used Set ingressNginx.controller.service.annotations to this-is-not-used Set harbor.oidc.groupClaimName to set-me Set issuers.letsencrypt.prod.email and issuers.letsencrypt.staging.email to email addresses of choice. Create buckets for storage on GCP (found under \"Storage\"). The names must match the bucket names found in your sc-config.yaml and wc-config.yaml in the config path. Set the default storageclass by running the following command: bin/ck8s ops kubectl sc \"patch storageclass csi-gce-pd -p '{\\\"metadata\\\": {\\\"annotations\\\":{\\\"storageclass.kubernetes.io/is-default-class\\\":\\\"true\\\"}}}'\" bin/ck8s ops kubectl wc \"patch storageclass csi-gce-pd -p '{\\\"metadata\\\": {\\\"annotations\\\":{\\\"storageclass.kubernetes.io/is-default-class\\\":\\\"true\\\"}}}'\" Apply the apps: bin/ck8s apply sc bin/ck8s apply wc Done. You should now have a functioning Compliant Kubernetes environment.","title":"On GCP"},{"location":"operator-manual/gcp/#compliant-kubernetes-deployment-on-gcp","text":"This document contains instructions on how to set up a Compliant Kubernetes environment (consisting of a service cluster and one or more workload clusters) on GCP. The document is split into two parts: Cluster setup (Setting up infrastructure and the Kubernetes clusters) Apps setup (including information about limitations) Before starting, make sure you have all necessary tools . In addition to these general tools, you will also need: A GCP project A JSON keyfile for running Terraform . SSH key that you will use to access GCP, which you have added to the metadata in your GCP project . (Optional) Another JSON keyfile for the GCP Persistent Disk CSI Driver . It is possible (but not recommended) to reuse the same JSON keyfile as you use for Terraform. Note This guide is written for compliantkubernetes-apps v0.13.0","title":"Compliant Kubernetes Deployment on GCP"},{"location":"operator-manual/gcp/#initial-setup","text":"Choose names for your service cluster and workload cluster(s): SERVICE_CLUSTER = \"testsc\" WORKLOAD_CLUSTERS =( \"testwc0\" )","title":"Initial setup"},{"location":"operator-manual/gcp/#cluster-setup","text":"Clone the compliantkubernetes-kubespray repository: git clone --recursive https://github.com/elastisys/compliantkubernetes-kubespray cd compliantkubernetes-kubespray For all commands in the cluster setup part of this guide, your working directory is assumed to be the root directory of this repository. In config/gcp/group_vars/all/ck8s-gcp.yml , set the value of gcp_pd_csi_sa_cred_file to the path of your JSON keyfile for GCP Persistent Disk CSI Driver. Modify kubespray/contrib/terraform/gcp/tfvars.json in the following way: Set gcp_project_id to the ID of your GCP project. Set keyfile_location to the location of your JSON keyfile. This will be used as credentials for accessing the GCP API when running Terraform. Set ssh_pub_key to the path of your public SSH key. In ssh_whitelist , api_server_whitelist and nodeport_whitelist , add IP address(es) that you want to be able to access the cluster. Set up the nodes by performing the following steps: Make copies of the Terraform variables, one for each cluster: pushd kubespray/contrib/terraform/gcp for CLUSTER in ${ SERVICE_CLUSTER } ${ WORKLOAD_CLUSTERS [@] } ; do cp tfvars.json $CLUSTER -tfvars.json done popd Set up the nodes with Terraform. If desired, first modify \"machines\" in kubespray/contrib/terraform/gcp/$CLUSTER-tfvars.json to add/remove nodes, change node sizes, etc. (For setting up compliantkubernetes-apps in the service cluster, one n1-standard-8 worker and one n1-standard-4 worker is enough.) pushd kubespray/contrib/terraform/gcp for CLUSTER in ${ SERVICE_CLUSTER } ${ WORKLOAD_CLUSTERS [@] } ; do terraform init terraform apply -var-file $CLUSTER -tfvars.json -auto-approve -state $CLUSTER .tfstate -var prefix = $CLUSTER done popd Save the outputs from apply . (Alternatively, get them later by running terraform output -state $CLUSTER.tfstate in the folder with the state file) Generate inventory file: pushd kubespray/contrib/terraform/gcp for CLUSTER in ${ SERVICE_CLUSTER } ${ WORKLOAD_CLUSTERS [@] } ; do ./generate-inventory.sh $CLUSTER .tfstate > $CLUSTER -inventory.ini done popd Initialize the config: export CK8S_CONFIG_PATH = ~/.ck8s/<environment-name> ./bin/ck8s-kubespray init $CLUSTER gcp <path to SSH key> [ <SOPS fingerprint> ] path to SSH key should point to your private SSH key. It will be copied into your config path and encrypted with SOPS, the original file left as it were. SOPS fingerprint is the gpg fingerprint that will be used for SOPS encryption. You need to set this or the environment variable CK8S_PGP_FP the first time SOPS is used in your specified config path. Copy the inventory files: pushd kubespray/contrib/terraform/gcp for CLUSTER in ${ SERVICE_CLUSTER } ${ WORKLOAD_CLUSTERS [@] } ; do mv $CLUSTER -inventory.ini $CK8S_CONFIG_PATH / $CLUSTER -config/inventory.ini done popd Run kubespray to set up the kubernetes cluster: ./bin/ck8s-kubespray apply $CLUSTER In your config path, open .state/kube_config_$CLUSTER.yaml with SOPS and change clusters.cluster.server to the control_plane_lb_ip_address you got from terraform apply .","title":"Cluster setup"},{"location":"operator-manual/gcp/#apps-setup","text":"The following instructions were made for release v0.13.0 of compliantkubernetes-apps. There may be discrepancies with newer versions.","title":"Apps setup"},{"location":"operator-manual/gcp/#limitations","text":"Note that there are a few limitations when using compliantkubernetes-apps on GCP at the moment, due to lack of support for certain features: Backup retention for InfluxDB is disabled due to it only being supported with S3 as object storage. If you want to circumvent this, consider using S3 as object storage or deploying Minio as a gateway. Fluentd does not work due to a missing output plugin. If you want to circumvent this, consider using S3 as object storage or deploying Minio as a gateway. Alternatively, Fluentd can be disabled in the compliantkubernetes-apps configuration, which has the consequence of no logs being saved from the service cluster. For information on how to modify the configuration to use S3 as object storage, refer to the administrator manual for AWS or Exoscale , in the section for apps configuration.","title":"Limitations"},{"location":"operator-manual/gcp/#setup","text":"Set up your DNS entries on a provider of your choice, using the ingress_controller_lb_ip_address from terraform apply as your loadbalancer IPs. You need the following entries: *.ops.<environment_name>. $DOMAIN A <service_cluster_lb_ip> grafana.<environment_name>. $DOMAIN A <service_cluster_lb_ip> harbor.<environment_name>. $DOMAIN A <service_cluster_lb_ip> kibana.<environment_name>. $DOMAIN A <service_cluster_lb_ip> dex.<environment_name>. $DOMAIN A <service_cluster_lb_ip> notary.harbor.<environment_name>. $DOMAIN A <service_cluster_lb_ip> Optionally, if alertmanager is enabled in the workload cluster, create the following DNS record: *.<environment_name>. $DOMAIN A <workload_cluster_lb_ip> In compliantkubernetes-apps , run: export CK8S_ENVIRONMENT_NAME = <environment-name> export CK8S_CLOUD_PROVIDER = baremetal ./bin/ck8s init You will need to modify secrets.yaml , sc-config.yaml and wc-config.yaml in your config path. secrets.yaml Uncomment objectStorage.gcs.keyfileData and paste the contents of your JSON keyfile as the value. sc-config.yaml AND wc-config.yaml Set global.baseDomain to <environment-name>.<dns-domain> and global.opsDomain to ops.<environment-name>.<dns-domain> . Set global.issuer to letsencrypt-prod . Set storageClasses.default to csi-gce-pd . Also set all storageClasses.*.enabled to false . Set objectStorage.type to \"gcs\" . Uncomment objectStorage.gcs.project and set it to the name of your GCP project. sc-config.yaml Set influxDB.backupRetention.enabled to false . Set ingressNginx.controller.service.type to this-is-not-used Set ingressNginx.controller.service.annotations to this-is-not-used Set harbor.oidc.groupClaimName to set-me Set issuers.letsencrypt.prod.email and issuers.letsencrypt.staging.email to email addresses of choice. Create buckets for storage on GCP (found under \"Storage\"). The names must match the bucket names found in your sc-config.yaml and wc-config.yaml in the config path. Set the default storageclass by running the following command: bin/ck8s ops kubectl sc \"patch storageclass csi-gce-pd -p '{\\\"metadata\\\": {\\\"annotations\\\":{\\\"storageclass.kubernetes.io/is-default-class\\\":\\\"true\\\"}}}'\" bin/ck8s ops kubectl wc \"patch storageclass csi-gce-pd -p '{\\\"metadata\\\": {\\\"annotations\\\":{\\\"storageclass.kubernetes.io/is-default-class\\\":\\\"true\\\"}}}'\" Apply the apps: bin/ck8s apply sc bin/ck8s apply wc Done. You should now have a functioning Compliant Kubernetes environment.","title":"Setup"},{"location":"operator-manual/getting-started/","text":"Getting Started Setting up Compliant Kubernetes consists of two parts: setting up at least two vanilla Kubernetes clusters and deploying compliantkubernetes-apps on top of them. Pre-requisites for Creating Vanilla Kubernetes clusters In theory, any vanilla Kubernetes cluster can be used for Compliant Kubernetes. We suggest the kubespray way. To this end, you need: Git Python3 pip Terraform Ansible Ansible is best installed as follows: git clone --recursive https://github.com/elastisys/compliantkubernetes-kubespray cd compliantkubernetes-kubespray pip3 install -r kubespray/requirements.txt Optional: For debugging, you may want CLI tools to interact with your chosen cloud provider: AWS CLI Exoscale CLI OpenStack Client VMware vSphere CLI (govmomi) Pre-requisites for compliantkubernetes-apps Using Ansible, these can be retrieved as follows: git clone https://github.com/elastisys/compliantkubernetes-apps cd compliantkubernetes-apps ansible-playbook -e 'ansible_python_interpreter=/usr/bin/python3' --ask-become-pass --connection local --inventory 127 .0.0.1, get-requirements.yaml Misc Compliant Kubernetes relies on SSH for accessing nodes. If you haven't already done so, generate an SSH key as follows: ssh-keygen Configuration secrets in Compliant Kubernetes are encrypted using SOPS . We currently only support using PGP when encrypting secrets. If you haven't already done so, generate your own PGP key as follows: gpg --full-generate-key","title":"Getting Started"},{"location":"operator-manual/getting-started/#getting-started","text":"Setting up Compliant Kubernetes consists of two parts: setting up at least two vanilla Kubernetes clusters and deploying compliantkubernetes-apps on top of them.","title":"Getting Started"},{"location":"operator-manual/getting-started/#pre-requisites-for-creating-vanilla-kubernetes-clusters","text":"In theory, any vanilla Kubernetes cluster can be used for Compliant Kubernetes. We suggest the kubespray way. To this end, you need: Git Python3 pip Terraform Ansible Ansible is best installed as follows: git clone --recursive https://github.com/elastisys/compliantkubernetes-kubespray cd compliantkubernetes-kubespray pip3 install -r kubespray/requirements.txt Optional: For debugging, you may want CLI tools to interact with your chosen cloud provider: AWS CLI Exoscale CLI OpenStack Client VMware vSphere CLI (govmomi)","title":"Pre-requisites for Creating Vanilla Kubernetes clusters"},{"location":"operator-manual/getting-started/#pre-requisites-for-compliantkubernetes-apps","text":"Using Ansible, these can be retrieved as follows: git clone https://github.com/elastisys/compliantkubernetes-apps cd compliantkubernetes-apps ansible-playbook -e 'ansible_python_interpreter=/usr/bin/python3' --ask-become-pass --connection local --inventory 127 .0.0.1, get-requirements.yaml","title":"Pre-requisites for compliantkubernetes-apps"},{"location":"operator-manual/getting-started/#misc","text":"Compliant Kubernetes relies on SSH for accessing nodes. If you haven't already done so, generate an SSH key as follows: ssh-keygen Configuration secrets in Compliant Kubernetes are encrypted using SOPS . We currently only support using PGP when encrypting secrets. If you haven't already done so, generate your own PGP key as follows: gpg --full-generate-key","title":"Misc"},{"location":"operator-manual/ingress/","text":"Ingress Compliant Kubernetes (CK8s) uses the Nginx Ingress controller to route external traffic to the correct Service inside the cluster. CK8s can configure the Ingress controller in two different ways depending on the underlying infrastructure. Using a Service of type LoadBalancer When using a cloud provider with a Kubernetes cloud integration such as AWS, Azure and Google cloud the Ingress controller can be exposed with a Service of type LoadBalancer. This will create an external load balancer in the cloud provider with an external ip-address. Any dns records should be pointed to the ip-address of the load balancer. Note This is only currently supported in CK8s for AWS. It is however possible to configure this for Azure and Google cloud as well but it's not done by default Using the host network For any cloud provider (or bare metal) not supporting these kind of public load balancers the Ingress controller uses the host network instead. This is done by configuring the Ingress controller as a DaemonSet so one Pod is created on each node. The Pods are configured to use the host network, so all traffic received on the node on port 80 and 443 will be intercepted by the Ingress controller Pod and then routed to the desired Service. On some clouds providers there is load balancing available for the worker nodes. For example Exoscale uses an \"elastic ip\" which provides one external ip which load balances to the available worker nodes. For these cloud providers this external ip of the load balancers should be used as the entry point in the dns. For the cloud providers where this is not available the easiest option is to just point the dns to the ip of any, or all, of the worker nodes. This is of course not a optimal solution because it adds a single point of failure on the worker node which is selected by the dns. Another option is to use any existing load balancer service if this is available. Installation The Nginx ingress is currently configured and installed by the compliantkubernetes-apps repository. The configuration is set in sc-config.yaml and wc-config.yaml under: ingressNginx : useHostPort : \"\" service : enabled : \"\" type : \"\" If the apps repository is initiated with the correct cloud provider these config options will get the correct defaults. For more ways to install the Nginx Ingress controller see https://kubernetes.github.io/ingress-nginx/deploy Ingress resource The Ingress resource is used to later route traffic to the desired Service. For more information about this see the official documentation .","title":"Ingress"},{"location":"operator-manual/ingress/#ingress","text":"Compliant Kubernetes (CK8s) uses the Nginx Ingress controller to route external traffic to the correct Service inside the cluster. CK8s can configure the Ingress controller in two different ways depending on the underlying infrastructure.","title":"Ingress"},{"location":"operator-manual/ingress/#using-a-service-of-type-loadbalancer","text":"When using a cloud provider with a Kubernetes cloud integration such as AWS, Azure and Google cloud the Ingress controller can be exposed with a Service of type LoadBalancer. This will create an external load balancer in the cloud provider with an external ip-address. Any dns records should be pointed to the ip-address of the load balancer. Note This is only currently supported in CK8s for AWS. It is however possible to configure this for Azure and Google cloud as well but it's not done by default","title":"Using a Service of type LoadBalancer"},{"location":"operator-manual/ingress/#using-the-host-network","text":"For any cloud provider (or bare metal) not supporting these kind of public load balancers the Ingress controller uses the host network instead. This is done by configuring the Ingress controller as a DaemonSet so one Pod is created on each node. The Pods are configured to use the host network, so all traffic received on the node on port 80 and 443 will be intercepted by the Ingress controller Pod and then routed to the desired Service. On some clouds providers there is load balancing available for the worker nodes. For example Exoscale uses an \"elastic ip\" which provides one external ip which load balances to the available worker nodes. For these cloud providers this external ip of the load balancers should be used as the entry point in the dns. For the cloud providers where this is not available the easiest option is to just point the dns to the ip of any, or all, of the worker nodes. This is of course not a optimal solution because it adds a single point of failure on the worker node which is selected by the dns. Another option is to use any existing load balancer service if this is available.","title":"Using the host network"},{"location":"operator-manual/ingress/#installation","text":"The Nginx ingress is currently configured and installed by the compliantkubernetes-apps repository. The configuration is set in sc-config.yaml and wc-config.yaml under: ingressNginx : useHostPort : \"\" service : enabled : \"\" type : \"\" If the apps repository is initiated with the correct cloud provider these config options will get the correct defaults. For more ways to install the Nginx Ingress controller see https://kubernetes.github.io/ingress-nginx/deploy","title":"Installation"},{"location":"operator-manual/ingress/#ingress-resource","text":"The Ingress resource is used to later route traffic to the desired Service. For more information about this see the official documentation .","title":"Ingress resource"},{"location":"operator-manual/openstack/","text":"Compliant Kubernetes on Openstack This document contains instructions on how to set up a Compliant Kubernetes environment (consisting of a service cluster and one or more workload clusters) on Openstack. Note This guide is written for compliantkubernetes-apps v0.13.0 TODO: The document is split into two parts: Cluster setup (setting up infrastructure and the Kubernetes clusters). We will be using the Terraform module for Openstack that can be found in the Kubespray repository . Please refer to it if you need more details about this part of the setup. Apps setup (including information about limitations) Before starting, make sure you have all necessary tools . In addition to these general tools, you will also need: Openstack credentials (either using openrc or the clouds.yaml configuration file) for setting up the infrastructure. Note Although recommended OpenStack authentication method is clouds.yaml it is more convenient to use the openrc method with compliant kubernetes as it works both with kubespray and terraform. If you are using the clouds.yaml method, at the moment, kubespray will still expect you to set a few environment variables. Initialize configuration folder Choose names for your service cluster and workload cluster(s): SERVICE_CLUSTER = \"testsc\" WORKLOAD_CLUSTERS =( \"testwc0\" \"testwc1\" ) Start by initializing a Compliant Kubernetes environment using Compliant Kubernetes Kubespray. All of this is done from the root of the compliantkubernetes-kubespray repository. export CK8S_CONFIG_PATH = ~/.ck8s/<environment-name> export SOPS_FP = <PGP-fingerprint> for CLUSTER in \" ${ SERVICE_CLUSTER } \" \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ./bin/ck8s-kubespray init \" ${ CLUSTER } \" openstack \" ${ SOPS_FP } \" done Infrastructure setup using Terraform Configure Terraform by creating a cluster.tfvars file for each cluster. The available options can be seen in kubespray/contrib/terraform/openstack/variables.tf . There is a sample file that can be copied to get something to start from. for CLUSTER in ${ SERVICE_CLUSTER } ${ WORKLOAD_CLUSTERS [@] } ; do cp kubespray/contrib/terraform/openstack/sample-inventory/cluster.tfvars \" ${ CK8S_CONFIG_PATH } / ${ CLUSTER } -config/cluster.tfvars\" done Note You really must edit the values in these files. There is no way to set sane defaults for what flavor to use, what availability zones or networks are available across providers. In the section below some guidance and samples are provided but remember that they might be useless to you depending on your needs and setup. Infrastructure guidance We recommend you to have at least three worker nodes with 4 cores and 8 GB memory each, and we recommend you to have at least 2 cores and 4 GB for your control plane nodes. Below is example cluster.tfvars for a few select openstack providers. The examples are copy-pastable, but you might want to change cluster_name and network_name (if neutron is used!). Citycloud Kna1 # your Kubernetes cluster name here cluster_name = \"your-cluster-name\" # image to use for bastion, masters, standalone etcd instances, and nodes image = \"Ubuntu 20.04 Focal Fossa 20200423\" # 0|1 bastion nodes number_of_bastions = 0 # standalone etcds number_of_etcd = 0 # masters number_of_k8s_masters = 1 number_of_k8s_masters_no_etcd = 0 number_of_k8s_masters_no_floating_ip = 0 number_of_k8s_masters_no_floating_ip_no_etcd = 0 flavor_k8s_master = \"96c7903e-32f0-421d-b6a2-a45c97b15665\" # nodes number_of_k8s_nodes = 3 number_of_k8s_nodes_no_floating_ip = 0 flavor_k8s_node = \"572a3b2e-6329-4053-b872-aecb1e70d8a6\" # networking # ssh access to nodes k8s_allowed_remote_ips = [\"0.0.0.0/0\"] worker_allowed_ports = [ { # Node ports \"protocol\" = \"tcp\" \"port_range_min\" = 30000 \"port_range_max\" = 32767 \"remote_ip_prefix\" = \"0.0.0.0/0\" }, { # HTTP \"protocol\" = \"tcp\" \"port_range_min\" = 80 \"port_range_max\" = 80 \"remote_ip_prefix\" = \"0.0.0.0/0\" }, { # HTTPS \"protocol\" = \"tcp\" \"port_range_min\" = 443 \"port_range_max\" = 443 \"remote_ip_prefix\" = \"0.0.0.0/0\" } ] network_name = \"name-of-your-network\" external_net = \"fba95253-5543-4078-b793-e2de58c31378\" floatingip_pool = \"ext-net\" use_access_ip = 0 use_server_groups = true Safespring sto1 # your Kubernetes cluster name here cluster_name = \"your-cluster-name\" # image to use for bastion, masters, standalone etcd instances, and nodes image = \"ubuntu-20.04\" # 0|1 bastion nodes number_of_bastions = 0 use_neutron = 0 # standalone etcds number_of_etcd = 0 # masters number_of_k8s_masters = 0 number_of_k8s_masters_no_etcd = 0 number_of_k8s_masters_no_floating_ip = 1 number_of_k8s_masters_no_floating_ip_no_etcd = 0 flavor_k8s_master = \"8a707999-0bce-4f2f-8243-b4253ba7c473\" # nodes number_of_k8s_nodes = 0 number_of_k8s_nodes_no_floating_ip = 3 flavor_k8s_node = \"5b40af67-9d11-45ed-a44f-e876766160a5\" # networking # ssh access to nodes k8s_allowed_remote_ips = [\"0.0.0.0/0\"] worker_allowed_ports = [ { # Node ports \"protocol\" = \"tcp\" \"port_range_min\" = 30000 \"port_range_max\" = 32767 \"remote_ip_prefix\" = \"0.0.0.0/0\" }, { # HTTP \"protocol\" = \"tcp\" \"port_range_min\" = 80 \"port_range_max\" = 80 \"remote_ip_prefix\" = \"0.0.0.0/0\" }, { # HTTPS \"protocol\" = \"tcp\" \"port_range_min\" = 443 \"port_range_max\" = 443 \"remote_ip_prefix\" = \"0.0.0.0/0\" } ] external_net = \"b19680b3-c00e-40f0-ad77-4448e81ae226\" use_access_ip = 1 network_name = \"public\" use_server_groups = true Expose Openstack credentials to Terraform Terraform will need access to Openstack credentials in order to create the infrastructure. More details can be found here . We will be using the declarative option with the clouds.yaml file. Since this file can contain credentials for multiple environments, we specify the name of the one we want to use in the environment variable OS_CLOUD : export OS_CLOUD = <name-of-openstack-cloud-environment> Initialize and apply Terraform MODULE_PATH = \" $( pwd ) /kubespray/contrib/terraform/openstack\" pushd \" ${ MODULE_PATH } \" for CLUSTER in \" ${ SERVICE_CLUSTER } \" \" ${ WORKLOAD_CLUSTERS [@] } \" ; do terraform init terraform apply -var-file = \" ${ CK8S_CONFIG_PATH } / ${ CLUSTER } -config/cluster.tfvars\" -state = \" ${ CK8S_CONFIG_PATH } / ${ CLUSTER } -config/terraform.tfstate\" done popd Warning The above will not work well if you are using a bastion host. This is due to some hard coded paths . To work around it, you may link the kubespray/contrib folder to the correct relative path, or make sure your CK8S_CONFIG_PATH is already at a proper place relative to the same. Install Kubernetes using Kubespray Before we can run Kubespray, we will need to go through the relevant variables. Additionally we will need to expose some credentials so that Kubespray can set up cloud provider integration. You will need to change at least one value: kube_oidc_url in group_vars/k8s_cluster/ck8s-k8s_cluster.yaml , normally this should be set to https://dex.BASE_DOMAIN . Note If you have use_access_ip = 0 in cluster.tfvars , you should add the public ip address of the master nodes to the variable supplementary_addresses_in_ssl_keys = [\"<master-0-ip-address>\",...] somewhere under group_vars/ . For cloud provider integration, you have a few options as described here . We will be going with the external cloud provider and simply source the Openstack credentials. See below for how to modify the variables that need to be modified. Setting up Kubespray variables In ${CK8S_CONFIG_PATH}/$CLUSTER-config/group_vars/k8s_cluster/ck8s-k8s-cluster-openstack.yaml , the default variables should look like this: etcd_kubeadm_enabled : true cloud_provider : external external_cloud_provider : openstack calico_mtu : 1480 external_openstack_cloud_controller_extra_args : # Must be different for every cluster in the same openstack project cluster-name : \"set-me\" cinder_csi_enabled : true persistent_volumes_enabled : true expand_persistent_volumes : true openstack_blockstorage_ignore_volume_az : true storage_classes : - name : cinder-csi is_default : true parameters : allowVolumeExpansion : true availability : nova cluster-name should be set to a name that is unique in the Openstack project you're deploying your clusters in. If you don't have any other clusters in the project, just make sure that the service cluster and workload clusters have different names. Cinder CSI is enabled by default along with the configuration options to enable persistent volumes and the expansion of these volumes. It is also set to ignore the volume availability zone to allow volumes to attach to nodes in different or mismatching zones. The default works well with both CityCloud and SafeSpring. If you want to set up LBaaS in your cluster, you can add the following config: external_openstack_lbaas_create_monitor : false external_openstack_lbaas_monitor_delay : \"1m\" external_openstack_lbaas_monitor_timeout : \"30s\" external_openstack_lbaas_monitor_max_retries : \"3\" external_openstack_lbaas_provider : octavia external_openstack_lbaas_use_octavia : true # external_openstack_lbaas_network_id: \"Neutron network ID to create LBaaS VIP\" external_openstack_lbaas_subnet_id : \"Neutron subnet ID to create LBaaS VIP\" external_openstack_lbaas_floating_network_id : \"Neutron network ID to get floating IP from\" # external_openstack_lbaas_floating_subnet_id: \"Neutron subnet ID to get floating IP from\" external_openstack_lbaas_method : \"ROUND_ROBIN\" external_openstack_lbaas_manage_security_groups : false external_openstack_lbaas_internal_lb : false The network_id and subnet_id variables need to be set by you, depending on whether or not you used floating IP. network_id should match the external_net variable in your Terraform variables, whereas the subnet_id should match the subnet ID that Terraform outputs after it is applied. Additionally, when you later set up compliantkubernetes-apps in your cluster, you should set ingressNginx.controller.service.enabled to true and ingressNginx.controller.service.type to LoadBalancer in both your sc-config.yaml and wc-config.yaml . Use the IP of the ingress-nginx-controller service in your cluster when you set up your DNS. Note At this point if the cluster is running on Safespring and you are using kubespray v2.17.0+ it is possible to create an application credential. Which will give the cluster its own set of credentials instead of using your own. To create a set of credentials use the following command: openstack application credential create <name> And set the following environment variables export OS_APPLICATION_CREDENTIAL_NAME: <name> export OS_APPLICATION_CREDENTIAL_ID: <project_id> export OS_APPLICATION_CREDENTIAL_SECRET: <secret> Run Kubespray Copy the script for generating dynamic ansible inventories: for CLUSTER in \" ${ SERVICE_CLUSTER } \" \" ${ WORKLOAD_CLUSTERS [@] } \" ; do cp kubespray/contrib/terraform/terraform.py \" ${ CK8S_CONFIG_PATH } / ${ CLUSTER } -config/inventory.ini\" chmod +x \" ${ CK8S_CONFIG_PATH } / ${ CLUSTER } -config/inventory.ini\" done Now it is time to run the Kubespray playbook! for CLUSTER in \" ${ SERVICE_CLUSTER } \" \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ./bin/ck8s-kubespray apply \" ${ CLUSTER } \" done Test access to the Kubernetes API You should now have an encrypted kubeconfig file for each cluster under $CK8S_CONFIG_PATH/.state . Check that they work like this: for CLUSTER in \" ${ SERVICE_CLUSTER } \" \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file \" ${ CK8S_CONFIG_PATH } /.state/kube_config_ ${ CLUSTER } .yaml\" \"kubectl --kubeconfig {} cluster-info\" done The output should be similar to this. Kubernetes control plane is running at https://<public-ip>:6443 To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. Kubernetes control plane is running at https://<public-ip>:6443 To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. Prepare for Compliant Kubernetes Apps To make the kubeconfig files work with Compliant Kubernetes Apps, you will need to rename or copy them, since Compliant Kubernetes Apps currently only support clusters named sc and wc . If you have multiple workload clusters, you can make this work by setting CK8S_CONFIG_PATH to each $CK8S_CONFIG_PATH/$CLUSTER-config in turn. I.e. CK8S_CONFIG_PATH will be different for compliantkubernetes-kubespray and compliantkubernetes-apps. # In compliantkubernetes-kubespray CK8S_CONFIG_PATH=~/.ck8s/<environment-name> # In compliantkubernetes-apps (one config path per workload cluster) CK8S_CONFIG_PATH=~/.ck8s/<environment-name>/<prefix>-config Copy the kubeconfig files to a path that Apps can find. Option 1 - A single workload cluster: cp \" ${ CK8S_CONFIG_PATH } /.state/kube_config_ ${ SERVICE_CLUSTER } .yaml\" \" ${ CK8S_CONFIG_PATH } /.state/kube_config_sc.yaml\" cp \" ${ CK8S_CONFIG_PATH } /.state/kube_config_ ${ WORKLOAD_CLUSTERS [@] } .yaml\" \" ${ CK8S_CONFIG_PATH } /.state/kube_config_wc.yaml\" You can now use the same CK8S_CONFIG_PATH for Apps as for compliantkubernetes-kubespray. Option 2 - A multiple workload cluster: mkdir -p \" ${ CK8S_CONFIG_PATH } / ${ SERVICE_CLUSTER } -config/.state\" cp \" ${ CK8S_CONFIG_PATH } /.state/kube_config_ ${ SERVICE_CLUSTER } .yaml\" \" ${ CK8S_CONFIG_PATH } / ${ SERVICE_CLUSTER } -config/.state/kube_config_sc.yaml\" for CLUSTER in \" ${ WORKLOAD_CLUSTERS [@] } \" ; do mkdir -p \" ${ CK8S_CONFIG_PATH } / ${ CLUSTER } -config/.state\" cp \" ${ CK8S_CONFIG_PATH } /.state/kube_config_ ${ CLUSTERS } .yaml\" \" ${ CK8S_CONFIG_PATH } / ${ CLUSTER } -config/.state/kube_config_wc.yaml\" done You will then need to set CK8S_CONFIG_PATH to each $CLUSTER-config folder in turn, in order to install Apps on the service cluster and workload clusters. With this you should be ready to install Compliant Kubernetes Apps on top of the clusters. This will be similar to any other cloud provider. Suggested next steps are: Configure DNS using your favorite DNS provider. Create object storage buckets for backups and container registry storage (if desired). Install Compliant Kubernetes Apps! See for example this guide for more details. Cleanup If you installed Compliant Kubernetes Apps, start by cleaning it up . Make sure you remove all PersistentVolumes and Services with type=LoadBalancer . These objects may create cloud resources that are not managed by Terraform, and therefore would not be removed when we destroy the infrastructure. Destroy the infrastructure using Terraform, the same way you created it: MODULE_PATH = \" $( pwd ) /kubespray/contrib/terraform/openstack\" pushd \" ${ MODULE_PATH } \" for CLUSTER in \" ${ SERVICE_CLUSTER } \" \" ${ WORKLOAD_CLUSTERS [@] } \" ; do terraform init terraform destroy -var-file = \" ${ CK8S_CONFIG_PATH } / ${ CLUSTER } -config/cluster.tfvars\" -state = \" ${ CK8S_CONFIG_PATH } / ${ CLUSTER } -config/terraform.tfstate\" done popd Don't forget to remove any DNS records and object storage buckets that you may have created.","title":"On Openstack"},{"location":"operator-manual/openstack/#compliant-kubernetes-on-openstack","text":"This document contains instructions on how to set up a Compliant Kubernetes environment (consisting of a service cluster and one or more workload clusters) on Openstack. Note This guide is written for compliantkubernetes-apps v0.13.0 TODO: The document is split into two parts: Cluster setup (setting up infrastructure and the Kubernetes clusters). We will be using the Terraform module for Openstack that can be found in the Kubespray repository . Please refer to it if you need more details about this part of the setup. Apps setup (including information about limitations) Before starting, make sure you have all necessary tools . In addition to these general tools, you will also need: Openstack credentials (either using openrc or the clouds.yaml configuration file) for setting up the infrastructure. Note Although recommended OpenStack authentication method is clouds.yaml it is more convenient to use the openrc method with compliant kubernetes as it works both with kubespray and terraform. If you are using the clouds.yaml method, at the moment, kubespray will still expect you to set a few environment variables.","title":"Compliant Kubernetes on Openstack"},{"location":"operator-manual/openstack/#initialize-configuration-folder","text":"Choose names for your service cluster and workload cluster(s): SERVICE_CLUSTER = \"testsc\" WORKLOAD_CLUSTERS =( \"testwc0\" \"testwc1\" ) Start by initializing a Compliant Kubernetes environment using Compliant Kubernetes Kubespray. All of this is done from the root of the compliantkubernetes-kubespray repository. export CK8S_CONFIG_PATH = ~/.ck8s/<environment-name> export SOPS_FP = <PGP-fingerprint> for CLUSTER in \" ${ SERVICE_CLUSTER } \" \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ./bin/ck8s-kubespray init \" ${ CLUSTER } \" openstack \" ${ SOPS_FP } \" done","title":"Initialize configuration folder"},{"location":"operator-manual/openstack/#infrastructure-setup-using-terraform","text":"Configure Terraform by creating a cluster.tfvars file for each cluster. The available options can be seen in kubespray/contrib/terraform/openstack/variables.tf . There is a sample file that can be copied to get something to start from. for CLUSTER in ${ SERVICE_CLUSTER } ${ WORKLOAD_CLUSTERS [@] } ; do cp kubespray/contrib/terraform/openstack/sample-inventory/cluster.tfvars \" ${ CK8S_CONFIG_PATH } / ${ CLUSTER } -config/cluster.tfvars\" done Note You really must edit the values in these files. There is no way to set sane defaults for what flavor to use, what availability zones or networks are available across providers. In the section below some guidance and samples are provided but remember that they might be useless to you depending on your needs and setup.","title":"Infrastructure setup using Terraform"},{"location":"operator-manual/openstack/#infrastructure-guidance","text":"We recommend you to have at least three worker nodes with 4 cores and 8 GB memory each, and we recommend you to have at least 2 cores and 4 GB for your control plane nodes. Below is example cluster.tfvars for a few select openstack providers. The examples are copy-pastable, but you might want to change cluster_name and network_name (if neutron is used!). Citycloud Kna1 # your Kubernetes cluster name here cluster_name = \"your-cluster-name\" # image to use for bastion, masters, standalone etcd instances, and nodes image = \"Ubuntu 20.04 Focal Fossa 20200423\" # 0|1 bastion nodes number_of_bastions = 0 # standalone etcds number_of_etcd = 0 # masters number_of_k8s_masters = 1 number_of_k8s_masters_no_etcd = 0 number_of_k8s_masters_no_floating_ip = 0 number_of_k8s_masters_no_floating_ip_no_etcd = 0 flavor_k8s_master = \"96c7903e-32f0-421d-b6a2-a45c97b15665\" # nodes number_of_k8s_nodes = 3 number_of_k8s_nodes_no_floating_ip = 0 flavor_k8s_node = \"572a3b2e-6329-4053-b872-aecb1e70d8a6\" # networking # ssh access to nodes k8s_allowed_remote_ips = [\"0.0.0.0/0\"] worker_allowed_ports = [ { # Node ports \"protocol\" = \"tcp\" \"port_range_min\" = 30000 \"port_range_max\" = 32767 \"remote_ip_prefix\" = \"0.0.0.0/0\" }, { # HTTP \"protocol\" = \"tcp\" \"port_range_min\" = 80 \"port_range_max\" = 80 \"remote_ip_prefix\" = \"0.0.0.0/0\" }, { # HTTPS \"protocol\" = \"tcp\" \"port_range_min\" = 443 \"port_range_max\" = 443 \"remote_ip_prefix\" = \"0.0.0.0/0\" } ] network_name = \"name-of-your-network\" external_net = \"fba95253-5543-4078-b793-e2de58c31378\" floatingip_pool = \"ext-net\" use_access_ip = 0 use_server_groups = true Safespring sto1 # your Kubernetes cluster name here cluster_name = \"your-cluster-name\" # image to use for bastion, masters, standalone etcd instances, and nodes image = \"ubuntu-20.04\" # 0|1 bastion nodes number_of_bastions = 0 use_neutron = 0 # standalone etcds number_of_etcd = 0 # masters number_of_k8s_masters = 0 number_of_k8s_masters_no_etcd = 0 number_of_k8s_masters_no_floating_ip = 1 number_of_k8s_masters_no_floating_ip_no_etcd = 0 flavor_k8s_master = \"8a707999-0bce-4f2f-8243-b4253ba7c473\" # nodes number_of_k8s_nodes = 0 number_of_k8s_nodes_no_floating_ip = 3 flavor_k8s_node = \"5b40af67-9d11-45ed-a44f-e876766160a5\" # networking # ssh access to nodes k8s_allowed_remote_ips = [\"0.0.0.0/0\"] worker_allowed_ports = [ { # Node ports \"protocol\" = \"tcp\" \"port_range_min\" = 30000 \"port_range_max\" = 32767 \"remote_ip_prefix\" = \"0.0.0.0/0\" }, { # HTTP \"protocol\" = \"tcp\" \"port_range_min\" = 80 \"port_range_max\" = 80 \"remote_ip_prefix\" = \"0.0.0.0/0\" }, { # HTTPS \"protocol\" = \"tcp\" \"port_range_min\" = 443 \"port_range_max\" = 443 \"remote_ip_prefix\" = \"0.0.0.0/0\" } ] external_net = \"b19680b3-c00e-40f0-ad77-4448e81ae226\" use_access_ip = 1 network_name = \"public\" use_server_groups = true","title":"Infrastructure guidance"},{"location":"operator-manual/openstack/#expose-openstack-credentials-to-terraform","text":"Terraform will need access to Openstack credentials in order to create the infrastructure. More details can be found here . We will be using the declarative option with the clouds.yaml file. Since this file can contain credentials for multiple environments, we specify the name of the one we want to use in the environment variable OS_CLOUD : export OS_CLOUD = <name-of-openstack-cloud-environment>","title":"Expose Openstack credentials to Terraform"},{"location":"operator-manual/openstack/#initialize-and-apply-terraform","text":"MODULE_PATH = \" $( pwd ) /kubespray/contrib/terraform/openstack\" pushd \" ${ MODULE_PATH } \" for CLUSTER in \" ${ SERVICE_CLUSTER } \" \" ${ WORKLOAD_CLUSTERS [@] } \" ; do terraform init terraform apply -var-file = \" ${ CK8S_CONFIG_PATH } / ${ CLUSTER } -config/cluster.tfvars\" -state = \" ${ CK8S_CONFIG_PATH } / ${ CLUSTER } -config/terraform.tfstate\" done popd Warning The above will not work well if you are using a bastion host. This is due to some hard coded paths . To work around it, you may link the kubespray/contrib folder to the correct relative path, or make sure your CK8S_CONFIG_PATH is already at a proper place relative to the same.","title":"Initialize and apply Terraform"},{"location":"operator-manual/openstack/#install-kubernetes-using-kubespray","text":"Before we can run Kubespray, we will need to go through the relevant variables. Additionally we will need to expose some credentials so that Kubespray can set up cloud provider integration. You will need to change at least one value: kube_oidc_url in group_vars/k8s_cluster/ck8s-k8s_cluster.yaml , normally this should be set to https://dex.BASE_DOMAIN . Note If you have use_access_ip = 0 in cluster.tfvars , you should add the public ip address of the master nodes to the variable supplementary_addresses_in_ssl_keys = [\"<master-0-ip-address>\",...] somewhere under group_vars/ . For cloud provider integration, you have a few options as described here . We will be going with the external cloud provider and simply source the Openstack credentials. See below for how to modify the variables that need to be modified.","title":"Install Kubernetes using Kubespray"},{"location":"operator-manual/openstack/#setting-up-kubespray-variables","text":"In ${CK8S_CONFIG_PATH}/$CLUSTER-config/group_vars/k8s_cluster/ck8s-k8s-cluster-openstack.yaml , the default variables should look like this: etcd_kubeadm_enabled : true cloud_provider : external external_cloud_provider : openstack calico_mtu : 1480 external_openstack_cloud_controller_extra_args : # Must be different for every cluster in the same openstack project cluster-name : \"set-me\" cinder_csi_enabled : true persistent_volumes_enabled : true expand_persistent_volumes : true openstack_blockstorage_ignore_volume_az : true storage_classes : - name : cinder-csi is_default : true parameters : allowVolumeExpansion : true availability : nova cluster-name should be set to a name that is unique in the Openstack project you're deploying your clusters in. If you don't have any other clusters in the project, just make sure that the service cluster and workload clusters have different names. Cinder CSI is enabled by default along with the configuration options to enable persistent volumes and the expansion of these volumes. It is also set to ignore the volume availability zone to allow volumes to attach to nodes in different or mismatching zones. The default works well with both CityCloud and SafeSpring. If you want to set up LBaaS in your cluster, you can add the following config: external_openstack_lbaas_create_monitor : false external_openstack_lbaas_monitor_delay : \"1m\" external_openstack_lbaas_monitor_timeout : \"30s\" external_openstack_lbaas_monitor_max_retries : \"3\" external_openstack_lbaas_provider : octavia external_openstack_lbaas_use_octavia : true # external_openstack_lbaas_network_id: \"Neutron network ID to create LBaaS VIP\" external_openstack_lbaas_subnet_id : \"Neutron subnet ID to create LBaaS VIP\" external_openstack_lbaas_floating_network_id : \"Neutron network ID to get floating IP from\" # external_openstack_lbaas_floating_subnet_id: \"Neutron subnet ID to get floating IP from\" external_openstack_lbaas_method : \"ROUND_ROBIN\" external_openstack_lbaas_manage_security_groups : false external_openstack_lbaas_internal_lb : false The network_id and subnet_id variables need to be set by you, depending on whether or not you used floating IP. network_id should match the external_net variable in your Terraform variables, whereas the subnet_id should match the subnet ID that Terraform outputs after it is applied. Additionally, when you later set up compliantkubernetes-apps in your cluster, you should set ingressNginx.controller.service.enabled to true and ingressNginx.controller.service.type to LoadBalancer in both your sc-config.yaml and wc-config.yaml . Use the IP of the ingress-nginx-controller service in your cluster when you set up your DNS. Note At this point if the cluster is running on Safespring and you are using kubespray v2.17.0+ it is possible to create an application credential. Which will give the cluster its own set of credentials instead of using your own. To create a set of credentials use the following command: openstack application credential create <name> And set the following environment variables export OS_APPLICATION_CREDENTIAL_NAME: <name> export OS_APPLICATION_CREDENTIAL_ID: <project_id> export OS_APPLICATION_CREDENTIAL_SECRET: <secret>","title":"Setting up Kubespray variables"},{"location":"operator-manual/openstack/#run-kubespray","text":"Copy the script for generating dynamic ansible inventories: for CLUSTER in \" ${ SERVICE_CLUSTER } \" \" ${ WORKLOAD_CLUSTERS [@] } \" ; do cp kubespray/contrib/terraform/terraform.py \" ${ CK8S_CONFIG_PATH } / ${ CLUSTER } -config/inventory.ini\" chmod +x \" ${ CK8S_CONFIG_PATH } / ${ CLUSTER } -config/inventory.ini\" done Now it is time to run the Kubespray playbook! for CLUSTER in \" ${ SERVICE_CLUSTER } \" \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ./bin/ck8s-kubespray apply \" ${ CLUSTER } \" done","title":"Run Kubespray"},{"location":"operator-manual/openstack/#test-access-to-the-kubernetes-api","text":"You should now have an encrypted kubeconfig file for each cluster under $CK8S_CONFIG_PATH/.state . Check that they work like this: for CLUSTER in \" ${ SERVICE_CLUSTER } \" \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file \" ${ CK8S_CONFIG_PATH } /.state/kube_config_ ${ CLUSTER } .yaml\" \"kubectl --kubeconfig {} cluster-info\" done The output should be similar to this. Kubernetes control plane is running at https://<public-ip>:6443 To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. Kubernetes control plane is running at https://<public-ip>:6443 To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.","title":"Test access to the Kubernetes API"},{"location":"operator-manual/openstack/#prepare-for-compliant-kubernetes-apps","text":"To make the kubeconfig files work with Compliant Kubernetes Apps, you will need to rename or copy them, since Compliant Kubernetes Apps currently only support clusters named sc and wc . If you have multiple workload clusters, you can make this work by setting CK8S_CONFIG_PATH to each $CK8S_CONFIG_PATH/$CLUSTER-config in turn. I.e. CK8S_CONFIG_PATH will be different for compliantkubernetes-kubespray and compliantkubernetes-apps. # In compliantkubernetes-kubespray CK8S_CONFIG_PATH=~/.ck8s/<environment-name> # In compliantkubernetes-apps (one config path per workload cluster) CK8S_CONFIG_PATH=~/.ck8s/<environment-name>/<prefix>-config Copy the kubeconfig files to a path that Apps can find. Option 1 - A single workload cluster: cp \" ${ CK8S_CONFIG_PATH } /.state/kube_config_ ${ SERVICE_CLUSTER } .yaml\" \" ${ CK8S_CONFIG_PATH } /.state/kube_config_sc.yaml\" cp \" ${ CK8S_CONFIG_PATH } /.state/kube_config_ ${ WORKLOAD_CLUSTERS [@] } .yaml\" \" ${ CK8S_CONFIG_PATH } /.state/kube_config_wc.yaml\" You can now use the same CK8S_CONFIG_PATH for Apps as for compliantkubernetes-kubespray. Option 2 - A multiple workload cluster: mkdir -p \" ${ CK8S_CONFIG_PATH } / ${ SERVICE_CLUSTER } -config/.state\" cp \" ${ CK8S_CONFIG_PATH } /.state/kube_config_ ${ SERVICE_CLUSTER } .yaml\" \" ${ CK8S_CONFIG_PATH } / ${ SERVICE_CLUSTER } -config/.state/kube_config_sc.yaml\" for CLUSTER in \" ${ WORKLOAD_CLUSTERS [@] } \" ; do mkdir -p \" ${ CK8S_CONFIG_PATH } / ${ CLUSTER } -config/.state\" cp \" ${ CK8S_CONFIG_PATH } /.state/kube_config_ ${ CLUSTERS } .yaml\" \" ${ CK8S_CONFIG_PATH } / ${ CLUSTER } -config/.state/kube_config_wc.yaml\" done You will then need to set CK8S_CONFIG_PATH to each $CLUSTER-config folder in turn, in order to install Apps on the service cluster and workload clusters. With this you should be ready to install Compliant Kubernetes Apps on top of the clusters. This will be similar to any other cloud provider. Suggested next steps are: Configure DNS using your favorite DNS provider. Create object storage buckets for backups and container registry storage (if desired). Install Compliant Kubernetes Apps! See for example this guide for more details.","title":"Prepare for Compliant Kubernetes Apps"},{"location":"operator-manual/openstack/#cleanup","text":"If you installed Compliant Kubernetes Apps, start by cleaning it up . Make sure you remove all PersistentVolumes and Services with type=LoadBalancer . These objects may create cloud resources that are not managed by Terraform, and therefore would not be removed when we destroy the infrastructure. Destroy the infrastructure using Terraform, the same way you created it: MODULE_PATH = \" $( pwd ) /kubespray/contrib/terraform/openstack\" pushd \" ${ MODULE_PATH } \" for CLUSTER in \" ${ SERVICE_CLUSTER } \" \" ${ WORKLOAD_CLUSTERS [@] } \" ; do terraform init terraform destroy -var-file = \" ${ CK8S_CONFIG_PATH } / ${ CLUSTER } -config/cluster.tfvars\" -state = \" ${ CK8S_CONFIG_PATH } / ${ CLUSTER } -config/terraform.tfstate\" done popd Don't forget to remove any DNS records and object storage buckets that you may have created.","title":"Cleanup"},{"location":"operator-manual/overview/","text":"Work in progress Please check back soon!","title":"Overview"},{"location":"operator-manual/overview/#work-in-progress","text":"Please check back soon!","title":"Work in progress"},{"location":"operator-manual/ovh-managed-kubernetes/","text":"Compliant Kubernetes Deployment on OVH Managed Kubernetes This document contains instructions on how to setup a service cluster and a workload cluster in OVH . The following are the main tasks addressed in this document: Setting up Compliant Kubernetes for OVH Managed Kubernetes Deploying Compliant Kubernetes on top of two Kubernetes clusters. The instructions below are just samples, you need to update them according to your requirements. Before starting, make sure you have all necessary tools . Note This guide is written for compliantkubernetes-apps v0.13.0 Setup Create two Kubernetes clusters in OVH, follow this guide . Sizing hint For the service cluster you can start with creating 3 nodes of size B2-7 and add a B2-15 node. The workload cluster is fine with 3 B2-7 nodes. Configure Compliant Kubernetes Start by preparing your shell with some variables that will be used. REGION=\"waw\" # Region for the cluster ISSUER_MAIL=\"user@example.com\" # Mail that will be used for the LetsEncrypt certificate issuer export CK8S_CONFIG_PATH=~/.ck8s/my-ovh-cluster # Path for the configuration export CK8S_ENVIRONMENT_NAME=my-ovh-cluster # Name of the environment export CK8S_PGP_FP=\"FOOBAR1234567\" # Fingerprint of your PGP key, retrieve with gpg --list-secret-keys export CK8S_CLOUD_PROVIDER=baremetal # We don't have a OVH flavor, but baremetal is fine export CK8S_FLAVOR=dev # Change to \"prod\" if it's a production cluster you're setting up S3_ACCESS_KEY=\"foo\" # Access key for S3, see https://docs.ovh.com/gb/en/public-cloud/getting_started_with_the_swift_S3_API/#create-ec2-credentials S3_SECRET_KEY=\"bar\" # Secret key for S3 Download the kubeconfig and set them up for Compliant Kubernetes by following these steps: Create the path where they're going to be stored mkdir -p \"${CK8S_CONFIG_PATH}/.state\" Download the kubeconfig from OVH for the service cluster and run: mv ~/Downloads/kubeconfig.yml \"${CK8S_CONFIG_PATH}/.state/kube_config_sc.yaml\" sops --encrypt --in-place --pgp \"${CK8S_PGP_FP}\" \"${CK8S_CONFIG_PATH}/.state/kube_config_sc.yaml\" Download the kubeconfig from OVH for the workload cluster and run: mv ~/Downloads/kubeconfig.yml \"${CK8S_CONFIG_PATH}/.state/kube_config_wc.yaml\" sops --encrypt --in-place --pgp \"${CK8S_PGP_FP}\" \"${CK8S_CONFIG_PATH}/.state/kube_config_wc.yaml\" Prepare DNS records Set up DNS records in OVH. Run this snippet and append it into \"Change in text format\" in your domain in OVH. IP=\"203.0.113.123\" cat <<EOF | envsubst *.ops.${CK8S_ENVIRONMENT_NAME} 60 IN A ${IP} *.${CK8S_ENVIRONMENT_NAME} 60 IN A ${IP} grafana.${CK8S_ENVIRONMENT_NAME} 60 IN A ${IP} harbor.${CK8S_ENVIRONMENT_NAME} 60 IN A ${IP} kibana.${CK8S_ENVIRONMENT_NAME} 60 IN A ${IP} dex.${CK8S_ENVIRONMENT_NAME} 60 IN A ${IP} notary.harbor.${CK8S_ENVIRONMENT_NAME} 60 IN A ${IP} EOF Since we don't know the IP for the loadbalancer yet, you can set them to 203.0.113.123 (TEST-NET-3) . Create required buckets that Compliant Kubernetes will use. cat <<EOF | sops --encrypt --pgp \"${CK8S_PGP_FP}\" --output-type ini --input-type ini /dev/stdin > s3cmd.ini [default] use_https = True host_base = s3.${REGION}.cloud.ovh.net host_bucket = s3.${REGION}.cloud.ovh.net access_key = ${S3_ACCESS_KEY} secret_key = ${S3_SECRET_KEY} EOF sops exec-file --no-fifo s3cfg.ini \"s3cmd --config {} mb s3://${CK8S_ENVIRONMENT_NAME}-harbor\" sops exec-file --no-fifo s3cfg.ini \"s3cmd --config {} mb s3://${CK8S_ENVIRONMENT_NAME}-velero\" sops exec-file --no-fifo s3cfg.ini \"s3cmd --config {} mb s3://${CK8S_ENVIRONMENT_NAME}-es-backup\" sops exec-file --no-fifo s3cfg.ini \"s3cmd --config {} mb s3://${CK8S_ENVIRONMENT_NAME}-influxdb\" sops exec-file --no-fifo s3cfg.ini \"s3cmd --config {} mb s3://${CK8S_ENVIRONMENT_NAME}-sc-logs\" Download Compliant Kubernetes and checkout the latest version. git clone git@github.com:elastisys/compliantkubernetes-apps.git cd compliantkubernetes-apps git checkout v0.13.0 Initialize the config. bin/ck8s init Update the service cluster configuration for OVH yq write --inplace sc-config.yaml global.baseDomain \"${CK8S_ENVIRONMENT_NAME}.${DOMAIN}\" yq write --inplace sc-config.yaml global.opsDomain \"ops.${CK8S_ENVIRONMENT_NAME}.${DOMAIN}\" yq write --inplace sc-config.yaml global.issuer \"letsencrypt-prod\" yq write --inplace sc-config.yaml storageClasses.default \"csi-cinder-high-speed\" yq write --inplace sc-config.yaml storageClasses.local.enabled \"false\" yq write --inplace sc-config.yaml objectStorage.s3.region \"${REGION}\" yq write --inplace sc-config.yaml objectStorage.s3.regionEndpoint \"https://s3.${REGION}.cloud.ovh.net/\" yq write --inplace sc-config.yaml fluentd.forwarder.useRegionEndpoint \"false\" yq write --inplace sc-config.yaml nfsProvisioner.server \"not-used\" yq write --inplace sc-config.yaml ingressNginx.controller.useHostPort \"false\" yq write --inplace sc-config.yaml ingressNginx.controller.service.enabled \"true\" yq write --inplace sc-config.yaml ingressNginx.controller.service.type \"LoadBalancer\" yq write --inplace sc-config.yaml ingressNginx.controller.service.annotations \"\" yq write --inplace sc-config.yaml issuers.letsencrypt.prod.email \"${ISSUER_MAIL}\" yq write --inplace sc-config.yaml issuers.letsencrypt.staging.email \"${ISSUER_MAIL}\" yq write --inplace sc-config.yaml metricsServer.enabled \"false\" Update the workload cluster configuration for OVH yq write --inplace wc-config.yaml global.baseDomain \"${CK8S_ENVIRONMENT_NAME}.${DOMAIN}\" yq write --inplace wc-config.yaml global.opsDomain \"ops.${CK8S_ENVIRONMENT_NAME}.${DOMAIN}\" yq write --inplace wc-config.yaml global.issuer \"letsencrypt-prod\" yq write --inplace wc-config.yaml storageClasses.default \"csi-cinder-high-speed\" yq write --inplace wc-config.yaml storageClasses.local.enabled \"false\" yq write --inplace wc-config.yaml objectStorage.s3.region \"${REGION}\" yq write --inplace wc-config.yaml objectStorage.s3.regionEndpoint \"https://s3.${REGION}.cloud.ovh.net/\" yq write --inplace wc-config.yaml ingressNginx.controller.useHostPort \"false\" yq write --inplace wc-config.yaml ingressNginx.controller.service.enabled \"true\" yq write --inplace wc-config.yaml ingressNginx.controller.service.type \"LoadBalancer\" yq write --inplace wc-config.yaml ingressNginx.controller.service.annotations \"\" yq write --inplace wc-config.yaml metricsServer.enabled \"false\" Set s3 credentials sops --set '[\"objectStorage\"][\"s3\"][\"accessKey\"] \"'\"${S3_ACCESS_KEY}\"'\"' secrets.yaml sops --set '[\"objectStorage\"][\"s3\"][\"secretKey\"] \"'\"${S3_SECRET_KEY}\"'\"' secrets.yaml Deploy Compliant Kubernetes Now you're ready to deploy Compliant Kubernetes. When the apply command is done, fetch the external IP assigned to the loadbalancer service and update the DNS record to match this. bin/ck8s apply sc bin/ck8s ops kubectl sc get svc -n ingress-nginx ingress-nginx-controller Run this snippet and update the DNS records you added previously to match the external IP of the service cluster load balancer. IP=\"SERVICE_CLUSTER_LB_IP\" cat <<EOF | envsubst *.ops.${CK8S_ENVIRONMENT_NAME} IN A ${IP} grafana.${CK8S_ENVIRONMENT_NAME} IN A ${IP} harbor.${CK8S_ENVIRONMENT_NAME} IN A ${IP} kibana.${CK8S_ENVIRONMENT_NAME} IN A ${IP} dex.${CK8S_ENVIRONMENT_NAME} IN A ${IP} notary.harbor.${CK8S_ENVIRONMENT_NAME} IN A ${IP} EOF Next, do the same for the workload cluster. bin/ck8s apply wc bin/ck8s ops kubectl wc get svc -n ingress-nginx ingress-nginx-controller Run this snippet and update the DNS records you added previously to match the external IP of the workload cluster load balancer. IP=\"WORKLOAD_CLUSTER_LB_IP\" cat <<EOF | envsubst *.${CK8S_ENVIRONMENT_NAME} IN A ${IP} EOF Limitations At the time of writing, there's some issues with velero and fluentd. Workarounds for making this work until fix is out Add s3_endpoint to fluentd, fixed with this issue To add this, you need to edit the config map fluentd uses. Find the <match **> tag and add s3_endpoint https://s3.REGION.cloud.ovh.net/ Where REGION matches what region you're using ( waw in the example). After that, delete the fluentd pod to force it to reload the configuration. $ bin/ck8s ops kubectl sc edit cm -n fluentd fluentd-aggregator-configmap . . . <match **> @id output-s3 @type s3 aws_key_id \"#{ENV['AWS_ACCESS_KEY_ID']}\" aws_sec_key \"#{ENV['AWS_ACCESS_SECRET_KEY']}\" s3_endpoint https://s3.waw.cloud.ovh.net/ # <--- Add this line s3_region waw . . $ bin/ck8s ops kubectl sc delete pod -n fluentd fluentd-0 Update velero to use 1.3.0 instead of 1.2.0, fixed with this issue For velero to work with OVH S3, velero needs to run with version 1.3.0 . bin/ck8s ops kubectl sc patch deployment -n velero velero -p '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"velero\",\"image\":\"velero/velero:v1.3.0\"}]}}}}' bin/ck8s ops kubectl sc patch daemonset -n velero restic -p '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"velero\",\"image\":\"velero/velero:v1.3.0\"}]}}}}' bin/ck8s ops kubectl wc patch deployment -n velero velero -p '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"velero\",\"image\":\"velero/velero:v1.3.0\"}]}}}}' bin/ck8s ops kubectl wc patch daemonset -n velero restic -p '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"velero\",\"image\":\"velero/velero:v1.3.0\"}]}}}}'","title":"On OVH Managed Kubernetes"},{"location":"operator-manual/ovh-managed-kubernetes/#compliant-kubernetes-deployment-on-ovh-managed-kubernetes","text":"This document contains instructions on how to setup a service cluster and a workload cluster in OVH . The following are the main tasks addressed in this document: Setting up Compliant Kubernetes for OVH Managed Kubernetes Deploying Compliant Kubernetes on top of two Kubernetes clusters. The instructions below are just samples, you need to update them according to your requirements. Before starting, make sure you have all necessary tools . Note This guide is written for compliantkubernetes-apps v0.13.0","title":"Compliant Kubernetes Deployment on OVH Managed Kubernetes"},{"location":"operator-manual/ovh-managed-kubernetes/#setup","text":"Create two Kubernetes clusters in OVH, follow this guide . Sizing hint For the service cluster you can start with creating 3 nodes of size B2-7 and add a B2-15 node. The workload cluster is fine with 3 B2-7 nodes.","title":"Setup"},{"location":"operator-manual/ovh-managed-kubernetes/#configure-compliant-kubernetes","text":"Start by preparing your shell with some variables that will be used. REGION=\"waw\" # Region for the cluster ISSUER_MAIL=\"user@example.com\" # Mail that will be used for the LetsEncrypt certificate issuer export CK8S_CONFIG_PATH=~/.ck8s/my-ovh-cluster # Path for the configuration export CK8S_ENVIRONMENT_NAME=my-ovh-cluster # Name of the environment export CK8S_PGP_FP=\"FOOBAR1234567\" # Fingerprint of your PGP key, retrieve with gpg --list-secret-keys export CK8S_CLOUD_PROVIDER=baremetal # We don't have a OVH flavor, but baremetal is fine export CK8S_FLAVOR=dev # Change to \"prod\" if it's a production cluster you're setting up S3_ACCESS_KEY=\"foo\" # Access key for S3, see https://docs.ovh.com/gb/en/public-cloud/getting_started_with_the_swift_S3_API/#create-ec2-credentials S3_SECRET_KEY=\"bar\" # Secret key for S3 Download the kubeconfig and set them up for Compliant Kubernetes by following these steps:","title":"Configure Compliant Kubernetes"},{"location":"operator-manual/ovh-managed-kubernetes/#create-the-path-where-theyre-going-to-be-stored","text":"mkdir -p \"${CK8S_CONFIG_PATH}/.state\"","title":"Create the path where they're going to be stored"},{"location":"operator-manual/ovh-managed-kubernetes/#download-the-kubeconfig-from-ovh-for-the-service-cluster-and-run","text":"mv ~/Downloads/kubeconfig.yml \"${CK8S_CONFIG_PATH}/.state/kube_config_sc.yaml\" sops --encrypt --in-place --pgp \"${CK8S_PGP_FP}\" \"${CK8S_CONFIG_PATH}/.state/kube_config_sc.yaml\"","title":"Download the kubeconfig from OVH for the service cluster and run:"},{"location":"operator-manual/ovh-managed-kubernetes/#download-the-kubeconfig-from-ovh-for-the-workload-cluster-and-run","text":"mv ~/Downloads/kubeconfig.yml \"${CK8S_CONFIG_PATH}/.state/kube_config_wc.yaml\" sops --encrypt --in-place --pgp \"${CK8S_PGP_FP}\" \"${CK8S_CONFIG_PATH}/.state/kube_config_wc.yaml\"","title":"Download the kubeconfig from OVH for the workload cluster and run:"},{"location":"operator-manual/ovh-managed-kubernetes/#prepare-dns-records","text":"Set up DNS records in OVH. Run this snippet and append it into \"Change in text format\" in your domain in OVH. IP=\"203.0.113.123\" cat <<EOF | envsubst *.ops.${CK8S_ENVIRONMENT_NAME} 60 IN A ${IP} *.${CK8S_ENVIRONMENT_NAME} 60 IN A ${IP} grafana.${CK8S_ENVIRONMENT_NAME} 60 IN A ${IP} harbor.${CK8S_ENVIRONMENT_NAME} 60 IN A ${IP} kibana.${CK8S_ENVIRONMENT_NAME} 60 IN A ${IP} dex.${CK8S_ENVIRONMENT_NAME} 60 IN A ${IP} notary.harbor.${CK8S_ENVIRONMENT_NAME} 60 IN A ${IP} EOF Since we don't know the IP for the loadbalancer yet, you can set them to 203.0.113.123 (TEST-NET-3) . Create required buckets that Compliant Kubernetes will use. cat <<EOF | sops --encrypt --pgp \"${CK8S_PGP_FP}\" --output-type ini --input-type ini /dev/stdin > s3cmd.ini [default] use_https = True host_base = s3.${REGION}.cloud.ovh.net host_bucket = s3.${REGION}.cloud.ovh.net access_key = ${S3_ACCESS_KEY} secret_key = ${S3_SECRET_KEY} EOF sops exec-file --no-fifo s3cfg.ini \"s3cmd --config {} mb s3://${CK8S_ENVIRONMENT_NAME}-harbor\" sops exec-file --no-fifo s3cfg.ini \"s3cmd --config {} mb s3://${CK8S_ENVIRONMENT_NAME}-velero\" sops exec-file --no-fifo s3cfg.ini \"s3cmd --config {} mb s3://${CK8S_ENVIRONMENT_NAME}-es-backup\" sops exec-file --no-fifo s3cfg.ini \"s3cmd --config {} mb s3://${CK8S_ENVIRONMENT_NAME}-influxdb\" sops exec-file --no-fifo s3cfg.ini \"s3cmd --config {} mb s3://${CK8S_ENVIRONMENT_NAME}-sc-logs\" Download Compliant Kubernetes and checkout the latest version. git clone git@github.com:elastisys/compliantkubernetes-apps.git cd compliantkubernetes-apps git checkout v0.13.0 Initialize the config. bin/ck8s init Update the service cluster configuration for OVH yq write --inplace sc-config.yaml global.baseDomain \"${CK8S_ENVIRONMENT_NAME}.${DOMAIN}\" yq write --inplace sc-config.yaml global.opsDomain \"ops.${CK8S_ENVIRONMENT_NAME}.${DOMAIN}\" yq write --inplace sc-config.yaml global.issuer \"letsencrypt-prod\" yq write --inplace sc-config.yaml storageClasses.default \"csi-cinder-high-speed\" yq write --inplace sc-config.yaml storageClasses.local.enabled \"false\" yq write --inplace sc-config.yaml objectStorage.s3.region \"${REGION}\" yq write --inplace sc-config.yaml objectStorage.s3.regionEndpoint \"https://s3.${REGION}.cloud.ovh.net/\" yq write --inplace sc-config.yaml fluentd.forwarder.useRegionEndpoint \"false\" yq write --inplace sc-config.yaml nfsProvisioner.server \"not-used\" yq write --inplace sc-config.yaml ingressNginx.controller.useHostPort \"false\" yq write --inplace sc-config.yaml ingressNginx.controller.service.enabled \"true\" yq write --inplace sc-config.yaml ingressNginx.controller.service.type \"LoadBalancer\" yq write --inplace sc-config.yaml ingressNginx.controller.service.annotations \"\" yq write --inplace sc-config.yaml issuers.letsencrypt.prod.email \"${ISSUER_MAIL}\" yq write --inplace sc-config.yaml issuers.letsencrypt.staging.email \"${ISSUER_MAIL}\" yq write --inplace sc-config.yaml metricsServer.enabled \"false\" Update the workload cluster configuration for OVH yq write --inplace wc-config.yaml global.baseDomain \"${CK8S_ENVIRONMENT_NAME}.${DOMAIN}\" yq write --inplace wc-config.yaml global.opsDomain \"ops.${CK8S_ENVIRONMENT_NAME}.${DOMAIN}\" yq write --inplace wc-config.yaml global.issuer \"letsencrypt-prod\" yq write --inplace wc-config.yaml storageClasses.default \"csi-cinder-high-speed\" yq write --inplace wc-config.yaml storageClasses.local.enabled \"false\" yq write --inplace wc-config.yaml objectStorage.s3.region \"${REGION}\" yq write --inplace wc-config.yaml objectStorage.s3.regionEndpoint \"https://s3.${REGION}.cloud.ovh.net/\" yq write --inplace wc-config.yaml ingressNginx.controller.useHostPort \"false\" yq write --inplace wc-config.yaml ingressNginx.controller.service.enabled \"true\" yq write --inplace wc-config.yaml ingressNginx.controller.service.type \"LoadBalancer\" yq write --inplace wc-config.yaml ingressNginx.controller.service.annotations \"\" yq write --inplace wc-config.yaml metricsServer.enabled \"false\" Set s3 credentials sops --set '[\"objectStorage\"][\"s3\"][\"accessKey\"] \"'\"${S3_ACCESS_KEY}\"'\"' secrets.yaml sops --set '[\"objectStorage\"][\"s3\"][\"secretKey\"] \"'\"${S3_SECRET_KEY}\"'\"' secrets.yaml","title":"Prepare DNS records"},{"location":"operator-manual/ovh-managed-kubernetes/#deploy-compliant-kubernetes","text":"Now you're ready to deploy Compliant Kubernetes. When the apply command is done, fetch the external IP assigned to the loadbalancer service and update the DNS record to match this. bin/ck8s apply sc bin/ck8s ops kubectl sc get svc -n ingress-nginx ingress-nginx-controller Run this snippet and update the DNS records you added previously to match the external IP of the service cluster load balancer. IP=\"SERVICE_CLUSTER_LB_IP\" cat <<EOF | envsubst *.ops.${CK8S_ENVIRONMENT_NAME} IN A ${IP} grafana.${CK8S_ENVIRONMENT_NAME} IN A ${IP} harbor.${CK8S_ENVIRONMENT_NAME} IN A ${IP} kibana.${CK8S_ENVIRONMENT_NAME} IN A ${IP} dex.${CK8S_ENVIRONMENT_NAME} IN A ${IP} notary.harbor.${CK8S_ENVIRONMENT_NAME} IN A ${IP} EOF Next, do the same for the workload cluster. bin/ck8s apply wc bin/ck8s ops kubectl wc get svc -n ingress-nginx ingress-nginx-controller Run this snippet and update the DNS records you added previously to match the external IP of the workload cluster load balancer. IP=\"WORKLOAD_CLUSTER_LB_IP\" cat <<EOF | envsubst *.${CK8S_ENVIRONMENT_NAME} IN A ${IP} EOF","title":"Deploy Compliant Kubernetes"},{"location":"operator-manual/ovh-managed-kubernetes/#limitations","text":"At the time of writing, there's some issues with velero and fluentd. Workarounds for making this work until fix is out","title":"Limitations"},{"location":"operator-manual/ovh-managed-kubernetes/#add-s3_endpoint-to-fluentd-fixed-with-this-issue","text":"To add this, you need to edit the config map fluentd uses. Find the <match **> tag and add s3_endpoint https://s3.REGION.cloud.ovh.net/ Where REGION matches what region you're using ( waw in the example). After that, delete the fluentd pod to force it to reload the configuration. $ bin/ck8s ops kubectl sc edit cm -n fluentd fluentd-aggregator-configmap . . . <match **> @id output-s3 @type s3 aws_key_id \"#{ENV['AWS_ACCESS_KEY_ID']}\" aws_sec_key \"#{ENV['AWS_ACCESS_SECRET_KEY']}\" s3_endpoint https://s3.waw.cloud.ovh.net/ # <--- Add this line s3_region waw . . $ bin/ck8s ops kubectl sc delete pod -n fluentd fluentd-0","title":"Add s3_endpoint to fluentd, fixed with this issue"},{"location":"operator-manual/ovh-managed-kubernetes/#update-velero-to-use-130-instead-of-120-fixed-with-this-issue","text":"For velero to work with OVH S3, velero needs to run with version 1.3.0 . bin/ck8s ops kubectl sc patch deployment -n velero velero -p '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"velero\",\"image\":\"velero/velero:v1.3.0\"}]}}}}' bin/ck8s ops kubectl sc patch daemonset -n velero restic -p '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"velero\",\"image\":\"velero/velero:v1.3.0\"}]}}}}' bin/ck8s ops kubectl wc patch deployment -n velero velero -p '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"velero\",\"image\":\"velero/velero:v1.3.0\"}]}}}}' bin/ck8s ops kubectl wc patch daemonset -n velero restic -p '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"velero\",\"image\":\"velero/velero:v1.3.0\"}]}}}}'","title":"Update velero to use 1.3.0 instead of 1.2.0, fixed with this issue"},{"location":"operator-manual/qa/","text":"Quality Assurance When you have created your Compliant Kubernetes cluster it can be wise to run some checks to ensure that it works as expected. This document details some snippets that you can follow in order to ensure some functionality of the cluster. Customer API and Harbor access Pre-requisites You've got Docker installed. You've exported CK8S_CONFIG_PATH in your shell. You've set baseDomain in your shell to what's used in your cluster. Your current working directory is the compliantkubernets-apps repository. You've installed the kubectl plugin kubelogin . See instructions on how to install it. Create and set user kubeconfig ./bin/ck8s user-kubeconfig sops -d -i --config ${CK8S_CONFIG_PATH}/.sops.yaml ${CK8S_CONFIG_PATH}/user/kubeconfig.yaml export KUBECONFIG=${CK8S_CONFIG_PATH}/user/kubeconfig.yaml Authenticate by issuing any kubectl command, e.g. kubectl get pods Your browser will be opened and you'll be asked to login through Dex. Login to Harbor GUI and create 'test' project Go to https://harbor.${baseDomain} , and login though OIDC. Create project 'test'. Click on your user in the top right corner and select User profile. Copy CLI secret. Push image to Harbor and scan it Pull Nginx from dockerhub docker pull nginx . Login to the Harbor registry docker login https://harbor.${baseDomain} Enter your Harbor username and the copied CLI secret. Prepare Nginx image for pushing to Harbor registry docker tag nginx harbor.${baseDomain}/test/nginx Push image to Harbor docker push harbor.${baseDomain}/test/nginx Enter 'test' project in the Harbor GUI, select the newly pushed image and scan it. Create secret for pulling images from harbor kubectl create secret generic regcred \\ --from-file=.dockerconfigjson=${HOME}/.docker/config.json \\ --type=kubernetes.io/dockerconfigjson kubectl patch serviceaccount default -p '{\"imagePullSecrets\": [{\"name\": \"regcred\"}]}' Test pulling from Harbor and start privileged and unprivileged pods kubectl run --image nginxinc/nginx-unprivileged nginx-unprivileged kubectl run --image harbor.${baseDomain}/test/nginx nginx-privileged # You should see that both pods and that nginx-unprivileged eventually becomes running while nginx-privileged does not. kubectl get pods # Check events from the nginx-privileged. kubectl describe pod nginx-privileged # You should see 'Error: container has runAsNonRoot and image will run as root'. Cleanup of created Kubernetes resources kubectl delete pod --all kubectl delete secret regcred","title":"QA"},{"location":"operator-manual/qa/#quality-assurance","text":"When you have created your Compliant Kubernetes cluster it can be wise to run some checks to ensure that it works as expected. This document details some snippets that you can follow in order to ensure some functionality of the cluster.","title":"Quality Assurance"},{"location":"operator-manual/qa/#customer-api-and-harbor-access","text":"","title":"Customer API and Harbor access"},{"location":"operator-manual/qa/#pre-requisites","text":"You've got Docker installed. You've exported CK8S_CONFIG_PATH in your shell. You've set baseDomain in your shell to what's used in your cluster. Your current working directory is the compliantkubernets-apps repository. You've installed the kubectl plugin kubelogin . See instructions on how to install it.","title":"Pre-requisites"},{"location":"operator-manual/qa/#create-and-set-user-kubeconfig","text":"./bin/ck8s user-kubeconfig sops -d -i --config ${CK8S_CONFIG_PATH}/.sops.yaml ${CK8S_CONFIG_PATH}/user/kubeconfig.yaml export KUBECONFIG=${CK8S_CONFIG_PATH}/user/kubeconfig.yaml Authenticate by issuing any kubectl command, e.g. kubectl get pods Your browser will be opened and you'll be asked to login through Dex.","title":"Create and set user kubeconfig"},{"location":"operator-manual/qa/#login-to-harbor-gui-and-create-test-project","text":"Go to https://harbor.${baseDomain} , and login though OIDC. Create project 'test'. Click on your user in the top right corner and select User profile. Copy CLI secret.","title":"Login to Harbor GUI and create 'test' project"},{"location":"operator-manual/qa/#push-image-to-harbor-and-scan-it","text":"Pull Nginx from dockerhub docker pull nginx . Login to the Harbor registry docker login https://harbor.${baseDomain} Enter your Harbor username and the copied CLI secret. Prepare Nginx image for pushing to Harbor registry docker tag nginx harbor.${baseDomain}/test/nginx Push image to Harbor docker push harbor.${baseDomain}/test/nginx Enter 'test' project in the Harbor GUI, select the newly pushed image and scan it.","title":"Push image to Harbor and scan it"},{"location":"operator-manual/qa/#create-secret-for-pulling-images-from-harbor","text":"kubectl create secret generic regcred \\ --from-file=.dockerconfigjson=${HOME}/.docker/config.json \\ --type=kubernetes.io/dockerconfigjson kubectl patch serviceaccount default -p '{\"imagePullSecrets\": [{\"name\": \"regcred\"}]}'","title":"Create secret for pulling images from harbor"},{"location":"operator-manual/qa/#test-pulling-from-harbor-and-start-privileged-and-unprivileged-pods","text":"kubectl run --image nginxinc/nginx-unprivileged nginx-unprivileged kubectl run --image harbor.${baseDomain}/test/nginx nginx-privileged # You should see that both pods and that nginx-unprivileged eventually becomes running while nginx-privileged does not. kubectl get pods # Check events from the nginx-privileged. kubectl describe pod nginx-privileged # You should see 'Error: container has runAsNonRoot and image will run as root'.","title":"Test pulling from Harbor and start privileged and unprivileged pods"},{"location":"operator-manual/qa/#cleanup-of-created-kubernetes-resources","text":"kubectl delete pod --all kubectl delete secret regcred","title":"Cleanup of created Kubernetes resources"},{"location":"operator-manual/troubleshooting/","text":"Troubleshooting Tools Help! Something is wrong with my Compliant Kubernetes cluster. Fear no more, this guide will help you make sense. This guide assumes that: You have pre-requisites installed. Your environment variables, in particular CK8S_CONFIG_PATH is set. Your config folder (e.g. for OpenStack ) is available. compliantkubernetes-apps and compliantkubernetes-kubespray is available. I have no clue where to start If you get lost, start checking from the \"physical layer\" and up. Are the Nodes still accessible via SSH? for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ansible -i ${ CK8S_CONFIG_PATH } / ${ CLUSTER } -config/inventory.ini all -m ping done Are the Nodes \"doing fine\"? Dmesg should not display unexpected messages. OOM will show up here. for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ansible -i ${ CK8S_CONFIG_PATH } / ${ CLUSTER } -config/inventory.ini all -m shell -a 'echo; hostname; dmesg | tail -n 10' done Uptime should show high uptime (e.g., days) and low load (e.g., less than 3): for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ansible -i $CK8S_CONFIG_PATH / ${ CLUSTER } -config/inventory.ini all -m shell -a 'echo; hostname; uptime' done Any process that uses too much CPU? for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ansible -i $CK8S_CONFIG_PATH / ${ CLUSTER } -config/inventory.ini all -m shell -a 'echo; hostname; ps -Ao user,uid,comm,pid,pcpu,tty --sort=-pcpu | head -n 6' done Is there enough disk space? All writeable file-systems should have at least 30% free. for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ansible -i $CK8S_CONFIG_PATH / ${ CLUSTER } -config/inventory.ini all -m shell -a 'echo; hostname; df -h' done Is there enough available memory? There should be at least a few GB of available memory. for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ansible -i $CK8S_CONFIG_PATH / ${ CLUSTER } -config/inventory.ini all -m shell -a 'echo; hostname; cat /proc/meminfo | grep Available' done Can Nodes access the Internet? for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ansible -i $CK8S_CONFIG_PATH / ${ CLUSTER } -config/inventory.ini all -m shell -a 'echo; hostname; curl --silent https://checkip.amazonaws.com' done Are the Nodes having the proper time? You should see System clock synchronized: yes and NTP service: active . for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ansible -i $CK8S_CONFIG_PATH / ${ CLUSTER } -config/inventory.ini all -m shell -a 'echo; timedatectl status' done Is the base OS doing fine? We generally run the latest Ubuntu LTS , at the time of this writing Ubuntu 20.04 LTS. You can confirm this by doing: for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ansible -i $CK8S_CONFIG_PATH / ${ CLUSTER } -config/inventory.ini all -m shell -a 'cat /etc/lsb-release' done Are systemd units running fine? You should see running and not degraded . for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ansible -i $CK8S_CONFIG_PATH / ${ CLUSTER } -config/inventory.ini all -m shell -a 'systemctl is-system-running' done Are the Kubernetes clusters doing fine? Are the Nodes reporting in on Kubernetes? All Kubernetes Nodes, both control-plane and workers, should be Ready : for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get nodes' done Is Rook doing fine? If Rook is installed, is Rook doing fine? You should see HEALTH_OK . for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} -n rook-ceph apply -f ./compliantkubernetes-kubespray/rook/toolbox-deploy.yaml' done for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} -n rook-ceph exec deploy/rook-ceph-tools -- ceph status' done Are Kubernetes Pods doing fine? Pods should be Running or Completed , and fully Ready (e.g., 1/1 or 6/6 )? for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get --all-namespaces pods' done Are all Deployments fine? Deployments should show all Pods Ready, Up-to-date and Available (e.g., 2/2 2 2 ). for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get --all-namespaces deployments' done Are all DaemonSets fine? DaemonSets should show as many Pods Desired, Current, Ready and Up-to-date, as Desired. for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get --all-namespaces ds' done Are Helm Releases fine? All Releases should be deployed . for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do export KUBECONFIG = kube_config_ $CLUSTER .yaml sops -d ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml > $KUBECONFIG helm list --all --all-namespaces shred $KUBECONFIG done Is cert-manager doing fine? Are (Cluster)Issuers fine? All Resources should be READY=True or valid . for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do export KUBECONFIG = kube_config_ $CLUSTER .yaml sops -d ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml > $KUBECONFIG kubectl get clusterissuers,issuers,certificates,orders,challenges --all-namespaces shred $KUBECONFIG done Where do I find the Nodes public and private IP? find . -name inventory.ini or for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ansible-inventory -i ${ CK8S_CONFIG_PATH } / ${ CLUSTER } -config/inventory.ini --list all done ansible_host is usually the public IP, while ip is usually the private IP. Node cannot be access via SSH Important Make sure it is \"not you\". Are you well connected to the VPN? Is this the only Node which lost SSH access? Important If you are using Rook, it is usually set up with replication 2, which means it can tolerate one restarting Node. Make sure that, either Rook is healthy or that you are really sure you are restarting the right Node. Try connecting to the unhealthy Node via a different Node and internal IP: UNHEALTHY_NODE = 172 .0.10.205 # You lost access to this one JUMP_NODE = 89 .145.xxx.yyy # You have access to this one ssh -J ubuntu@ $JUMP_NODE ubuntu@ $UNHEALTHY_NODE Try rebooting the Node via cloud provider specific CLI: UNHEALTHY_NODE = cksc-worker-2 # Example for ExoScale exo vm reboot --force $UNHEALTHY_NODE If using Rook make sure its health goes back to HEALTH_OK . A Node has incorrect time Incorrect time on a Node can have sever consequences with replication and monitoring. In fact, if you follow ISO 27001, A.12.4.4 Clock Synchronisation requires you to ensure clocks are synchronized. These days, Linux distributions should come out-of-the-box with timesyncd for time synchronization via NTP. To figure out what is wrong, SSH into the target Node and try the following: sudo systemctl status systemd-timesyncd sudo journalctl --unit systemd-timesyncd sudo timedatectl status sudo timedatectl timesync-status sudo timedatectl show-timesync Possible causes include incorrect NTP server settings, or NTP being blocked by firewall. For reminder, NTP works over UDP port 123. Node seems not fine Important If you are using Rook, it is usually set up with replication 2, which means it can tolerate one restarting Node. Make sure that, either Rook is healthy or that you are really sure you are restarting the right Node. Try rebooting the Node: UNHEALTHY_NODE = 89 .145.xxx.yyy ssh ubuntu@ $UNHEALTHY_NODE sudo reboot If using Rook make sure its health goes back to HEALTH_OK . Node seems really not fine. I want a new one. Is it 2AM? Do not replace Nodes, instead simply add a new one. You might run out of capacity, you might lose redundancy, you might replace the wrong Node. Prefer to add a Node and see if that solves the problem. Okay, I want to add a new Node. Prefer this option if you \"quickly\" need to add CPU, memory or storage (i.e., Rook) capacity. First, check for infrastructure drift, as shown here . Depending on your provider: Add a new Node by editing the *.tfvars . Re-apply Terraform. Re-create the inventory.ini (skip this step if the cluster is using a dynamic inventory). Re-apply Kubespray. Re-fix the Kubernetes API URL. Check that the new Node joined the cluster, as shown here . A systemd unit failed SSH into the Node. Check which systemd unit is failing: systemctl --failed Gather more information: FAILED_UNIT = fwupd-refresh.service systemctl status $FAILED_UNIT journalctl --unit $FAILED_UNIT Rook seems not fine Please check the following upstream documents: Rook Common Issues Ceph Common Issues Pod seems not fine Before starting, set up a handy environment: CLUSTER = cksc # Cluster containing the unhealthy Pod export KUBECONFIG = kube_config_ $CLUSTER .yaml sops -d ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml > $KUBECONFIG Check that you are on the right cluster: kubectl get nodes Find the name of the Pod which is not fine: kubectl get pod -A # Copy-paste the Pod and Pod namespace below UNHEALTHY_POD = prometheus-kube-prometheus-stack-prometheus-0 UNHEALTHY_POD_NAMESPACE = monitoring Gather some \"evidence\" for later diagnostics, when the heat is over: kubectl describe pod -n $UNHEALTHY_POD_NAMESPACE $UNHEALTHY_POD kubectl logs -n $UNHEALTHY_POD_NAMESPACE $UNHEALTHY_POD Try to kill and check if the underlying Deployment, StatefulSet or DaemonSet will restart it: kubectl delete pod -n $UNHEALTHY_POD_NAMESPACE $UNHEALTHY_POD kubectl get pod -A --watch Helm Release is failed Before starting, set up a handy environment: CLUSTER = cksc # Cluster containing the failed Release export KUBECONFIG = kube_config_ $CLUSTER .yaml sops -d ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml > $KUBECONFIG Check that you are on the right cluster: kubectl get nodes Find the failed Release: helm ls --all-namespaces --all FAILED_RELEASE = user-rbac FAILED_RELEASE_NAMESPACE = kube-system Just to make sure, do a drift check, as shown here . Remove the failed Release: helm uninstall -n $FAILED_RELEASE_NAMESPACE $FAILED_RELEASE Re-apply apps according to documentation. cert-manager is not fine Follow cert-manager's troubleshooting, specifically: Troubleshooting Troubleshooting Issuing ACME Certificates How do I check if infrastructure drifted due to manual intervention? Go to the docs of the cloud provider and run Terraform plan instead of apply . For Exoscale, it looks as follows: TF_SCRIPTS_DIR = $( readlink -f compliantkubernetes-kubespray/kubespray/contrib/terraform/exoscale ) for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do pushd ${ TF_SCRIPTS_DIR } export TF_VAR_inventory_file = ${ CK8S_CONFIG_PATH } / ${ CLUSTER } -config/inventory.ini terraform init terraform plan \\ -var-file = ${ CK8S_CONFIG_PATH } / ${ CLUSTER } -config/cluster.tfvars \\ -state = ${ CK8S_CONFIG_PATH } / ${ CLUSTER } -config/terraform.tfstate popd done How do I check if the Kubespray setup drifted due to manual intervention? At the time of this writing, this cannot be done, but efforts are underway . How do I check if apps drifted due to manual intervention? # For service cluster ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ SERVICE_CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_sc.yaml ./compliantkubernetes-apps/bin/ck8s ops helmfile sc diff # Respond \"n\" if you get WARN # For the workload clusters for CLUSTER in \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_wc.yaml ./compliantkubernetes-apps/bin/ck8s ops helmfile wc diff # Respond \"n\" if you get WARN done Velero backup stuck in progress Velero is known to get stuck InProgress when doing backups velero backup get NAME STATUS ERRORS WARNINGS CREATED EXPIRES STORAGE LOCATION SELECTOR velero-daily-backup-20211005143248 InProgress 0 0 2021 -10-05 14 :32:48 +0200 CEST 29d default !nobackup First try to delete the backup ./velero backup delete velero-daily-backup-20211005143248 Then kill all the pods under the velero namespace ./compliantkubernetes-apps/bin/ck8s ops kubectl wc delete pods -n velero --all Check that the backup is gone velero backup get NAME STATUS ERRORS WARNINGS CREATED EXPIRES STORAGE LOCATION SELECTOR Recreate the backup from a schedule velero backup create --from-schedule velero-daily-backup","title":"Troubleshooting"},{"location":"operator-manual/troubleshooting/#troubleshooting-tools","text":"Help! Something is wrong with my Compliant Kubernetes cluster. Fear no more, this guide will help you make sense. This guide assumes that: You have pre-requisites installed. Your environment variables, in particular CK8S_CONFIG_PATH is set. Your config folder (e.g. for OpenStack ) is available. compliantkubernetes-apps and compliantkubernetes-kubespray is available.","title":"Troubleshooting Tools"},{"location":"operator-manual/troubleshooting/#i-have-no-clue-where-to-start","text":"If you get lost, start checking from the \"physical layer\" and up.","title":"I have no clue where to start"},{"location":"operator-manual/troubleshooting/#are-the-nodes-still-accessible-via-ssh","text":"for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ansible -i ${ CK8S_CONFIG_PATH } / ${ CLUSTER } -config/inventory.ini all -m ping done","title":"Are the Nodes still accessible via SSH?"},{"location":"operator-manual/troubleshooting/#are-the-nodes-doing-fine","text":"Dmesg should not display unexpected messages. OOM will show up here. for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ansible -i ${ CK8S_CONFIG_PATH } / ${ CLUSTER } -config/inventory.ini all -m shell -a 'echo; hostname; dmesg | tail -n 10' done Uptime should show high uptime (e.g., days) and low load (e.g., less than 3): for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ansible -i $CK8S_CONFIG_PATH / ${ CLUSTER } -config/inventory.ini all -m shell -a 'echo; hostname; uptime' done Any process that uses too much CPU? for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ansible -i $CK8S_CONFIG_PATH / ${ CLUSTER } -config/inventory.ini all -m shell -a 'echo; hostname; ps -Ao user,uid,comm,pid,pcpu,tty --sort=-pcpu | head -n 6' done Is there enough disk space? All writeable file-systems should have at least 30% free. for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ansible -i $CK8S_CONFIG_PATH / ${ CLUSTER } -config/inventory.ini all -m shell -a 'echo; hostname; df -h' done Is there enough available memory? There should be at least a few GB of available memory. for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ansible -i $CK8S_CONFIG_PATH / ${ CLUSTER } -config/inventory.ini all -m shell -a 'echo; hostname; cat /proc/meminfo | grep Available' done Can Nodes access the Internet? for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ansible -i $CK8S_CONFIG_PATH / ${ CLUSTER } -config/inventory.ini all -m shell -a 'echo; hostname; curl --silent https://checkip.amazonaws.com' done Are the Nodes having the proper time? You should see System clock synchronized: yes and NTP service: active . for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ansible -i $CK8S_CONFIG_PATH / ${ CLUSTER } -config/inventory.ini all -m shell -a 'echo; timedatectl status' done","title":"Are the Nodes \"doing fine\"?"},{"location":"operator-manual/troubleshooting/#is-the-base-os-doing-fine","text":"We generally run the latest Ubuntu LTS , at the time of this writing Ubuntu 20.04 LTS. You can confirm this by doing: for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ansible -i $CK8S_CONFIG_PATH / ${ CLUSTER } -config/inventory.ini all -m shell -a 'cat /etc/lsb-release' done Are systemd units running fine? You should see running and not degraded . for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ansible -i $CK8S_CONFIG_PATH / ${ CLUSTER } -config/inventory.ini all -m shell -a 'systemctl is-system-running' done","title":"Is the base OS doing fine?"},{"location":"operator-manual/troubleshooting/#are-the-kubernetes-clusters-doing-fine","text":"Are the Nodes reporting in on Kubernetes? All Kubernetes Nodes, both control-plane and workers, should be Ready : for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get nodes' done","title":"Are the Kubernetes clusters doing fine?"},{"location":"operator-manual/troubleshooting/#is-rook-doing-fine","text":"If Rook is installed, is Rook doing fine? You should see HEALTH_OK . for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} -n rook-ceph apply -f ./compliantkubernetes-kubespray/rook/toolbox-deploy.yaml' done for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} -n rook-ceph exec deploy/rook-ceph-tools -- ceph status' done","title":"Is Rook doing fine?"},{"location":"operator-manual/troubleshooting/#are-kubernetes-pods-doing-fine","text":"Pods should be Running or Completed , and fully Ready (e.g., 1/1 or 6/6 )? for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get --all-namespaces pods' done Are all Deployments fine? Deployments should show all Pods Ready, Up-to-date and Available (e.g., 2/2 2 2 ). for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get --all-namespaces deployments' done Are all DaemonSets fine? DaemonSets should show as many Pods Desired, Current, Ready and Up-to-date, as Desired. for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get --all-namespaces ds' done","title":"Are Kubernetes Pods doing fine?"},{"location":"operator-manual/troubleshooting/#are-helm-releases-fine","text":"All Releases should be deployed . for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do export KUBECONFIG = kube_config_ $CLUSTER .yaml sops -d ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml > $KUBECONFIG helm list --all --all-namespaces shred $KUBECONFIG done","title":"Are Helm Releases fine?"},{"location":"operator-manual/troubleshooting/#is-cert-manager-doing-fine","text":"Are (Cluster)Issuers fine? All Resources should be READY=True or valid . for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do export KUBECONFIG = kube_config_ $CLUSTER .yaml sops -d ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml > $KUBECONFIG kubectl get clusterissuers,issuers,certificates,orders,challenges --all-namespaces shred $KUBECONFIG done","title":"Is cert-manager doing fine?"},{"location":"operator-manual/troubleshooting/#where-do-i-find-the-nodes-public-and-private-ip","text":"find . -name inventory.ini or for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ansible-inventory -i ${ CK8S_CONFIG_PATH } / ${ CLUSTER } -config/inventory.ini --list all done ansible_host is usually the public IP, while ip is usually the private IP.","title":"Where do I find the Nodes public and private IP?"},{"location":"operator-manual/troubleshooting/#node-cannot-be-access-via-ssh","text":"Important Make sure it is \"not you\". Are you well connected to the VPN? Is this the only Node which lost SSH access? Important If you are using Rook, it is usually set up with replication 2, which means it can tolerate one restarting Node. Make sure that, either Rook is healthy or that you are really sure you are restarting the right Node. Try connecting to the unhealthy Node via a different Node and internal IP: UNHEALTHY_NODE = 172 .0.10.205 # You lost access to this one JUMP_NODE = 89 .145.xxx.yyy # You have access to this one ssh -J ubuntu@ $JUMP_NODE ubuntu@ $UNHEALTHY_NODE Try rebooting the Node via cloud provider specific CLI: UNHEALTHY_NODE = cksc-worker-2 # Example for ExoScale exo vm reboot --force $UNHEALTHY_NODE If using Rook make sure its health goes back to HEALTH_OK .","title":"Node cannot be access via SSH"},{"location":"operator-manual/troubleshooting/#a-node-has-incorrect-time","text":"Incorrect time on a Node can have sever consequences with replication and monitoring. In fact, if you follow ISO 27001, A.12.4.4 Clock Synchronisation requires you to ensure clocks are synchronized. These days, Linux distributions should come out-of-the-box with timesyncd for time synchronization via NTP. To figure out what is wrong, SSH into the target Node and try the following: sudo systemctl status systemd-timesyncd sudo journalctl --unit systemd-timesyncd sudo timedatectl status sudo timedatectl timesync-status sudo timedatectl show-timesync Possible causes include incorrect NTP server settings, or NTP being blocked by firewall. For reminder, NTP works over UDP port 123.","title":"A Node has incorrect time"},{"location":"operator-manual/troubleshooting/#node-seems-not-fine","text":"Important If you are using Rook, it is usually set up with replication 2, which means it can tolerate one restarting Node. Make sure that, either Rook is healthy or that you are really sure you are restarting the right Node. Try rebooting the Node: UNHEALTHY_NODE = 89 .145.xxx.yyy ssh ubuntu@ $UNHEALTHY_NODE sudo reboot If using Rook make sure its health goes back to HEALTH_OK .","title":"Node seems not fine"},{"location":"operator-manual/troubleshooting/#node-seems-really-not-fine-i-want-a-new-one","text":"Is it 2AM? Do not replace Nodes, instead simply add a new one. You might run out of capacity, you might lose redundancy, you might replace the wrong Node. Prefer to add a Node and see if that solves the problem.","title":"Node seems really not fine. I want a new one."},{"location":"operator-manual/troubleshooting/#okay-i-want-to-add-a-new-node","text":"Prefer this option if you \"quickly\" need to add CPU, memory or storage (i.e., Rook) capacity. First, check for infrastructure drift, as shown here . Depending on your provider: Add a new Node by editing the *.tfvars . Re-apply Terraform. Re-create the inventory.ini (skip this step if the cluster is using a dynamic inventory). Re-apply Kubespray. Re-fix the Kubernetes API URL. Check that the new Node joined the cluster, as shown here .","title":"Okay, I want to add a new Node."},{"location":"operator-manual/troubleshooting/#a-systemd-unit-failed","text":"SSH into the Node. Check which systemd unit is failing: systemctl --failed Gather more information: FAILED_UNIT = fwupd-refresh.service systemctl status $FAILED_UNIT journalctl --unit $FAILED_UNIT","title":"A systemd unit failed"},{"location":"operator-manual/troubleshooting/#rook-seems-not-fine","text":"Please check the following upstream documents: Rook Common Issues Ceph Common Issues","title":"Rook seems not fine"},{"location":"operator-manual/troubleshooting/#pod-seems-not-fine","text":"Before starting, set up a handy environment: CLUSTER = cksc # Cluster containing the unhealthy Pod export KUBECONFIG = kube_config_ $CLUSTER .yaml sops -d ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml > $KUBECONFIG Check that you are on the right cluster: kubectl get nodes Find the name of the Pod which is not fine: kubectl get pod -A # Copy-paste the Pod and Pod namespace below UNHEALTHY_POD = prometheus-kube-prometheus-stack-prometheus-0 UNHEALTHY_POD_NAMESPACE = monitoring Gather some \"evidence\" for later diagnostics, when the heat is over: kubectl describe pod -n $UNHEALTHY_POD_NAMESPACE $UNHEALTHY_POD kubectl logs -n $UNHEALTHY_POD_NAMESPACE $UNHEALTHY_POD Try to kill and check if the underlying Deployment, StatefulSet or DaemonSet will restart it: kubectl delete pod -n $UNHEALTHY_POD_NAMESPACE $UNHEALTHY_POD kubectl get pod -A --watch","title":"Pod seems not fine"},{"location":"operator-manual/troubleshooting/#helm-release-is-failed","text":"Before starting, set up a handy environment: CLUSTER = cksc # Cluster containing the failed Release export KUBECONFIG = kube_config_ $CLUSTER .yaml sops -d ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml > $KUBECONFIG Check that you are on the right cluster: kubectl get nodes Find the failed Release: helm ls --all-namespaces --all FAILED_RELEASE = user-rbac FAILED_RELEASE_NAMESPACE = kube-system Just to make sure, do a drift check, as shown here . Remove the failed Release: helm uninstall -n $FAILED_RELEASE_NAMESPACE $FAILED_RELEASE Re-apply apps according to documentation.","title":"Helm Release is failed"},{"location":"operator-manual/troubleshooting/#cert-manager-is-not-fine","text":"Follow cert-manager's troubleshooting, specifically: Troubleshooting Troubleshooting Issuing ACME Certificates","title":"cert-manager is not fine"},{"location":"operator-manual/troubleshooting/#how-do-i-check-if-infrastructure-drifted-due-to-manual-intervention","text":"Go to the docs of the cloud provider and run Terraform plan instead of apply . For Exoscale, it looks as follows: TF_SCRIPTS_DIR = $( readlink -f compliantkubernetes-kubespray/kubespray/contrib/terraform/exoscale ) for CLUSTER in ${ SERVICE_CLUSTER } \" ${ WORKLOAD_CLUSTERS [@] } \" ; do pushd ${ TF_SCRIPTS_DIR } export TF_VAR_inventory_file = ${ CK8S_CONFIG_PATH } / ${ CLUSTER } -config/inventory.ini terraform init terraform plan \\ -var-file = ${ CK8S_CONFIG_PATH } / ${ CLUSTER } -config/cluster.tfvars \\ -state = ${ CK8S_CONFIG_PATH } / ${ CLUSTER } -config/terraform.tfstate popd done","title":"How do I check if infrastructure drifted due to manual intervention?"},{"location":"operator-manual/troubleshooting/#how-do-i-check-if-the-kubespray-setup-drifted-due-to-manual-intervention","text":"At the time of this writing, this cannot be done, but efforts are underway .","title":"How do I check if the Kubespray setup drifted due to manual intervention?"},{"location":"operator-manual/troubleshooting/#how-do-i-check-if-apps-drifted-due-to-manual-intervention","text":"# For service cluster ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ SERVICE_CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_sc.yaml ./compliantkubernetes-apps/bin/ck8s ops helmfile sc diff # Respond \"n\" if you get WARN # For the workload clusters for CLUSTER in \" ${ WORKLOAD_CLUSTERS [@] } \" ; do ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_wc.yaml ./compliantkubernetes-apps/bin/ck8s ops helmfile wc diff # Respond \"n\" if you get WARN done","title":"How do I check if apps drifted due to manual intervention?"},{"location":"operator-manual/troubleshooting/#velero-backup-stuck-in-progress","text":"Velero is known to get stuck InProgress when doing backups velero backup get NAME STATUS ERRORS WARNINGS CREATED EXPIRES STORAGE LOCATION SELECTOR velero-daily-backup-20211005143248 InProgress 0 0 2021 -10-05 14 :32:48 +0200 CEST 29d default !nobackup First try to delete the backup ./velero backup delete velero-daily-backup-20211005143248 Then kill all the pods under the velero namespace ./compliantkubernetes-apps/bin/ck8s ops kubectl wc delete pods -n velero --all Check that the backup is gone velero backup get NAME STATUS ERRORS WARNINGS CREATED EXPIRES STORAGE LOCATION SELECTOR Recreate the backup from a schedule velero backup create --from-schedule velero-daily-backup","title":"Velero backup stuck in progress"},{"location":"user-guide/","text":"User Guide Overview This guide is for users who manage application on top of Compliant Kubernetes. A user can be described via the following user stories: As a Continuous Integration (CI) pipeline, I want to push new container images to the container registry. As a Continuous Delivery (CD) pipeline, I want to push changes to the Compliant Kubernetes cluster, so that the new version of the application is running. As an application developer, I want to inspect how my application is running, so that I can take better development decisions. As a super user, I want to configure Role-Based Access Control (RBAC) to delegate access to application developers. As a super user, I want to configure NetworkPolicies, so that applications running in the same Compliant Kubernetes cluster are well zoned. As a super user, I want to configure Policies (e.g., \"do not use latest as a container tag), so as to avoid trivial mistakes. Note We suggest application developers to only perform changes to a production Compliant Kubernetes cluster via a Continuous Delivery Pipeline. This method, also known as GitOps, provides an audit log, review and testing of system changes for \"free\". This significantly facilitates complying with change management policies. Running Example To make the most out of Compliant Kubernetes, this documentation features a minimalistic NodeJS application. It allows to explore all Compliant Kubernetes benefits, including: Container registry ; Deploying ; Logging ; Metrics ; Alerts . The application features: some REST endpoints ( / , /users ); structured logging ; metrics endpoint ; Dockerfile , which showcases: How to run as non-root ; Helm Chart , which showcases: HTTPS Ingresses ; ServiceMonitor for metrics collection ; PrometheusRule for alerting ; resources for capacity management ; script for local development and testing ; Bonus: ability to make it crash ( /crash ). If you are a newcomer to Compliant Kubernetes, we suggest you clone the user demo as follows: git clone https://github.com/elastisys/compliantkubernetes/ cd compliantkubernetes/user-demo","title":"Overview"},{"location":"user-guide/#user-guide-overview","text":"This guide is for users who manage application on top of Compliant Kubernetes. A user can be described via the following user stories: As a Continuous Integration (CI) pipeline, I want to push new container images to the container registry. As a Continuous Delivery (CD) pipeline, I want to push changes to the Compliant Kubernetes cluster, so that the new version of the application is running. As an application developer, I want to inspect how my application is running, so that I can take better development decisions. As a super user, I want to configure Role-Based Access Control (RBAC) to delegate access to application developers. As a super user, I want to configure NetworkPolicies, so that applications running in the same Compliant Kubernetes cluster are well zoned. As a super user, I want to configure Policies (e.g., \"do not use latest as a container tag), so as to avoid trivial mistakes. Note We suggest application developers to only perform changes to a production Compliant Kubernetes cluster via a Continuous Delivery Pipeline. This method, also known as GitOps, provides an audit log, review and testing of system changes for \"free\". This significantly facilitates complying with change management policies.","title":"User Guide Overview"},{"location":"user-guide/#running-example","text":"To make the most out of Compliant Kubernetes, this documentation features a minimalistic NodeJS application. It allows to explore all Compliant Kubernetes benefits, including: Container registry ; Deploying ; Logging ; Metrics ; Alerts . The application features: some REST endpoints ( / , /users ); structured logging ; metrics endpoint ; Dockerfile , which showcases: How to run as non-root ; Helm Chart , which showcases: HTTPS Ingresses ; ServiceMonitor for metrics collection ; PrometheusRule for alerting ; resources for capacity management ; script for local development and testing ; Bonus: ability to make it crash ( /crash ). If you are a newcomer to Compliant Kubernetes, we suggest you clone the user demo as follows: git clone https://github.com/elastisys/compliantkubernetes/ cd compliantkubernetes/user-demo","title":"Running Example"},{"location":"user-guide/alerts/","text":"Alerts Compliant Kubernetes (CK8S) includes alerts via Alertmanager . Important By default, you will get some platform alerts, specifically those originating from the workload cluster. This may benefit you, by giving you improved \"situational awareness\". Please decide if these alerts are of interest to you or not. Feel free to silence them, as the Compliant Kubernetes administrator will take responsibility for them. Your focus should be on user alerts or application-level alerts , i.e., alerts under the control and responsibility of the Compliant Kubernetes user. We will focus on user alerts in this document. Compliance needs Many regulations require you to have an incident management process. Alerts help you discover abnormal application behavior that need attention. This maps to ISO 27001 \u2013 Annex A.16: Information Security Incident Management . Enabling user alerts User alerts are handled by a project called AlertManager , which needs to be enabled by the administrator. Get in touch with the administrator and they will be happy to help. Configuring user alerts User alerts are configured via the Secret alertmanager-alertmanager located in the alertmanager namespace. This configuration file is specified here . # retrieve the old configuration kubectl get -n alertmanager secret alertmanager-alertmanager -o jsonpath = '{.data.alertmanager\\.yaml}' | base64 -d > alertmanager.yaml # edit alertmanager.yaml as needed # re-apply the new configuration kubectl delete -n alertmanager secret alertmanager-alertmanager kubectl create secret generic -n alertmanager alertmanager-alertmanager --from-file = alertmanager.yaml Make sure to configure and test a receiver for you alerts, e.g., Slack or OpsGenie. Note If you get an access denied error, check with your Compliant Kubernetes administrator. Accessing user AlertManager If you want to access AlertManager, for example to confirm that its configuration was picked up correctly, proceed as follows: Type: kubectl proxy . Open this link in your browser. Setting up an alert Before setting up an alert, you must create a ServiceMonitor to collect metrics from your application. Then, create a PrometheusRule following the example below: apiVersion : monitoring.coreos.com/v1 kind : PrometheusRule metadata : creationTimestamp : null labels : prometheus : example role : alert-rules name : prometheus-example-rules spec : groups : - name : ./example.rules rules : - alert : ExampleAlert expr : vector(1) Running Example The user demo already includes a PrometheusRule , to configure an alert: {{ - if .Values.prometheusRule.enabled - }} apiVersion : monitoring.coreos.com/v1 kind : PrometheusRule metadata : name : {{ include \"ck8s-user-demo.fullname\" . }} labels : {{ - include \"ck8s-user-demo.labels\" . | nindent 4 }} spec : groups : - name : ./example.rules rules : - alert : ApplicationIsActuallyUsed expr : rate(http_request_duration_seconds_count[1m])>1 {{ - end }}","title":"Alerts"},{"location":"user-guide/alerts/#alerts","text":"Compliant Kubernetes (CK8S) includes alerts via Alertmanager . Important By default, you will get some platform alerts, specifically those originating from the workload cluster. This may benefit you, by giving you improved \"situational awareness\". Please decide if these alerts are of interest to you or not. Feel free to silence them, as the Compliant Kubernetes administrator will take responsibility for them. Your focus should be on user alerts or application-level alerts , i.e., alerts under the control and responsibility of the Compliant Kubernetes user. We will focus on user alerts in this document.","title":"Alerts"},{"location":"user-guide/alerts/#compliance-needs","text":"Many regulations require you to have an incident management process. Alerts help you discover abnormal application behavior that need attention. This maps to ISO 27001 \u2013 Annex A.16: Information Security Incident Management .","title":"Compliance needs"},{"location":"user-guide/alerts/#enabling-user-alerts","text":"User alerts are handled by a project called AlertManager , which needs to be enabled by the administrator. Get in touch with the administrator and they will be happy to help.","title":"Enabling user alerts"},{"location":"user-guide/alerts/#configuring-user-alerts","text":"User alerts are configured via the Secret alertmanager-alertmanager located in the alertmanager namespace. This configuration file is specified here . # retrieve the old configuration kubectl get -n alertmanager secret alertmanager-alertmanager -o jsonpath = '{.data.alertmanager\\.yaml}' | base64 -d > alertmanager.yaml # edit alertmanager.yaml as needed # re-apply the new configuration kubectl delete -n alertmanager secret alertmanager-alertmanager kubectl create secret generic -n alertmanager alertmanager-alertmanager --from-file = alertmanager.yaml Make sure to configure and test a receiver for you alerts, e.g., Slack or OpsGenie. Note If you get an access denied error, check with your Compliant Kubernetes administrator.","title":"Configuring user alerts"},{"location":"user-guide/alerts/#accessing-user-alertmanager","text":"If you want to access AlertManager, for example to confirm that its configuration was picked up correctly, proceed as follows: Type: kubectl proxy . Open this link in your browser.","title":"Accessing user AlertManager"},{"location":"user-guide/alerts/#setting-up-an-alert","text":"Before setting up an alert, you must create a ServiceMonitor to collect metrics from your application. Then, create a PrometheusRule following the example below: apiVersion : monitoring.coreos.com/v1 kind : PrometheusRule metadata : creationTimestamp : null labels : prometheus : example role : alert-rules name : prometheus-example-rules spec : groups : - name : ./example.rules rules : - alert : ExampleAlert expr : vector(1)","title":"Setting up an alert"},{"location":"user-guide/alerts/#running-example","text":"The user demo already includes a PrometheusRule , to configure an alert: {{ - if .Values.prometheusRule.enabled - }} apiVersion : monitoring.coreos.com/v1 kind : PrometheusRule metadata : name : {{ include \"ck8s-user-demo.fullname\" . }} labels : {{ - include \"ck8s-user-demo.labels\" . | nindent 4 }} spec : groups : - name : ./example.rules rules : - alert : ApplicationIsActuallyUsed expr : rate(http_request_duration_seconds_count[1m])>1 {{ - end }}","title":"Running Example"},{"location":"user-guide/backup/","text":"Backups Compliant Kubernetes (CK8S) includes backup functionality through Velero, a backup tool for Kubernetes Resources and Persistent Volumes. For backup of container images, Harbor is used instead. Compliance needs The requirements to comply with ISO 27001 are stated in ISO 27001:2013 . The annexes that are relevant to backups are: Annex 12 , article A.12.3.1 \"Information Backup\". What is Velero? Velero is an open source, cloud native tool for backing up and migrating Kubernetes Resources and Persistent Volumes. It has been developed by VMware since 2017. It allows for both manual and scheduled backups, and also allows for subsets of Resources in a cluster to be backed up rather than necessarily backing up everything. Usage Velero is deployed in both the workload cluster and the service cluster. Following are instructions for backing up and restoring resources. Backing up Velero takes a daily backup of all Kubernetes Resources in all user namespaces. Persistent Volumes will be backed up if they are tied to a Pod. If backups are not wanted the label compliantkubernetes.io/nobackup can be added to opt-out of the daily backups. In the service cluster, Grafana (and its associated Persistent Volume) is configured to be backed up. Backups are stored for 720 hours (30 days). Restoring Restoring from a backup with Velero is meant to be a type of disaster recovery. Velero will not overwrite existing Resources when restoring. As such, if you want to restore the state of a Resource that is still running, the Resource must be deleted first. To restore a backup on demand, contact your Compliant Kubernetes administrator.","title":"Backups"},{"location":"user-guide/backup/#backups","text":"Compliant Kubernetes (CK8S) includes backup functionality through Velero, a backup tool for Kubernetes Resources and Persistent Volumes. For backup of container images, Harbor is used instead.","title":"Backups"},{"location":"user-guide/backup/#compliance-needs","text":"The requirements to comply with ISO 27001 are stated in ISO 27001:2013 . The annexes that are relevant to backups are: Annex 12 , article A.12.3.1 \"Information Backup\".","title":"Compliance needs"},{"location":"user-guide/backup/#what-is-velero","text":"Velero is an open source, cloud native tool for backing up and migrating Kubernetes Resources and Persistent Volumes. It has been developed by VMware since 2017. It allows for both manual and scheduled backups, and also allows for subsets of Resources in a cluster to be backed up rather than necessarily backing up everything.","title":"What is Velero?"},{"location":"user-guide/backup/#usage","text":"Velero is deployed in both the workload cluster and the service cluster. Following are instructions for backing up and restoring resources.","title":"Usage"},{"location":"user-guide/backup/#backing-up","text":"Velero takes a daily backup of all Kubernetes Resources in all user namespaces. Persistent Volumes will be backed up if they are tied to a Pod. If backups are not wanted the label compliantkubernetes.io/nobackup can be added to opt-out of the daily backups. In the service cluster, Grafana (and its associated Persistent Volume) is configured to be backed up. Backups are stored for 720 hours (30 days).","title":"Backing up"},{"location":"user-guide/backup/#restoring","text":"Restoring from a backup with Velero is meant to be a type of disaster recovery. Velero will not overwrite existing Resources when restoring. As such, if you want to restore the state of a Resource that is still running, the Resource must be deleted first. To restore a backup on demand, contact your Compliant Kubernetes administrator.","title":"Restoring"},{"location":"user-guide/ci-cd/","text":"CI/CD Integration Compliant Kubernetes does not come with a CI/CD solution. Fortunately, it can be easily integrated with your existing CI/CD solution. Important Access control is an extremely important topic for passing an audit for compliance with data privacy and data security regulations. For example, Swedish patient data law requires all persons to be identified with individual credentials and that logs should capture who did what. Therefore, Compliant Kubernetes has put significant thought into how to do proper access control. As a consequence, CI/CD solutions that require cluster-wide permissions and/or introduce their own notion of access control are highly discouraged. Make sure you thoroughly evaluate your CI/CD solution with your CISO before investing in it. Background For the purpose of Compliant Kubernetes, one can distinguish between two \"styles\" of CI/CD: push-style and pull-style. Push-style CI/CD -- like GitLab CI or GitHub Actions -- means that a commit will trigger some commands on a CI/CD worker, which will push changes into the Compliant Kubernetes cluster. The CI/CD worker generally runs outside the Kubernetes cluster. Push-style CI/CD solutions should work out-of-the-box and require no special considerations for Compliant Kubernetes. Pull-styles CI/CD -- like ArgoCD or Flux -- means that a special controller is installed inside the cluster, which monitors a Git repository. When a change is detected the controller \"pulls\" changes into the cluster from the Git repository. The special controller often requires considerable permissions and introduces a new notion of access control, which is problematic from a compliance perspective. As shown below, some pull-style CI/CD solutions can be used with Compliant Kubernetes, others not. Push-style CI/CD Please follow this tutorial to create the relevant credentials. For improved access control, make sure the Role you create gets the least permissions possible. For example, if your application only consists of a Deployment, Service and Ingress, those should be the only resources available to the Role. The user-token must be treated as a secret and injected into the CI/CD pipeline via a proper secrets handing feature, such as GitLab CI's protected variable and GitHub Action's secrets . ArgoCD By default, ArgoCD installs a ClusterRole with wide permissions , which can be used to bypass Compliant Kubernetes's access control. Using it as-is might be non-compliant with various regulations. Instead, edit the default ArgoCD manifest to create a very restricted Role that only operates in the target namespace. Flux v1 Flux v1 is in maintenance mode and might become obsolete soon. Flux v2 Flux v2 brings is own notion of access control and requires special considerations to ensure it obey Compliant Kubernetes access control. Installing it can only be done by the administrator of the Compliant Kubernetes cluster, after having made a thorough risk-reward analysis. At the time of this writing, due to these special considerations, we discourage Flux v2.","title":"CI/CD"},{"location":"user-guide/ci-cd/#cicd-integration","text":"Compliant Kubernetes does not come with a CI/CD solution. Fortunately, it can be easily integrated with your existing CI/CD solution. Important Access control is an extremely important topic for passing an audit for compliance with data privacy and data security regulations. For example, Swedish patient data law requires all persons to be identified with individual credentials and that logs should capture who did what. Therefore, Compliant Kubernetes has put significant thought into how to do proper access control. As a consequence, CI/CD solutions that require cluster-wide permissions and/or introduce their own notion of access control are highly discouraged. Make sure you thoroughly evaluate your CI/CD solution with your CISO before investing in it.","title":"CI/CD Integration"},{"location":"user-guide/ci-cd/#background","text":"For the purpose of Compliant Kubernetes, one can distinguish between two \"styles\" of CI/CD: push-style and pull-style. Push-style CI/CD -- like GitLab CI or GitHub Actions -- means that a commit will trigger some commands on a CI/CD worker, which will push changes into the Compliant Kubernetes cluster. The CI/CD worker generally runs outside the Kubernetes cluster. Push-style CI/CD solutions should work out-of-the-box and require no special considerations for Compliant Kubernetes. Pull-styles CI/CD -- like ArgoCD or Flux -- means that a special controller is installed inside the cluster, which monitors a Git repository. When a change is detected the controller \"pulls\" changes into the cluster from the Git repository. The special controller often requires considerable permissions and introduces a new notion of access control, which is problematic from a compliance perspective. As shown below, some pull-style CI/CD solutions can be used with Compliant Kubernetes, others not.","title":"Background"},{"location":"user-guide/ci-cd/#push-style-cicd","text":"Please follow this tutorial to create the relevant credentials. For improved access control, make sure the Role you create gets the least permissions possible. For example, if your application only consists of a Deployment, Service and Ingress, those should be the only resources available to the Role. The user-token must be treated as a secret and injected into the CI/CD pipeline via a proper secrets handing feature, such as GitLab CI's protected variable and GitHub Action's secrets .","title":"Push-style CI/CD"},{"location":"user-guide/ci-cd/#argocd","text":"By default, ArgoCD installs a ClusterRole with wide permissions , which can be used to bypass Compliant Kubernetes's access control. Using it as-is might be non-compliant with various regulations. Instead, edit the default ArgoCD manifest to create a very restricted Role that only operates in the target namespace.","title":"ArgoCD"},{"location":"user-guide/ci-cd/#flux-v1","text":"Flux v1 is in maintenance mode and might become obsolete soon.","title":"Flux v1"},{"location":"user-guide/ci-cd/#flux-v2","text":"Flux v2 brings is own notion of access control and requires special considerations to ensure it obey Compliant Kubernetes access control. Installing it can only be done by the administrator of the Compliant Kubernetes cluster, after having made a thorough risk-reward analysis. At the time of this writing, due to these special considerations, we discourage Flux v2.","title":"Flux v2"},{"location":"user-guide/demarcation/","text":"Can I? It's not that I don't trust you. Rather, I don't want you to worry. Compliant Kubernetes comes with a lot of safeguards to ensure you protect your business reputation and earn the trust of your customers. Furthermore, it is a good idea to keep regulators happy, since they bring public trust into digitalization. Public trust is necessary to shift customers away from pen-and-paper to drive usage of your amazing application. If you used Kubernetes before, especially if you acted as a Kubernetes administrator, then being a Compliant Kubernetes user might feel a bit limiting. For example, you might not be able to run containers with root ( uid=0 ) as you were used to. Again, these are not limitations, rather safeguards. Why? As previously reported, Kubernetes is not secure by default, nor by itself . This is due to the fact that Kubernetes prefers to keep its \"wow, it just works\" experience. This might be fine for a company that does not process personal data. However, if you are in a regulated industry, for example, because you process personal data or health information, your regulators will be extremely unhappy to learn that your platform does not conform to security best practices. In case of Compliant Kubernetes this implies a clear separation of roles and responsibilities between Compliant Kubernetes users and administrators. The mission of administrators is to make you, the Compliant Kubernetes user, succeed. Besides allowing you to develop features as fast as possible, the administrator also needs to ensure that you build on top of a platform that lives up to regulatory requirements, specifically data privacy and data security regulations. General Principle Compliant Kubernetes does not allow users to make any changes which may compromise the security of the platform. This includes compromising or working around access control, logging, monitoring, backups, alerting, etc. For example, accidental deletion of the CustomResourceDefinitions of Prometheus would prevent administrators from getting alerts and fixing cluster issues before your application is impacted. Similarly, accidentally deleting fluentd Pods would make it impossible to capture the Kubernetes audit log and investigate data breaches. Specifics To stick to the general principles above, Compliant Kubernetes puts the following technical safeguards. This list may be updated in the future to take into account the fast evolving risk and technological landscape. More technically, Compliant Kubernetes does not allow users to: change the Kubernetes API through CustomResourceDefinitions or Dynamic Webhooks ; gain more container execution permissions by mutating PodSecurityPolicies ; this implies that you cannot run container images as root or mount hostPaths ; mutate ClusterRoles or Roles so as to escalate privileges ; mutate Kubernetes resources in administrator-owned namespaces, such as monitoring or kube-system ; re-configure system Pods, such as Prometheus or fluentd; access the hosts directly. But what if I really need to? Unfortunately, many application asks for more permissions than Compliant Kubernetes allows by default. When looking at the Kubernetes resources, the following are problematic: ClusterRoles, ClusterRoleBindings Too permissive Roles and RoleBindings PodSecurityPolicy and/or use of privileged PodSecurityPolicy CustomResourceDefinitions WebhookConfiguration In such a case, ask your administrator to make a risk-reward analysis. As long as they stick to the general principles, this should be fine. However, as much as they want to help, they might not be allowed to say \"yes\". Remember, administrators are there to help you focus on application development, but at the same time they are responsible to protect your application against security risks.","title":"Can I?"},{"location":"user-guide/demarcation/#can-i","text":"It's not that I don't trust you. Rather, I don't want you to worry. Compliant Kubernetes comes with a lot of safeguards to ensure you protect your business reputation and earn the trust of your customers. Furthermore, it is a good idea to keep regulators happy, since they bring public trust into digitalization. Public trust is necessary to shift customers away from pen-and-paper to drive usage of your amazing application. If you used Kubernetes before, especially if you acted as a Kubernetes administrator, then being a Compliant Kubernetes user might feel a bit limiting. For example, you might not be able to run containers with root ( uid=0 ) as you were used to. Again, these are not limitations, rather safeguards.","title":"Can I?"},{"location":"user-guide/demarcation/#why","text":"As previously reported, Kubernetes is not secure by default, nor by itself . This is due to the fact that Kubernetes prefers to keep its \"wow, it just works\" experience. This might be fine for a company that does not process personal data. However, if you are in a regulated industry, for example, because you process personal data or health information, your regulators will be extremely unhappy to learn that your platform does not conform to security best practices. In case of Compliant Kubernetes this implies a clear separation of roles and responsibilities between Compliant Kubernetes users and administrators. The mission of administrators is to make you, the Compliant Kubernetes user, succeed. Besides allowing you to develop features as fast as possible, the administrator also needs to ensure that you build on top of a platform that lives up to regulatory requirements, specifically data privacy and data security regulations.","title":"Why?"},{"location":"user-guide/demarcation/#general-principle","text":"Compliant Kubernetes does not allow users to make any changes which may compromise the security of the platform. This includes compromising or working around access control, logging, monitoring, backups, alerting, etc. For example, accidental deletion of the CustomResourceDefinitions of Prometheus would prevent administrators from getting alerts and fixing cluster issues before your application is impacted. Similarly, accidentally deleting fluentd Pods would make it impossible to capture the Kubernetes audit log and investigate data breaches.","title":"General Principle"},{"location":"user-guide/demarcation/#specifics","text":"To stick to the general principles above, Compliant Kubernetes puts the following technical safeguards. This list may be updated in the future to take into account the fast evolving risk and technological landscape. More technically, Compliant Kubernetes does not allow users to: change the Kubernetes API through CustomResourceDefinitions or Dynamic Webhooks ; gain more container execution permissions by mutating PodSecurityPolicies ; this implies that you cannot run container images as root or mount hostPaths ; mutate ClusterRoles or Roles so as to escalate privileges ; mutate Kubernetes resources in administrator-owned namespaces, such as monitoring or kube-system ; re-configure system Pods, such as Prometheus or fluentd; access the hosts directly.","title":"Specifics"},{"location":"user-guide/demarcation/#but-what-if-i-really-need-to","text":"Unfortunately, many application asks for more permissions than Compliant Kubernetes allows by default. When looking at the Kubernetes resources, the following are problematic: ClusterRoles, ClusterRoleBindings Too permissive Roles and RoleBindings PodSecurityPolicy and/or use of privileged PodSecurityPolicy CustomResourceDefinitions WebhookConfiguration In such a case, ask your administrator to make a risk-reward analysis. As long as they stick to the general principles, this should be fine. However, as much as they want to help, they might not be allowed to say \"yes\". Remember, administrators are there to help you focus on application development, but at the same time they are responsible to protect your application against security risks.","title":"But what if I really need to?"},{"location":"user-guide/getting-started/","text":"Getting Started As a user, you will need: docker helm kubectl oidc-login ; we suggest installing it via krew The easier is to request a demo environment from a managed Compliant Kubernetes provider . You should receive: URLs for Compliant Kubernetes UI components, such as the dashboard, container registry, logs, etc. A kubeconfig file for configuring kubectl access to the cluster. (Optionally) Static username and password. Normally, you should log in via a username and a password of your organizations identity provider. If you want to setup your own Compliant Kubernetes installation, head to the Administrator Manual . Running Example Verification Make sure you configure your environment properly: export KUBECONFIG=path/of/kubeconfig.yaml # leave empty if you use the default of ~/.kube/config export DOMAIN= # the domain you received from the administrator To verify if the required tools are installed and work as expected, type: docker version kubectl version --client helm version # You should see the version number of installed tools and no errors. To verify the received KUBECONFIG, type: # Notice that you will be asked to complete browser-based single sign-on kubectl get nodes # You should see the Nodes of your Kubernetes cluster To verify the received URLs, type: curl --head https://dex. $DOMAIN /healthz curl --include https://harbor. $DOMAIN /api/v2.0/health curl --head https://grafana. $DOMAIN /healthz curl --head https://kibana. $DOMAIN /api/status","title":"Getting Started"},{"location":"user-guide/getting-started/#getting-started","text":"As a user, you will need: docker helm kubectl oidc-login ; we suggest installing it via krew The easier is to request a demo environment from a managed Compliant Kubernetes provider . You should receive: URLs for Compliant Kubernetes UI components, such as the dashboard, container registry, logs, etc. A kubeconfig file for configuring kubectl access to the cluster. (Optionally) Static username and password. Normally, you should log in via a username and a password of your organizations identity provider. If you want to setup your own Compliant Kubernetes installation, head to the Administrator Manual .","title":"Getting Started"},{"location":"user-guide/getting-started/#running-example","text":"","title":"Running Example"},{"location":"user-guide/getting-started/#verification","text":"Make sure you configure your environment properly: export KUBECONFIG=path/of/kubeconfig.yaml # leave empty if you use the default of ~/.kube/config export DOMAIN= # the domain you received from the administrator To verify if the required tools are installed and work as expected, type: docker version kubectl version --client helm version # You should see the version number of installed tools and no errors. To verify the received KUBECONFIG, type: # Notice that you will be asked to complete browser-based single sign-on kubectl get nodes # You should see the Nodes of your Kubernetes cluster To verify the received URLs, type: curl --head https://dex. $DOMAIN /healthz curl --include https://harbor. $DOMAIN /api/v2.0/health curl --head https://grafana. $DOMAIN /healthz curl --head https://kibana. $DOMAIN /api/status","title":"Verification"},{"location":"user-guide/kubernetes-api/","text":"Kubernetes API The Kubernetes API is the entrypoint to managing your Kubernetes resources. Your Compliant Kubernetes administrator will provide you with a kubeconfig file upon onboarding, which is required to access the API. The following sections describe how to access the cluster in order to manage your Kubernetes resources. Authentication and Access Control in Compliant Kubernetes In order to facilitate access control and audit logging, Compliant Kubernetes imposes a certain way to access the Kubernetes API. The kubeconfig file provides individual access to the Kubernetes API through dex . Normally, you should authenticate using your organizations identity provider connected to dex, but it is also possible for your administrator to configure static usernames and passwords. The authorization is done by the Kubernetes API based on Kubernetes role-based access controls . Your cluster administrator will grant you permissions as part of onboarding. You have administrator access to the user workload Kubernetes Namespaces by default. In order to follow the principle of least privilege , you as an user should only have sufficient access to manage resources required by your application. User access to the Kubernetes API may need to be restricted from case to case to follow the principle of least privilege. Note Regardless of your privilege, you will not be able to see components such as Harbor and Elasticsearch via the Kubernetes API. This is in order to comply with common logging policies, which requires logging to be sent to a tamper-proof environment. The tamper-proof environment needs to be separated from the production cluster. Usage guide This section focuses on using the kubeconfig. Using the kubeconfig file The kubeconfig file can be used with kubectl by: Setting and exporting the KUBECONFIG environment variable: Merging the configuration with your existing kubeconfig file, see Kubernetes documentation on merging kubeconfig files . Authenticating to the Kubernetes API To authenticate to the Kubernetes API, run a kubectl command. The oidc-login plugin will launch a browser where you log in to the cluster: This page contains the authentication options provided by your administrator. Select your log in method and log in: Once you have logged in through the browser, you are authenticated to the cluster: Your credentials will then be used by the Kubernetes API to make sure you are authorized. You are now logged in and can use kubectl to manage your Kubernetes resources! Running Example Pre-verification Make sure you are in the right namespace on the right cluster: kubectl get nodes kubectl config view --minify --output 'jsonpath={..namespace}' ; echo Configure an Image Pull Secret To start, make sure you configure the Kubernetes cluster with an image pull secret. Ideally, you should create a container registry Robot Account , which only has pull permissions and use its token. Important Using your own registry credentials as an image pull secret, instead of creating a robot account, is against best practices and may violate data privacy regulations. Your registry credentials identify you and allow you to both push and pull images. A robot account should identify the Kubernetes cluster and be only allowed to pull images. DOCKER_USER = # enter robot account name DOCKER_PASSWORD = # enter robot token Now create a pull secret and (optionally) use it by default in the current namespace. # Create a pull secret kubectl create secret docker-registry pull-secret \\ --docker-server = harbor. $DOMAIN \\ --docker-username = $DOCKER_USER \\ --docker-password = $DOCKER_PASSWORD # Set default pull secret in current namespace kubectl patch serviceaccount default -p '{\"imagePullSecrets\": [{\"name\": \"pull-secret\"}]}' Note For each Kubernetes namespace, you will have to create an image pull secret and configure it to be default. Aim to have a one-to-one-to-one mapping between Kubernetes namespaces, container registry projects and robot accounts. Deploy user demo Example Here is an example Helm Chart to get you started. If you haven't done so already, clone the user demo and ensure you are in the right folder: git clone https://github.com/elastisys/compliantkubernetes/ cd compliantkubernetes/user-demo Ensure you use the right registry project and image tag, i.e., those that you pushed in the previous example : REGISTRY_PROJECT = demo TAG = v1 You are ready to deploy the application. helm upgrade \\ --install \\ myapp \\ deploy/ck8s-user-demo/ \\ --set image.repository = harbor. $DOMAIN / $REGISTRY_PROJECT /ck8s-user-demo \\ --set image.tag = $TAG \\ --set ingress.hostname = demo. $DOMAIN Verification Verify that the application was deployed successfully: kubectl get pods # Wait until the status of your Pod is Running. Verify that the certificate was issued successfully: kubectl get certificate # Wait until your certificate shows READY True. Verify that your application is online. You may use your browser or curl : curl --include https://demo. $DOMAIN # First line should be HTTP/2 200 Do not expose $DOMAIN to your users. Although your administrator will set *.$DOMAIN to point to your applications, prefer to buy a branded domain. For example, register the domain myapp.com and point it via a CNAME or ALIAS record to myapp.$DOMAIN . Further reading dex on GitHub oidc-login/kubelogin on GitHub Organizing Cluster Access Using kubeconfig Files","title":"Kubernetes API"},{"location":"user-guide/kubernetes-api/#kubernetes-api","text":"The Kubernetes API is the entrypoint to managing your Kubernetes resources. Your Compliant Kubernetes administrator will provide you with a kubeconfig file upon onboarding, which is required to access the API. The following sections describe how to access the cluster in order to manage your Kubernetes resources.","title":"Kubernetes API"},{"location":"user-guide/kubernetes-api/#authentication-and-access-control-in-compliant-kubernetes","text":"In order to facilitate access control and audit logging, Compliant Kubernetes imposes a certain way to access the Kubernetes API. The kubeconfig file provides individual access to the Kubernetes API through dex . Normally, you should authenticate using your organizations identity provider connected to dex, but it is also possible for your administrator to configure static usernames and passwords. The authorization is done by the Kubernetes API based on Kubernetes role-based access controls . Your cluster administrator will grant you permissions as part of onboarding. You have administrator access to the user workload Kubernetes Namespaces by default. In order to follow the principle of least privilege , you as an user should only have sufficient access to manage resources required by your application. User access to the Kubernetes API may need to be restricted from case to case to follow the principle of least privilege. Note Regardless of your privilege, you will not be able to see components such as Harbor and Elasticsearch via the Kubernetes API. This is in order to comply with common logging policies, which requires logging to be sent to a tamper-proof environment. The tamper-proof environment needs to be separated from the production cluster.","title":"Authentication and Access Control in Compliant Kubernetes"},{"location":"user-guide/kubernetes-api/#usage-guide","text":"This section focuses on using the kubeconfig.","title":"Usage guide"},{"location":"user-guide/kubernetes-api/#using-the-kubeconfig-file","text":"The kubeconfig file can be used with kubectl by: Setting and exporting the KUBECONFIG environment variable: Merging the configuration with your existing kubeconfig file, see Kubernetes documentation on merging kubeconfig files .","title":"Using the kubeconfig file"},{"location":"user-guide/kubernetes-api/#authenticating-to-the-kubernetes-api","text":"To authenticate to the Kubernetes API, run a kubectl command. The oidc-login plugin will launch a browser where you log in to the cluster: This page contains the authentication options provided by your administrator. Select your log in method and log in: Once you have logged in through the browser, you are authenticated to the cluster: Your credentials will then be used by the Kubernetes API to make sure you are authorized. You are now logged in and can use kubectl to manage your Kubernetes resources!","title":"Authenticating to the Kubernetes API"},{"location":"user-guide/kubernetes-api/#running-example","text":"","title":"Running Example"},{"location":"user-guide/kubernetes-api/#pre-verification","text":"Make sure you are in the right namespace on the right cluster: kubectl get nodes kubectl config view --minify --output 'jsonpath={..namespace}' ; echo","title":"Pre-verification"},{"location":"user-guide/kubernetes-api/#configure-an-image-pull-secret","text":"To start, make sure you configure the Kubernetes cluster with an image pull secret. Ideally, you should create a container registry Robot Account , which only has pull permissions and use its token. Important Using your own registry credentials as an image pull secret, instead of creating a robot account, is against best practices and may violate data privacy regulations. Your registry credentials identify you and allow you to both push and pull images. A robot account should identify the Kubernetes cluster and be only allowed to pull images. DOCKER_USER = # enter robot account name DOCKER_PASSWORD = # enter robot token Now create a pull secret and (optionally) use it by default in the current namespace. # Create a pull secret kubectl create secret docker-registry pull-secret \\ --docker-server = harbor. $DOMAIN \\ --docker-username = $DOCKER_USER \\ --docker-password = $DOCKER_PASSWORD # Set default pull secret in current namespace kubectl patch serviceaccount default -p '{\"imagePullSecrets\": [{\"name\": \"pull-secret\"}]}' Note For each Kubernetes namespace, you will have to create an image pull secret and configure it to be default. Aim to have a one-to-one-to-one mapping between Kubernetes namespaces, container registry projects and robot accounts.","title":"Configure an Image Pull Secret"},{"location":"user-guide/kubernetes-api/#deploy-user-demo","text":"Example Here is an example Helm Chart to get you started. If you haven't done so already, clone the user demo and ensure you are in the right folder: git clone https://github.com/elastisys/compliantkubernetes/ cd compliantkubernetes/user-demo Ensure you use the right registry project and image tag, i.e., those that you pushed in the previous example : REGISTRY_PROJECT = demo TAG = v1 You are ready to deploy the application. helm upgrade \\ --install \\ myapp \\ deploy/ck8s-user-demo/ \\ --set image.repository = harbor. $DOMAIN / $REGISTRY_PROJECT /ck8s-user-demo \\ --set image.tag = $TAG \\ --set ingress.hostname = demo. $DOMAIN","title":"Deploy user demo"},{"location":"user-guide/kubernetes-api/#verification","text":"Verify that the application was deployed successfully: kubectl get pods # Wait until the status of your Pod is Running. Verify that the certificate was issued successfully: kubectl get certificate # Wait until your certificate shows READY True. Verify that your application is online. You may use your browser or curl : curl --include https://demo. $DOMAIN # First line should be HTTP/2 200 Do not expose $DOMAIN to your users. Although your administrator will set *.$DOMAIN to point to your applications, prefer to buy a branded domain. For example, register the domain myapp.com and point it via a CNAME or ALIAS record to myapp.$DOMAIN .","title":"Verification"},{"location":"user-guide/kubernetes-api/#further-reading","text":"dex on GitHub oidc-login/kubelogin on GitHub Organizing Cluster Access Using kubeconfig Files","title":"Further reading"},{"location":"user-guide/logs/","text":"Logging Compliant Kubernetes (CK8s) provides the mechanism to manage your cluster as well as the lifecycle of thousands of containerized applications deployed in the cluster. The resources managed by CK8s are expected to be highly distributed with dynamic behaviors. An instance of CK8s cluster environment involves several components with nodes that host hundreds of containers that are constantly being spun up and destroyed based on workloads. When dealing with a large pool of containerized applications and workloads in CK8s, it is imperative to be proactive with continuous monitoring and debugging information in order to observe what is going on the cluster. These information can be seen at the container, node, or cluster level. Logging as one of the three pillars of observability is a crucial element to manage and monitor services and infrastructure. It allows you to track debugging information at different levels of granularity. Compliance needs The requirements to comply with ISO 27001 are stated in ISO 27001:2013 . The annexes that mostly concerns logging are: Annex 12 , article A.12.4.1 \"Event Logging\" and A.12.4.3 \"Administrator and Operator Logs\". Annex 16 which deals with incident management. In Compliant Kubernetes, Elasticsearch is separate from the production workload, hence it complies with A.12.4.2 \"Protection of Log Information\". The cloud provider should ensure that the clock of Kubernetes nodes is synchronized, hence complying with A.12.4.4 \"Clock Synchronisation\". Open Distro for Elasticsearch Raw logs in CK8s are normalized, filtered, and processed by fluentd and shipped to Elasticsearch for storage and analysis. CK8s uses fully open source version of Elasticsearch called Open Distro for Elasticsearch . Open Distro for Elasticsearch provides a powerful, easy-to-use event monitoring and alerting system, enabling you to monitor, search, visualize your data among other things. Kibana is used as visualization and analysis interface for Elasticsearch for all your logs. Visualization using Kibana Kibana is used as a data visualization and exploration tool for log time-series and aggregate analytics. It offers powerful and easy-to-use features such as histograms, line graphs, pie charts, heat maps, and built-in geospatial support. When you log into Kibana, you will encounter a page similar to the one shown below. Since we are concerned with searching logs and their visualization, we will focus on Visualize and Explore Data as indicated by the red rectangle in the figure above. If you are interested to know more about the rest please visit the official Kibana documentation . Before we dive into Kibana, let us discuss the type of logs ingested into Elasticsearch. Logs in CK8s cluster are filtered and indexed by fluentd into three categories: kubeaudit logs related to Kubernetes audits to provide security-relevant chronological set of records documenting the sequence of activities that have affected system by individual users, administrators or other components of the system. This is mostly related to the ISO 27001 requirement A.12.4.3 \"Administrator and Operator Logs\". Kubernetes logs that provide insight into CK8s resources such as nodes, Pods, containers, deployments and replica sets. This allows you to observe the interactions between those resources and see the effects that one action has on another. Generally, logs in the CK8s ecosystem can be divided into the cluster level (logs outputted by components such as the kubelet, the API server, the scheduler) and the application level (logs generated by pods and containers). This is mostly related to the ISO 27001 requirement A.12.4.3 \"Administrator and Operator Logs\". Others logs other than the above two are indexed and shipped to Elasticsearch as others . Such logs are basically your application level logs. This is mostly related to the ISO 27001 requirement A.12.4.1 \"Event Logging\". Let us dive into Kibana then. Data Visualization and Exploration in Kibana As you can see in the figure above, data visualzation and expoloration in Kibana has three components: Discover , Visualize and Dashboard . The following section describes each components using examples. Discover The discover component in Kibana is used for exploring, searching and filtering logs. Click Discover in the main Kibana page to access the features provided by it. The figure below shows partial view of the page that you will get under Discover . As you can see in the above figure, the kubeaudit index logs are loaded by default. If you want to explore logs from either of the other two log indices please select the right index under the dropdown menu marked log index category . To appreciate Kibana's searching and filtering capability, let us get data for the following question: Get all logs that were collected for the past 20 hours in host 172.16.0.3 where the responseStatus reason is notfound We can use different ways to find the answer for the question. Below is one possible solution. Write sourceIPs: 172.16.0.3 in the search textbox . Click Add Filter and select responseStatus.reason and is under field and Operator dropdown menus respectively. Finally, enter notfound under Value input box and click Save . The following figure shows the details. To enter the 20 hours, click part that is labelled Time in the Discover figure above, then enter 20 under the input box and select hours in the dropdown menu. Make sure that you are under Relative tab. Finally, click update . The following figure shows how to set the hours. Note that the data will be automatically updated as time passes to reflect the past 20 hours data from the current time. Once you are done, you will see a result similar to the following figure. Visualize The Visualize component in Kibana is to create different visualizations. Let us create a couple of visualizations. To create visualizations: Go to the main Kibana page and click Visual . Click Create visualization link located on the top right side of the page. Select a visualization type, we will use Pie here. Choose the index name or saved query name, if any, under New Pie / Choose a source . We will use the Kubernetes index here. By default a pie chart with the total number of logs will be provided by Kibana. Let us divide the pie chart based on the number of logs contributed by each namespace . To do that perform the following steps: Under Buckets click add then Split Slices . See the figure below. Under aggregation select Significant Terms terms. see the figure below. Select Kubernetes.namespace_name.keyword under field . See the figure below. The final result will look like the following figure. Please save the pie chart as we will use it later. Let us create a similar pie chart using host instead of namespace . The chart will look like the following figure. Dashboard The Dashboard component in Kibana is used for organizing related visualizations together. Let us bring the two visualizations that we created above together in a single dashboard. To do that: Go to the main Kibana page and click Dashboard . Click Create Dashboard link located on the top right side of the page. Click Add existing link located on the left side. Select the name of the two charts/visualizations that you created above. The figure below shows the dashboard generated from the above steps showing the two pie charts in a single page. Accessing Falco and OPA Logs To access Falco or OPA logs, go to the Discover panel and write Falco or OPA on the search textbox . Make sure that the Kubernetes log index category is selected. The figure below shows the search result for Falco logs. The figure below shows the search result for OPA logs. Running Example The user demo application already includes structured logging: For each HTTP request, it logs the URL, the user agent, etc. Compliant Kubernetes further adds the Pod name, Helm Chart name, Helm Release name, etc. to each log entry. The screenshot below gives an example of log entries produced by the user demo application. It was obtained by using the index pattern kubernetes* and the filter kubernetes.labels.app_kubernetes_io/instance:myapp . Note You may want to save frequently used searches as dashboards. Compliant Kubernetes saves and backs these up for you. Further Reading Open Distro for Elasticsearch Kibana Open Distro for Elasticsearch \u2013 How Different Is It? Fluentd","title":"Logs"},{"location":"user-guide/logs/#logging","text":"Compliant Kubernetes (CK8s) provides the mechanism to manage your cluster as well as the lifecycle of thousands of containerized applications deployed in the cluster. The resources managed by CK8s are expected to be highly distributed with dynamic behaviors. An instance of CK8s cluster environment involves several components with nodes that host hundreds of containers that are constantly being spun up and destroyed based on workloads. When dealing with a large pool of containerized applications and workloads in CK8s, it is imperative to be proactive with continuous monitoring and debugging information in order to observe what is going on the cluster. These information can be seen at the container, node, or cluster level. Logging as one of the three pillars of observability is a crucial element to manage and monitor services and infrastructure. It allows you to track debugging information at different levels of granularity.","title":"Logging"},{"location":"user-guide/logs/#compliance-needs","text":"The requirements to comply with ISO 27001 are stated in ISO 27001:2013 . The annexes that mostly concerns logging are: Annex 12 , article A.12.4.1 \"Event Logging\" and A.12.4.3 \"Administrator and Operator Logs\". Annex 16 which deals with incident management. In Compliant Kubernetes, Elasticsearch is separate from the production workload, hence it complies with A.12.4.2 \"Protection of Log Information\". The cloud provider should ensure that the clock of Kubernetes nodes is synchronized, hence complying with A.12.4.4 \"Clock Synchronisation\".","title":"Compliance needs"},{"location":"user-guide/logs/#open-distro-for-elasticsearch","text":"Raw logs in CK8s are normalized, filtered, and processed by fluentd and shipped to Elasticsearch for storage and analysis. CK8s uses fully open source version of Elasticsearch called Open Distro for Elasticsearch . Open Distro for Elasticsearch provides a powerful, easy-to-use event monitoring and alerting system, enabling you to monitor, search, visualize your data among other things. Kibana is used as visualization and analysis interface for Elasticsearch for all your logs.","title":"Open Distro for Elasticsearch"},{"location":"user-guide/logs/#visualization-using-kibana","text":"Kibana is used as a data visualization and exploration tool for log time-series and aggregate analytics. It offers powerful and easy-to-use features such as histograms, line graphs, pie charts, heat maps, and built-in geospatial support. When you log into Kibana, you will encounter a page similar to the one shown below. Since we are concerned with searching logs and their visualization, we will focus on Visualize and Explore Data as indicated by the red rectangle in the figure above. If you are interested to know more about the rest please visit the official Kibana documentation . Before we dive into Kibana, let us discuss the type of logs ingested into Elasticsearch. Logs in CK8s cluster are filtered and indexed by fluentd into three categories: kubeaudit logs related to Kubernetes audits to provide security-relevant chronological set of records documenting the sequence of activities that have affected system by individual users, administrators or other components of the system. This is mostly related to the ISO 27001 requirement A.12.4.3 \"Administrator and Operator Logs\". Kubernetes logs that provide insight into CK8s resources such as nodes, Pods, containers, deployments and replica sets. This allows you to observe the interactions between those resources and see the effects that one action has on another. Generally, logs in the CK8s ecosystem can be divided into the cluster level (logs outputted by components such as the kubelet, the API server, the scheduler) and the application level (logs generated by pods and containers). This is mostly related to the ISO 27001 requirement A.12.4.3 \"Administrator and Operator Logs\". Others logs other than the above two are indexed and shipped to Elasticsearch as others . Such logs are basically your application level logs. This is mostly related to the ISO 27001 requirement A.12.4.1 \"Event Logging\". Let us dive into Kibana then.","title":"Visualization using Kibana"},{"location":"user-guide/logs/#data-visualization-and-exploration-in-kibana","text":"As you can see in the figure above, data visualzation and expoloration in Kibana has three components: Discover , Visualize and Dashboard . The following section describes each components using examples.","title":"Data Visualization and Exploration in Kibana"},{"location":"user-guide/logs/#discover","text":"The discover component in Kibana is used for exploring, searching and filtering logs. Click Discover in the main Kibana page to access the features provided by it. The figure below shows partial view of the page that you will get under Discover . As you can see in the above figure, the kubeaudit index logs are loaded by default. If you want to explore logs from either of the other two log indices please select the right index under the dropdown menu marked log index category . To appreciate Kibana's searching and filtering capability, let us get data for the following question: Get all logs that were collected for the past 20 hours in host 172.16.0.3 where the responseStatus reason is notfound We can use different ways to find the answer for the question. Below is one possible solution. Write sourceIPs: 172.16.0.3 in the search textbox . Click Add Filter and select responseStatus.reason and is under field and Operator dropdown menus respectively. Finally, enter notfound under Value input box and click Save . The following figure shows the details. To enter the 20 hours, click part that is labelled Time in the Discover figure above, then enter 20 under the input box and select hours in the dropdown menu. Make sure that you are under Relative tab. Finally, click update . The following figure shows how to set the hours. Note that the data will be automatically updated as time passes to reflect the past 20 hours data from the current time. Once you are done, you will see a result similar to the following figure.","title":"Discover"},{"location":"user-guide/logs/#visualize","text":"The Visualize component in Kibana is to create different visualizations. Let us create a couple of visualizations. To create visualizations: Go to the main Kibana page and click Visual . Click Create visualization link located on the top right side of the page. Select a visualization type, we will use Pie here. Choose the index name or saved query name, if any, under New Pie / Choose a source . We will use the Kubernetes index here. By default a pie chart with the total number of logs will be provided by Kibana. Let us divide the pie chart based on the number of logs contributed by each namespace . To do that perform the following steps: Under Buckets click add then Split Slices . See the figure below. Under aggregation select Significant Terms terms. see the figure below. Select Kubernetes.namespace_name.keyword under field . See the figure below. The final result will look like the following figure. Please save the pie chart as we will use it later. Let us create a similar pie chart using host instead of namespace . The chart will look like the following figure.","title":"Visualize"},{"location":"user-guide/logs/#dashboard","text":"The Dashboard component in Kibana is used for organizing related visualizations together. Let us bring the two visualizations that we created above together in a single dashboard. To do that: Go to the main Kibana page and click Dashboard . Click Create Dashboard link located on the top right side of the page. Click Add existing link located on the left side. Select the name of the two charts/visualizations that you created above. The figure below shows the dashboard generated from the above steps showing the two pie charts in a single page.","title":"Dashboard"},{"location":"user-guide/logs/#accessing-falco-and-opa-logs","text":"To access Falco or OPA logs, go to the Discover panel and write Falco or OPA on the search textbox . Make sure that the Kubernetes log index category is selected. The figure below shows the search result for Falco logs. The figure below shows the search result for OPA logs.","title":"Accessing Falco and OPA Logs"},{"location":"user-guide/logs/#running-example","text":"The user demo application already includes structured logging: For each HTTP request, it logs the URL, the user agent, etc. Compliant Kubernetes further adds the Pod name, Helm Chart name, Helm Release name, etc. to each log entry. The screenshot below gives an example of log entries produced by the user demo application. It was obtained by using the index pattern kubernetes* and the filter kubernetes.labels.app_kubernetes_io/instance:myapp . Note You may want to save frequently used searches as dashboards. Compliant Kubernetes saves and backs these up for you.","title":"Running Example"},{"location":"user-guide/logs/#further-reading","text":"Open Distro for Elasticsearch Kibana Open Distro for Elasticsearch \u2013 How Different Is It? Fluentd","title":"Further Reading"},{"location":"user-guide/metrics/","text":"Metrics This guide gives an introduction to Prometheus and Grafana and where they fit in Compliant Kubernetes, in terms of reducing the compliance burden. Why Prometheus and Grafana? Prometheus is an open-source solution for monitoring and alerting. It works by collecting and processing metrics from the various services in the cluster. It is widely used, stable, and a CNCF member. It is relatively easy to write ServiceMonitors for any custom services to get monitoring data from them into Prometheus. Grafana is the most widely used technology for visualization of metrics and analytics. It supports a multitude of data sources and it is easy to create custom dashboards. Grafana is created by Grafana Labs, a CNCF Silver Member. Compliance needs The requirements to comply with ISO 27001 are stated in ISO 27001:2013 The annexes that mostly concerns monitoring and alerting are Annex 12 , article A.12.1.3 \"capacity management\", and Annex 16 which deals with incident management. Capacity management Article A.12.1.3 states that \"The use of resources must be monitored, tuned and projections made of future capacity requirements to ensure the required system performance to meet the business objectives.\" Promethus and Grafana helps with this as the resource usage, such as storage capacity, CPU, and network usage can be monitored. Using visualization in Grafana, projections can be made as to future capacity requirements. The article goes on to say that \"Capacity management also needs to be: Pro-active \u2013 for example, using capacity considerations as part of change management; Re-active \u2013 e.g. triggers and alerts for when capacity usage is reaching a critical point so that timely increases, temporary or permanent can be made.\" Prometheus has a rich alerting functionality, allowing you to set up alerts to warn if, for example, thresholds are exceeded or performance is degraded. Incident management Annex A.16.1 is about management of information security incidents, events and weaknesses. The objective in this Annex A area is to ensure a consistent and effective approach to the lifecycle of incidents, events and weaknesses. Incidents needs to be tracked, reported, and lessons learned from them to improve processes and reduce the possibility of similar incidents occurring in the future. Prometheus and Grafana can help with this by making it easier to: collect evidence as soon as possible after the occurrence. conduct an information security forensics analysis communicate the existence of the information security incident or any relevant details to the leadership. Prometheus and Grafana in Compliant Kubernetes Prometheus Compliant Kubernetes installs the prometheus-operator by default. The Prometheus Operator for Kubernetes provides easy monitoring definitions for Kubernetes services and deployment and management of Prometheus instances as it can create/configure/manage Prometheus clusters atop Kubernetes. The following CRDs are installed by default. crd apigroup kind used by description alertmanagers monitoring.coreos.com Alertmanager prometheus-alerts podmonitors monitoring.coreos.com PodMonitor customer-rbac prometheuses monitoring.coreos.com Prometheus prometheusrules monitoring.coreos.com PrometheusRule customer-rbac, elasticsearch servicemonitors monitoring.coreos.com ServiceMonitor customer-rbac, dex, grafana, kibana, elastisearch, influxdb thanosrulers monitoring.coreos.com ThanosRuler Accessing Prometheus The web interface is not exposed by default in Compliant Kubernetes. In order to access it, the most straight-forward way is to use port forwarding via the Kubernetes API . kubectl -- -n monitoring port-forward prometheus-prometheus-operator-prometheus-0 9090:9090 Depending on your Compliant Kubernetes settings, access to the Prometheus server might have been disabled by the administrator. Grafana Grafana can be accessed at the endpoint provided by the Compliant Kubernetes install scripts. If you have configured dex you can login with a connected account. Compliant Kubernetes deploys Grafana with a selection of dashboards by default. Dashboards are accessed by clicking the Dashboard icon (for squares) at the lefthand side of the grafana window and selecting Manage. Some examples of useful dashboards are listed below. Node health The Nodes dashboard (Nodes) gives a quick overview of the status (health) of a node in the cluster. By selecting an instance in the \"instance\" dropdown metrics for CPU, Load, Memory, Disk and Network I/O is showed for that node. The time frame can be changed either by using the time dropdown or selecting directly in the graphs. Pod health The Pods dashboard (Kubernetes/Compute resources/Pods) gives a quick overview of the status (health) of a pod in the cluster. By selecting a pod in the \"pod\" dropdown metrics for CPU, Memory, and Network I/O is showed for that node. The time frame can be changed either by using the time dropdown or selecting directly in the graphs. Running Example The user demo already includes a ServiceMonitor , as required for Compliant Kubernetes to collect metrics from its /metrics endpoint: {{ - if .Values.serviceMonitor.enabled - }} apiVersion : monitoring.coreos.com/v1 kind : ServiceMonitor metadata : name : {{ include \"ck8s-user-demo.fullname\" . }} labels : {{ - include \"ck8s-user-demo.labels\" . | nindent 4 }} spec : selector : matchLabels : {{ - include \"ck8s-user-demo.selectorLabels\" . | nindent 6 }} endpoints : - port : http {{ - end }} The screenshot below shows a Grafana dashboard featuring the query rate(http_request_duration_seconds_count[1m]) . It shows the request rate for the user demo application for each path and status code. As can be seen, the /users endpoint is getting popular. Note You may want to save frequently used dashboards. Compliant Kubernetes saves and backs these up for you. Further reading For more information please refer to the official Prometheus and Grafana documentation.","title":"Metrics"},{"location":"user-guide/metrics/#metrics","text":"This guide gives an introduction to Prometheus and Grafana and where they fit in Compliant Kubernetes, in terms of reducing the compliance burden.","title":"Metrics"},{"location":"user-guide/metrics/#why-prometheus-and-grafana","text":"Prometheus is an open-source solution for monitoring and alerting. It works by collecting and processing metrics from the various services in the cluster. It is widely used, stable, and a CNCF member. It is relatively easy to write ServiceMonitors for any custom services to get monitoring data from them into Prometheus. Grafana is the most widely used technology for visualization of metrics and analytics. It supports a multitude of data sources and it is easy to create custom dashboards. Grafana is created by Grafana Labs, a CNCF Silver Member.","title":"Why Prometheus and Grafana?"},{"location":"user-guide/metrics/#compliance-needs","text":"The requirements to comply with ISO 27001 are stated in ISO 27001:2013 The annexes that mostly concerns monitoring and alerting are Annex 12 , article A.12.1.3 \"capacity management\", and Annex 16 which deals with incident management.","title":"Compliance needs"},{"location":"user-guide/metrics/#capacity-management","text":"Article A.12.1.3 states that \"The use of resources must be monitored, tuned and projections made of future capacity requirements to ensure the required system performance to meet the business objectives.\" Promethus and Grafana helps with this as the resource usage, such as storage capacity, CPU, and network usage can be monitored. Using visualization in Grafana, projections can be made as to future capacity requirements. The article goes on to say that \"Capacity management also needs to be: Pro-active \u2013 for example, using capacity considerations as part of change management; Re-active \u2013 e.g. triggers and alerts for when capacity usage is reaching a critical point so that timely increases, temporary or permanent can be made.\" Prometheus has a rich alerting functionality, allowing you to set up alerts to warn if, for example, thresholds are exceeded or performance is degraded.","title":"Capacity management"},{"location":"user-guide/metrics/#incident-management","text":"Annex A.16.1 is about management of information security incidents, events and weaknesses. The objective in this Annex A area is to ensure a consistent and effective approach to the lifecycle of incidents, events and weaknesses. Incidents needs to be tracked, reported, and lessons learned from them to improve processes and reduce the possibility of similar incidents occurring in the future. Prometheus and Grafana can help with this by making it easier to: collect evidence as soon as possible after the occurrence. conduct an information security forensics analysis communicate the existence of the information security incident or any relevant details to the leadership.","title":"Incident management"},{"location":"user-guide/metrics/#prometheus-and-grafana-in-compliant-kubernetes","text":"","title":"Prometheus and Grafana in Compliant Kubernetes"},{"location":"user-guide/metrics/#prometheus","text":"Compliant Kubernetes installs the prometheus-operator by default. The Prometheus Operator for Kubernetes provides easy monitoring definitions for Kubernetes services and deployment and management of Prometheus instances as it can create/configure/manage Prometheus clusters atop Kubernetes. The following CRDs are installed by default. crd apigroup kind used by description alertmanagers monitoring.coreos.com Alertmanager prometheus-alerts podmonitors monitoring.coreos.com PodMonitor customer-rbac prometheuses monitoring.coreos.com Prometheus prometheusrules monitoring.coreos.com PrometheusRule customer-rbac, elasticsearch servicemonitors monitoring.coreos.com ServiceMonitor customer-rbac, dex, grafana, kibana, elastisearch, influxdb thanosrulers monitoring.coreos.com ThanosRuler","title":"Prometheus"},{"location":"user-guide/metrics/#accessing-prometheus","text":"The web interface is not exposed by default in Compliant Kubernetes. In order to access it, the most straight-forward way is to use port forwarding via the Kubernetes API . kubectl -- -n monitoring port-forward prometheus-prometheus-operator-prometheus-0 9090:9090 Depending on your Compliant Kubernetes settings, access to the Prometheus server might have been disabled by the administrator.","title":"Accessing Prometheus"},{"location":"user-guide/metrics/#grafana","text":"Grafana can be accessed at the endpoint provided by the Compliant Kubernetes install scripts. If you have configured dex you can login with a connected account. Compliant Kubernetes deploys Grafana with a selection of dashboards by default. Dashboards are accessed by clicking the Dashboard icon (for squares) at the lefthand side of the grafana window and selecting Manage. Some examples of useful dashboards are listed below.","title":"Grafana"},{"location":"user-guide/metrics/#node-health","text":"The Nodes dashboard (Nodes) gives a quick overview of the status (health) of a node in the cluster. By selecting an instance in the \"instance\" dropdown metrics for CPU, Load, Memory, Disk and Network I/O is showed for that node. The time frame can be changed either by using the time dropdown or selecting directly in the graphs.","title":"Node health"},{"location":"user-guide/metrics/#pod-health","text":"The Pods dashboard (Kubernetes/Compute resources/Pods) gives a quick overview of the status (health) of a pod in the cluster. By selecting a pod in the \"pod\" dropdown metrics for CPU, Memory, and Network I/O is showed for that node. The time frame can be changed either by using the time dropdown or selecting directly in the graphs.","title":"Pod health"},{"location":"user-guide/metrics/#running-example","text":"The user demo already includes a ServiceMonitor , as required for Compliant Kubernetes to collect metrics from its /metrics endpoint: {{ - if .Values.serviceMonitor.enabled - }} apiVersion : monitoring.coreos.com/v1 kind : ServiceMonitor metadata : name : {{ include \"ck8s-user-demo.fullname\" . }} labels : {{ - include \"ck8s-user-demo.labels\" . | nindent 4 }} spec : selector : matchLabels : {{ - include \"ck8s-user-demo.selectorLabels\" . | nindent 6 }} endpoints : - port : http {{ - end }} The screenshot below shows a Grafana dashboard featuring the query rate(http_request_duration_seconds_count[1m]) . It shows the request rate for the user demo application for each path and status code. As can be seen, the /users endpoint is getting popular. Note You may want to save frequently used dashboards. Compliant Kubernetes saves and backs these up for you.","title":"Running Example"},{"location":"user-guide/metrics/#further-reading","text":"For more information please refer to the official Prometheus and Grafana documentation.","title":"Further reading"},{"location":"user-guide/registry/","text":"Harbor - private container registry This guide gives an introduction to Harbor and where it fits in Compliant Kubernetes, in terms of reducing the compliance burden. What is a container registry and why it is needed? A container registry is a system where you can store your container images in order to later use the images when you deploy your application (e.g. as a Pod in a Kubernetes cluster). The images need a permanent storage since they are used many times by different instances, especially in Kubernetes where Pods (which are using the images) are considered ephemeral, so it is not enough to just store images directly on nodes/virtual machines. There are many popular container registries available as services, e.g. Docker Hub and Google Container Registry. A common workflow with container registries is to build your images in a CI/CD pipeline, push the images to your registry, let the pipeline change your deployments that uses the images, and let the deployments pull down the new images from the repository. What is Harbor? Harbor is an open source container registry tool that allows you to host a registry privately. It also comes with some extra features such as vulnerability scanning and role based access control, this increases security and eases compliance with certain regulations. Harbor is also a CNCF Graduated project , proving that it is widely used and is well supported. Why is Harbor used in Compliant Kubernetes? Harbor is used in Compliant Kubernetes to provide a secure container registry and a way to manage container image vulnerabilities. Harbor comes packaged with a container image vulnerability scanner that can check if there are any known vulnerabilities in the images you upload to Harbor. The default scanner is Trivy, which provides a comprehensive vulnerability detection both at the OS package and language-specific package levels. Below you can see both an image that has not been scanned and the same image after it has been scanned. After the image is scanned you can see the description, vulnerable package, and severity of each vulnerability as well as if it has been fixed in a later version. You can either scan the images manually or enable automatic scanning whenever a new image is pushed to Harbor, we recommend automatic scanning. In Harbor you can then also restrict so that you can't pull down images that have vulnerabilities of a certain severity or higher. This ensures that you don't accidentally start to use vulnerable images. If you try to deploy a Pod that uses a vulnerable image it will fail to pull the image. When you then inspect the Pod with kubectl describe you will find an error message similar to this: Failed to pull image \"harbor.test.compliantkubernetes.io/test/ubuntu\": rpc error: code = Unknown desc = Error response from daemon: unknown: current image with 77 vulnerabilities cannot be pulled due to configured policy in 'Prevent images with vulnerability severity of \"Medium\" or higher from running.' To continue with pull, please contact your project administrator to exempt matched vulnerabilities through configuring the CVE whitelist. By default we also prevent you from running images from anywhere else than your Harbor instance. This is to ensure that you use all of these security features and don't accidentally pull down vulnerable images from other container registries. We are using Open Policy Agent and Gatekeeper to manage this prevention. If you try to deploy a Pod with an image from another registry you will get an error message similar to this: for: \"unsafe-image.yaml\": admission webhook \"validation.gatekeeper.sh\" denied the request: [denied by require-harbor-repo] container \"unsafe-container\" has an invalid image repo \"unsafe.registry.io/ubuntu\", allowed repos are [\"harbor.test.compliantkubernetes.io\"] Running Example Configure container registry credentials First, retrieve your Harbor CLI secret and configure your local Docker client. In your browser, type harbor.$DOMAIN where $DOMAIN is the information you retrieved from your administrator. Log into Harbor using Single Sign-On (SSO) via OpenID. In the right-top corner, click on your username, then \"User Profile\". Copy your CLI secret. Now log into the container registry: docker login harbor.$DOMAIN . You should see Login Succeeded . Create a registry project Example Here is an example Dockerfile and .dockerignore to get you started. Don't forget to run as non-root. If you haven't already done so, create a project called demo via the Harbor UI, which you have accessed in the previous step. Clone the user demo If you haven't done so already, clone the user demo: git clone https://github.com/elastisys/compliantkubernetes/ cd compliantkubernetes/user-demo Build and push the image REGISTRY_PROJECT = demo # Name of the project, created above TAG = v1 # Container image tag docker build -t harbor. $DOMAIN / $REGISTRY_PROJECT /ck8s-user-demo: $TAG . docker push harbor. $DOMAIN / $REGISTRY_PROJECT /ck8s-user-demo: $TAG You should see no error message. Note down the sha256 of the image. Verification Go to harbor.$DOMAIN . Choose the demo project. Check if the image was uploaded successfully, by comparing the tag's sha256 with the one returned by the docker push command above. (Optional) While you're at it, why not run the vulnerability scanner on the image you just pushed. Further reading For more information please refer to the official Harbor , Trivy , Open Policy Agent and Gatekeeper documentation.","title":"Container registry"},{"location":"user-guide/registry/#harbor-private-container-registry","text":"This guide gives an introduction to Harbor and where it fits in Compliant Kubernetes, in terms of reducing the compliance burden.","title":"Harbor - private container registry"},{"location":"user-guide/registry/#what-is-a-container-registry-and-why-it-is-needed","text":"A container registry is a system where you can store your container images in order to later use the images when you deploy your application (e.g. as a Pod in a Kubernetes cluster). The images need a permanent storage since they are used many times by different instances, especially in Kubernetes where Pods (which are using the images) are considered ephemeral, so it is not enough to just store images directly on nodes/virtual machines. There are many popular container registries available as services, e.g. Docker Hub and Google Container Registry. A common workflow with container registries is to build your images in a CI/CD pipeline, push the images to your registry, let the pipeline change your deployments that uses the images, and let the deployments pull down the new images from the repository.","title":"What is a container registry and why it is needed?"},{"location":"user-guide/registry/#what-is-harbor","text":"Harbor is an open source container registry tool that allows you to host a registry privately. It also comes with some extra features such as vulnerability scanning and role based access control, this increases security and eases compliance with certain regulations. Harbor is also a CNCF Graduated project , proving that it is widely used and is well supported.","title":"What is Harbor?"},{"location":"user-guide/registry/#why-is-harbor-used-in-compliant-kubernetes","text":"Harbor is used in Compliant Kubernetes to provide a secure container registry and a way to manage container image vulnerabilities. Harbor comes packaged with a container image vulnerability scanner that can check if there are any known vulnerabilities in the images you upload to Harbor. The default scanner is Trivy, which provides a comprehensive vulnerability detection both at the OS package and language-specific package levels. Below you can see both an image that has not been scanned and the same image after it has been scanned. After the image is scanned you can see the description, vulnerable package, and severity of each vulnerability as well as if it has been fixed in a later version. You can either scan the images manually or enable automatic scanning whenever a new image is pushed to Harbor, we recommend automatic scanning. In Harbor you can then also restrict so that you can't pull down images that have vulnerabilities of a certain severity or higher. This ensures that you don't accidentally start to use vulnerable images. If you try to deploy a Pod that uses a vulnerable image it will fail to pull the image. When you then inspect the Pod with kubectl describe you will find an error message similar to this: Failed to pull image \"harbor.test.compliantkubernetes.io/test/ubuntu\": rpc error: code = Unknown desc = Error response from daemon: unknown: current image with 77 vulnerabilities cannot be pulled due to configured policy in 'Prevent images with vulnerability severity of \"Medium\" or higher from running.' To continue with pull, please contact your project administrator to exempt matched vulnerabilities through configuring the CVE whitelist. By default we also prevent you from running images from anywhere else than your Harbor instance. This is to ensure that you use all of these security features and don't accidentally pull down vulnerable images from other container registries. We are using Open Policy Agent and Gatekeeper to manage this prevention. If you try to deploy a Pod with an image from another registry you will get an error message similar to this: for: \"unsafe-image.yaml\": admission webhook \"validation.gatekeeper.sh\" denied the request: [denied by require-harbor-repo] container \"unsafe-container\" has an invalid image repo \"unsafe.registry.io/ubuntu\", allowed repos are [\"harbor.test.compliantkubernetes.io\"]","title":"Why is Harbor used in Compliant Kubernetes?"},{"location":"user-guide/registry/#running-example","text":"","title":"Running Example"},{"location":"user-guide/registry/#configure-container-registry-credentials","text":"First, retrieve your Harbor CLI secret and configure your local Docker client. In your browser, type harbor.$DOMAIN where $DOMAIN is the information you retrieved from your administrator. Log into Harbor using Single Sign-On (SSO) via OpenID. In the right-top corner, click on your username, then \"User Profile\". Copy your CLI secret. Now log into the container registry: docker login harbor.$DOMAIN . You should see Login Succeeded .","title":"Configure container registry credentials"},{"location":"user-guide/registry/#create-a-registry-project","text":"Example Here is an example Dockerfile and .dockerignore to get you started. Don't forget to run as non-root. If you haven't already done so, create a project called demo via the Harbor UI, which you have accessed in the previous step.","title":"Create a registry project"},{"location":"user-guide/registry/#clone-the-user-demo","text":"If you haven't done so already, clone the user demo: git clone https://github.com/elastisys/compliantkubernetes/ cd compliantkubernetes/user-demo","title":"Clone the user demo"},{"location":"user-guide/registry/#build-and-push-the-image","text":"REGISTRY_PROJECT = demo # Name of the project, created above TAG = v1 # Container image tag docker build -t harbor. $DOMAIN / $REGISTRY_PROJECT /ck8s-user-demo: $TAG . docker push harbor. $DOMAIN / $REGISTRY_PROJECT /ck8s-user-demo: $TAG You should see no error message. Note down the sha256 of the image.","title":"Build and push the image"},{"location":"user-guide/registry/#verification","text":"Go to harbor.$DOMAIN . Choose the demo project. Check if the image was uploaded successfully, by comparing the tag's sha256 with the one returned by the docker push command above. (Optional) While you're at it, why not run the vulnerability scanner on the image you just pushed.","title":"Verification"},{"location":"user-guide/registry/#further-reading","text":"For more information please refer to the official Harbor , Trivy , Open Policy Agent and Gatekeeper documentation.","title":"Further reading"}]}